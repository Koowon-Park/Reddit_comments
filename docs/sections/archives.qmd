---
title: "archive"
format: html
editor: visual
---

# LEO CODE
```{r eval=FALSE, echo=FALSE} 
# Load the necessary library
#install.packages("jsonlite") 
#install.packages("plyr") # Run this 2 lines in console only if not installed
library(jsonlite)
library(plyr)
# Set your working directory to where the text file is located
setwd("/Users/admin/Desktop/A HEC/Master/Semestre 3/Knowledge Graphs and Generative AI /Reddit")

# Read the text file line by line
lines <- readLines("RC_2015-01 - 15k.txt")  # Make sure to specify the correct filename

# Parse each line to JSON safely, handle errors and ensure missing fields are handled
json_data_list <- lapply(lines, function(line) {
  # Try to parse each JSON line, return NULL if there's an error
  tryCatch({
    parsed_json <- fromJSON(line, flatten = TRUE)
    as.data.frame(parsed_json, stringsAsFactors = FALSE)  # Convert to data frame
  }, error = function(e) NULL)  # Handle errors by returning NULL
})

# Filter out any NULL values (lines that failed to parse)
json_data_list <- Filter(Negate(is.null), json_data_list)

# Check if we have any parsed data
if (length(json_data_list) == 0) {
  stop("No valid JSON objects were parsed.")
}

# Combine all data frames in the list into a single data frame, handling different columns
df_reddit <- rbind.fill(json_data_list)  # rbind.fill fills missing columns with NA

# Write the data frame to a CSV file (optional)
write.csv(df_reddit, "reddit_small.csv", row.names = FALSE)
# Print a message when done
cat("Conversion complete! CSV saved as reddit_small.csv\n")
```

# URS CODE
## Check consistency accross txt file

```{r echo=FALSE, eval=FALSE}
# Load required libraries
library(jsonlite)
library(dplyr)

# Function to analyze JSON structure consistency
analyze_reddit_comments <- function(file_path) {
  # Read the file line by line
  lines <- readLines(file_path)
  
  # Parse each line as JSON and store the results
  parsed_data <- lapply(lines, fromJSON)
  
  # Get field names from each JSON object
  field_names <- lapply(parsed_data, names)
  
  # Check if all comments have the same fields
  are_fields_consistent <- length(unique(lapply(field_names, sort))) == 1
  
  # Get the common fields if structure is consistent
  common_fields <- sort(unique(unlist(field_names)))
  
  # Count number of records
  num_records <- length(lines)
  
  # Analyze field presence and types
  field_analysis <- data.frame(
    field_name = character(0),
    present_in_records = numeric(0),
    data_type = character(0)
  )
  
  for (field in common_fields) {
    field_presence <- sum(sapply(parsed_data, function(x) !is.null(x[[field]])))
    # Get the most common data type for this field
    field_type <- class(parsed_data[[1]][[field]])
    
    field_analysis <- rbind(field_analysis, 
                           data.frame(
                             field_name = field,
                             present_in_records = field_presence,
                             data_type = field_type[1]
                           ))
  }
  
  # Return analysis results
  return(list(
    total_records = num_records,
    structure_consistent = are_fields_consistent,
    common_fields = common_fields,
    field_analysis = field_analysis
  ))
}

#check wd
file.exists("../../../RC_2015-01 - 15k")

# usage:
results <- analyze_reddit_comments("../../../RC_2015-01 - 15k")

# Print results
cat("Total number of records:", results$total_records, "\n")
cat("Structure consistent across all records:", results$structure_consistent, "\n\n")
cat("Field analysis:\n")
print(results$field_analysis)
```

## Transform data into a CSV

```{r eval=FALSE, echo=FALSE}
# Function to convert Reddit comments to CSV
convert_reddit_to_csv <- function(input_file, output_file) {
  # Read and parse all JSON lines
  data <- stream_in(file(input_file), verbose = FALSE)
  
  # Select and transform relevant columns
  cleaned_data <- data %>%
    select(
      id,                    # unique identifier
      name,                  # full name
      link_id,               # post ID
      parent_id,            # parent comment/post ID
      subreddit,             # community
      subreddit_id,          # community ID
      author,                # username
      author_flair_text,     # user flair (ex, male)
      body,                  # comment text
      score,                 # net upvotes
      created_utc,           # timestamp
      controversiality,      # controversy flag
      gilded,               # times gilded
      edited                 # whether edited
    ) %>%
    # Convert UTC timestamp to datetime
    mutate(
      created_utc = as.POSIXct(as.numeric(created_utc), origin = "1970-01-01", tz = "UTC"),
      # Clean body text (remove newlines and quotes)
      body = gsub("\\n", " ", body),
      body = gsub("\"", "'", body)
    )
  
  # Write to CSV
  write.csv(cleaned_data, output_file, row.names = FALSE, quote = TRUE)
  
  # Return summary of the conversion
  return(list(
    total_rows = nrow(cleaned_data),
    columns = colnames(cleaned_data),
    file_size = file.size(output_file)
  ))
}

# Execute the conversion
results <- convert_reddit_to_csv("../../../RC_2015-01 - 15k", "reddit_comments_15k.csv")

# Print summary
cat("Conversion completed!\n")
cat("Total rows processed:", results$total_rows, "\n")
cat("Columns saved:", paste(results$columns, collapse = ", "), "\n")
cat("Output file size:", round(results$file_size/1024/1024, 2), "MB\n")
```
```{r}
#load the data
df_reddit <- read.csv("reddit_comments_15k.csv")

#show the unique first two letters of link_id
unique(substr(df_reddit$link_id, 1, 2))
#show the unique first two letters of parent_id
unique(substr(df_reddit$parent_id, 1, 2))

# create a new column parent_id_small without 3 first characters
df_reddit$parent_id_small <- substr(df_reddit$parent_id, 4, nchar(df_reddit$parent_id))

# create a new columnm link_id_small without 3 first characters
df_reddit$link_id_small <- substr(df_reddit$link_id, 4, nchar(df_reddit$link_id))
set.seed(123)

#download a random sample of data as a txt file
#write.table(df_reddit[sample(nrow(df_reddit), 10), ], "reddit_sample.txt", sep = ",", row.names = TRUE, quote = TRUE)

#show random sample of the columns id, link_id, parent_id, created_utc, subreddit
df_reddit[sample(nrow(df_reddit), 10), c("id", "name", "link_id", "parent_id", "created_utc", "subreddit_id", "subreddit")]

#show rows where link_id and parent_id matches
df_reddit[df_reddit$link_id == df_reddit$parent_id, c("id", "name", "link_id", "parent_id", "created_utc", "subreddit_id", "subreddit")]

#show rows where subreddit is AskReddit, link_id contains '2qyrje' and parent_id start with t3_ in ascending order of created_utc
df_reddit[grepl("2qyrje", df_reddit$link_id) & grepl("^t3_", df_reddit$parent_id) & df_reddit$subreddit == "AskReddit", c("id", "name", "link_id", "parent_id", "created_utc", "subreddit_id", "subreddit", "body")]

```

#### Similarities between words

In this part we analyze similarities between words (through documents). We restrict ourselves to a subset corresponding to words with frequency rank less than 40 (it should correspond to the 40 most frequent words but several words have the same frequency rank). We use the cosine similarity and plot the heatmap.

```{r similarity_words, eval=TRUE, echo=TRUE}
df.subreddit.freq <- textstat_frequency(df.subreddit.dfm) %>%
  filter(rank <= 40)
df.subreddit.freq$feature

df.subreddit.cos <- textstat_simil(
  df.subreddit.dfm[, df.subreddit.freq$feature],
  method = "cosine",
  margin = "feature")
df.subreddit.cos.mat <- melt(as.matrix(df.subreddit.cos)) # Convert the object to matrix then to data frame

ggplot(data = df.subreddit.cos.mat, aes(x=Var1, y=Var2, fill=value)) +
  scale_fill_gradient2(
    low = "blue",
    high = "red",
    mid = "white",
    midpoint = 0.5,
    limit = c(0, 1),
    name = "Cosine") +
  geom_tile() + 
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 5),
    axis.text.y = element_text(size = 5)) +
  xlab("") + 
  ylab("")
```

Lots of words are similar in terms of cosine similarity. this is peculiar which means that they are used in a similar proportion through subreddits.

#### Clustering Words

```{r clustering_words, eval=TRUE, echo=TRUE}
#cluster and rotate labels
df.subreddit.hc <- hclust(as.dist(1 - df.subreddit.cos))
plot(df.subreddit.hc)
```

#### Test topic extracted

```{r leo_test, message= FALSE, eval=TRUE, echo=TRUE}
# Preprocess the text in the body column
df.subreddit_clean <- df.subreddit %>%
  mutate(body = tolower(body)) %>%
  mutate(body = removePunctuation(body)) %>%
  mutate(body = removeNumbers(body))

# Tokenize words and remove common stop words
df_words <- df.subreddit_clean %>%
  unnest_tokens(word, body) %>%
  anti_join(stop_words)  # Remove common stop words like "the", "is", etc.

# Count the most frequent words
word_counts <- df_words %>%
  count(word, sort = TRUE)

# Display the top 20 most common words
head(word_counts, 20)

```
```{r leo_test2, eval=TRUE, echo=TRUE}
# Load necessary libraries
library(dplyr)
library(tidytext)
library(tidyr)

# Preprocess the text data
df.subreddit_clean <- df.subreddit %>%
  mutate(body = tolower(body)) %>%
  mutate(body = removePunctuation(body)) %>%
  mutate(body = removeNumbers(body))

# Tokenize the text, remove stopwords
df_words <- df.subreddit_clean %>%
  unnest_tokens(word, body) %>%
  anti_join(stop_words)  # Remove common stopwords

# Count word occurrences per subreddit
df_word_count <- df_words %>%
  count(subreddit, word, sort = TRUE)

# Concatenate the words and their counts into a single string for each subreddit
df_combined <- df_word_count %>%
  group_by(subreddit) %>%
  summarise(
    words = paste(word, collapse = ", "),          # Concatenate words with commas
    word_count = paste(n, collapse = ", ")         # Concatenate word counts
  ) %>%
  ungroup()

# Display the results
print(df_combined)



```

