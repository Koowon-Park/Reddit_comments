# Annex

## TF-IDF

$$TF=\frac{Number\ of\ times\ the\ term\ appears\ in\ the\ document}{Total\ number\ of\ terms\ in\ the\ document}$$

$$IDF=\log{\frac{Number\ of\ times\ the\ term\ appears\ in\ the\ document}{Total\ number\ of\ terms\ in\ the\ document}}$$

These two equations combined show when a word is important within a single subreddit, they balance the commonality within a document (a subreddit) and the rarity within the entire corpus (reddit as a whole)

```{r}
head(df.subreddit.tfidf.tidy, n = 20) %>% flextable() %>% autofit()
```

## Topic Modelling I
### Jaccard index
For this, we will extensively use the quanteda package and the objects created above.

Then use the functions textstat_simil() and textstat_dist() to compute the Jaccard index matrix.

We choose the Jaccard index because it is a good measure of similarity between two documents.

```{r}
print(p_similarities)
```

We observe that similarity using Jaccard index on the top 1000 tf-idf words is inconclusive.

Indeed, we do not observe any clear pattern.
This is due to the fact that we have a lot of words that are specific to each subreddit.

### Clusters

Hierarchical clustering and K-means.
The first one is applied on the dissimilarities (Euclidean).
The second one is applied on the features, here, TF-IDF.
To illustrate the methods, we decide to create 3 clusters.

will try to cluster the subreddits to see if we can regroup them in a more meaningful way

```{r}
plot(df.subreddit.hc)
df.subreddit.clust <- cutree(df.subreddit.hc, k = 3)
df.subreddit.clust
#k-means

df.subreddit.km <- kmeans(df.subreddit.tfidf.small, centers = 3)
df.subreddit.km$cluster
```

Only 50 where selected to show, but same applies to all subreddits.
Again, we observe that the clustering is not very clear, even worse.
We have a lot of subreddits that are not well regrouped.

This is due to the fact that we have a lot of specific words in each subreddit.

### LSA

We restrict here the PCA for links between subreddits and topics to only dimension 2 and 3 to obverse its efficacy.

We only show the features (words) that are mostly related to dim2 and dim3 so that the chart is not overcrowded.

We observe for example that the word `fuck` is associated with dim 3 and is interestingly inversly correlated to the word `happy` which makes sense.
This is more convincing than the similarity and clustering analysis we did before.

```{r}
# Adjust layout for margins
p_interactive_pca <- p_interactive_pca %>% layout(
  margin = list(l = 0, r = 0, t = 0, b = 0))
```

### Topic Term Analysis

The Topic-Term Analysis produces similar results as the LDA. This plot also provides the probabilities, therefore refining our understanding.

We still observe that lots of topics have the same terms.Therefore, adjusting k topics may be helpful. We first try with 7 topics

```{r}
plot_topic_term_1
```

### Topic Document Analysis

```{r}
plot_topic_document_analysis
```

## Topic Modelling II

First attempt at web-scraping

```{r, eval=FALSE}
# Example list of subreddit names (replace this with your dataframe column)
#subreddit_names <- head(df.subreddit$subreddit, 1000)
# Select rows 1001 to 3500
subreddit_names <- df.subreddit$subreddit[5715:nrow(df.subreddit)]
#subreddit_names <- c("100yearsago", "AskReddit", "nonexistent_subreddit", "private_subreddit")

# Function to scrape display-name and description
get_subreddit_metadata <- function(subreddit) {
  # Construct the URL
  url <- paste0("https://www.reddit.com/r/", subreddit, "/")
  
  # Try to scrape the metadata
  tryCatch({
    # Read the HTML content of the subreddit page
    page <- read_html(url)
    
    # Extract the <shreddit-subreddit-header> element
    header_element <- page %>%
      html_node("shreddit-subreddit-header")  # Target the specific element
    
    # Extract the attributes
    display_name <- header_element %>% html_attr("display-name")
    description <- header_element %>% html_attr("description")
    
    # Return the results
    return(data.frame(
      subreddit = subreddit,
      print("sub"),
      display_name = display_name,
      description = description,
      stringsAsFactors = FALSE
    ))
  }, error = function(e) {
    # If an error occurs (e.g., subreddit doesn't exist), return NA
    return(data.frame(
      subreddit = subreddit,
      display_name = NA,
      description = NA,
      stringsAsFactors = FALSE
    ))
  })
}

# Introduce a delay between requests
subreddit_metadata <- do.call(rbind, lapply(subreddit_names, function(subreddit) {
  # Randomize sleep time between 3 and 5 seconds
  sleep_time <- runif(1, min = 2, max = 4)

# Sleep for the random duration
  Sys.sleep(sleep_time) # Wait 2 seconds between requests
  get_subreddit_metadata(subreddit)
}))

# Save the cleaned dataset to a new CSV file
#write_csv(subreddit_metadata, "subreddit_metadata.csv")
# View the results
print(subreddit_metadata)
```

#### Second attempt, better way to scrape data.

```{r, eval=FALSE}
# Function to scrape display-name and description
get_subreddit_metadata <- function(subreddit) {
  # Construct the URL
  url <- paste0("https://www.reddit.com/r/", subreddit, "/")
  
  tryCatch({
    # Make the GET request with a User-Agent
    response <- GET(url, add_headers("User-Agent" = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"))
    
    # Check HTTP status code
    if (status_code(response) != 200) {
      cat(paste("Failed to fetch:", subreddit, "Status Code:", status_code(response), "\n"))
      return(data.frame(
        subreddit = subreddit,
        display_name = NA,
        description = NA,
        stringsAsFactors = FALSE
      ))
    }
    
    # Parse the HTML content
    page <- read_html(content(response, "text"))
    
    # Extract the <shreddit-subreddit-header> element
    header_element <- page %>%
      html_node("shreddit-subreddit-header")
    
    # Check if the element exists
    if (is.null(header_element)) {
      cat(paste("No header element found for subreddit:", subreddit, "\n"))
      return(data.frame(
        subreddit = subreddit,
        display_name = NA,
        description = NA,
        stringsAsFactors = FALSE
      ))
    }
    
    # Extract the attributes
    display_name <- header_element %>% html_attr("display-name")
    description <- header_element %>% html_attr("description")
    
    # Return the metadata
    return(data.frame(
      subreddit = subreddit,
      display_name = display_name,
      description = description,
      stringsAsFactors = FALSE
    ))
  }, error = function(e) {
    cat(paste("Error for subreddit:", subreddit, "with error:", e$message, "\n"))
    return(data.frame(
      subreddit = subreddit,
      display_name = NA,
      description = NA,
      stringsAsFactors = FALSE
    ))
  })
}

# Example usage with delays
subreddit_names <- tail(df.subreddit$subreddit, 2190)
#print(subreddit_names)

request_counter <- 0

# Introduce random delays between requests
results <- do.call(rbind, lapply(subreddit_names, function(subreddit) {
  # Increment the request counter
  request_counter <<- request_counter + 1
  
  # Print the current request number
  cat(paste("Request number:", request_counter, "\n"))
  
  # Randomize sleep time between 2 and 4 seconds
  sleep_time <- runif(1, min = 2, max = 3.8)
  Sys.sleep(sleep_time) # Delay between requests
  
  # Fetch metadata
  get_subreddit_metadata(subreddit)
}))

# View results
print(results)


```

Store in csv file

```{r}
#write_csv(subreddit_metadata, "subreddit_metadata_first1000.csv")
#write_csv(results, "subreddit_metadata_last2500.csv")
```

For computational purpose we don't run the code above as it takes a lot of time to scrape the data. We will use the data that we have already scraped.

## Sentiment

## Affin

```{r, eval=TRUE, echo=FALSE}  
# show the sentiment of the comments
table_a
```

## Bing

```{r, eval=TRUE, echo=FALSE}
#show 
table_b
```

After reading the comments, we observe mixed results.

  -  \[Dude Congrats Im hoping to lose a large amount of weight in the same amount of time Huge inspiration here Keep kicking ass\] was classified as neutral with a score of 0.

  - \[Why did the chicken cross the road It didnt HHAHHAHAHAHHAHAHHA sorry that wasnt funny 472\] was classified as quite negative with a score of -2; it seems, it did not understand the full context
  
## NRC

```{r, eval=TRUE, echo=FALSE}
#show 
table_n
```

We observe here that comments like "Night time moon light and a light snow The glow of the moon and the silence is amazing", which evokes a serene and appreciative tone for natural beauty suggest a strong positive emotion.
The three dictionaries seem to have classified this comment as positive with the BING and AFINN dictionary attributing a higher score than the NRC.

## Score

```{r}
print(summary(df_hugo$score))
```

We now explore the distribution of high-scoring comments (Score > 9). We log-scale the axis to better visualize the results.

```{r}
df_high_score <- df_hugo %>%
  filter(score %in% (10:3967))

ggplot(df_high_score, aes(x = score)) +
  geom_histogram(bins = 30, fill = viridis_4, color = "black", alpha = 0.7) +
  scale_y_log10() +
  scale_x_log10()+
  labs(title = "Log-Distribution of High Scoring (10+ upvotes) Comments",
       x = "Score",
       y = "Log(Frequency)") +
  theme_minimal() +
  scale_y_continuous(labels = scales::label_comma())
```
As expected, the higher scoring comments are rarer.

We perform the same analysis on comments with high amounts of Downvotes (Score = -10 and below).

```{r}
df_low_score <- df_hugo %>%
  filter(score %in% (-302:-10))

# Plotting the distribution of low scoring comments using ggplot2 with log transformation on both axes
ggplot(df_low_score, aes(x = score)) +
  geom_histogram(bins = 30, fill = viridis_5, color = "black", alpha = 0.7) +
  scale_y_log10() +
  scale_x_reverse() +
  labs(title = "Log-Distribution of Low Scoring (-10 and below) Comments",
       x = "Score",
       y = "Log(Frequency)") +
  theme_minimal() +
  scale_y_continuous(labels = scales::label_comma())

```

We come to the same conclusion as the high-score comments. There are a lot of comments with a small amount of downvotes, and very few that score extremely negatively.

## Score per subreddit

We also look at the distribution of the number of comments per subreddit retained.

```{r}
# Plotting histogram of the number of comments per subreddit for high scoring comments without subreddit names
count_distribution <- df_high_score %>%
  group_by(subreddit) %>%
  summarise(comment_count = n()) %>%
  filter(comment_count >= 5) %>%
  arrange(desc(comment_count))

# Plotting the distribution of number of comments per subreddit
ggplot(count_distribution, aes(x = comment_count)) +
  geom_histogram(bins = 30, fill = viridis_6, alpha = 0.8) +
  labs(title = "Distribution of Number of Comments per Subreddit for High Scoring Comments",
       x = "Number of Comments",
       y = "Frequency") +
  theme_minimal()
```

And look at the distribution of number of comments per subreddit with low scoring comments.
```{r}
# Plotting histogram of the number of comments per subreddit for low scoring comments without subreddit names
count_distribution_low <- df_low_score %>%
  group_by(subreddit) %>%
  summarise(comment_count = n()) %>%
  filter(comment_count >= 2) %>%
  arrange(desc(comment_count))

# Plotting the distribution of number of comments per subreddit for low scores
ggplot(count_distribution_low, aes(x = comment_count)) +
  geom_histogram(bins = 20, fill = viridis_10, alpha = 0.8) +
  labs(title = "Distribution of Number of Comments per Subreddit for Low Scoring Comments",
       x = "Number of Comments",
       y = "Frequency") +
  theme_minimal()

```


### Clusters of subreddits per scores

We now want to cluster the different subreddits in our dataset based on their scoring behaviour. To do this, we first use the Elbow method with the Within-Sum-of-Squares to find the number of clusters.

```{r}
# Clustering analysis based on number of comments and average score for high scoring subreddits
df_subreddit_cluster <- df_high_score %>%
  group_by(subreddit) %>%
  summarise(avg_score = mean(score), comment_count = n())

# Scaling the data for clustering
cluster_data <- df_subreddit_cluster %>%
  select(comment_count, avg_score) %>%
  scale()

# Determining the optimal number of clusters using the elbow method
set.seed(123)
fviz_nbclust(cluster_data, kmeans, method = "wss") +
  labs(title = "Elbow Method for Determining Optimal Number of Clusters",
       x = "Number of Clusters",
       y = "Within-Cluster Sum of Squares") +
  theme_minimal()
```

We'll settle with 5 clusters.

```{r}
# Running k-means clustering with 5 clusters
set.seed(123) # For reproducibility
kmeans_result <- kmeans(cluster_data, centers = 5)
df_subreddit_cluster$cluster <- as.factor(kmeans_result$cluster)

# Plotting the clusters
ggplot(df_subreddit_cluster, aes(x = comment_count, y = avg_score, color = cluster)) +
  geom_point(size = 3, alpha = 0.7) +
  scale_color_viridis_d() +
  labs(title = "Clustering of Subreddits Based on Number of Comments and Average Score",
       x = "Number of Comments",
       y = "Average Score") +
  theme_minimal() 
```

We see an interesting distribution of clusters.

 -  Our 1st cluster focuses on subreddits with a large number of comments, with medium to high scores.
 -  Our 2nd cluster focuses on subreddits with an average number of comments, with medium to high scores.
 -  Our 3rd cluster focuses on subreddits with a low number of comments, with medium to high scores.
 -  Our 4th cluster focuses on subreddits with a low number of comments, with high to very high scores.
 -  Our 5th cluster focuses on subreddits with a low number of comments, with low scores.
