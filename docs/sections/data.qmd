# Data

-   Sources
-   Description
-   Wrangling/cleaning
-   Spotting mistakes and missing data (could be part of EDA too)
-   Listing anomalies and outliers (could be part of EDA too)

setting eval=FALSE will not run the code when knitting the document

# LEO CODE
```{r eval=FALSE, echo=FALSE} 
# Load the necessary library
#install.packages("jsonlite") 
#install.packages("plyr") # Run this 2 lines in console only if not installed
library(jsonlite)
library(plyr)
# Set your working directory to where the text file is located
setwd("/Users/admin/Desktop/A HEC/Master/Semestre 3/Knowledge Graphs and Generative AI /Reddit")

# Read the text file line by line
lines <- readLines("RC_2015-01 - 15k.txt")  # Make sure to specify the correct filename

# Parse each line to JSON safely, handle errors and ensure missing fields are handled
json_data_list <- lapply(lines, function(line) {
  # Try to parse each JSON line, return NULL if there's an error
  tryCatch({
    parsed_json <- fromJSON(line, flatten = TRUE)
    as.data.frame(parsed_json, stringsAsFactors = FALSE)  # Convert to data frame
  }, error = function(e) NULL)  # Handle errors by returning NULL
})

# Filter out any NULL values (lines that failed to parse)
json_data_list <- Filter(Negate(is.null), json_data_list)

# Check if we have any parsed data
if (length(json_data_list) == 0) {
  stop("No valid JSON objects were parsed.")
}

# Combine all data frames in the list into a single data frame, handling different columns
df_reddit <- rbind.fill(json_data_list)  # rbind.fill fills missing columns with NA

# Write the data frame to a CSV file (optional)
write.csv(df_reddit, "reddit_small.csv", row.names = FALSE)
# Print a message when done
cat("Conversion complete! CSV saved as reddit_small.csv\n")
```

# URS CODE
## Check consistency accross txt file

```{r echo=FALSE, eval=FALSE}
# Load required libraries
library(jsonlite)
library(dplyr)

# Function to analyze JSON structure consistency
analyze_reddit_comments <- function(file_path) {
  # Read the file line by line
  lines <- readLines(file_path)
  
  # Parse each line as JSON and store the results
  parsed_data <- lapply(lines, fromJSON)
  
  # Get field names from each JSON object
  field_names <- lapply(parsed_data, names)
  
  # Check if all comments have the same fields
  are_fields_consistent <- length(unique(lapply(field_names, sort))) == 1
  
  # Get the common fields if structure is consistent
  common_fields <- sort(unique(unlist(field_names)))
  
  # Count number of records
  num_records <- length(lines)
  
  # Analyze field presence and types
  field_analysis <- data.frame(
    field_name = character(0),
    present_in_records = numeric(0),
    data_type = character(0)
  )
  
  for (field in common_fields) {
    field_presence <- sum(sapply(parsed_data, function(x) !is.null(x[[field]])))
    # Get the most common data type for this field
    field_type <- class(parsed_data[[1]][[field]])
    
    field_analysis <- rbind(field_analysis, 
                           data.frame(
                             field_name = field,
                             present_in_records = field_presence,
                             data_type = field_type[1]
                           ))
  }
  
  # Return analysis results
  return(list(
    total_records = num_records,
    structure_consistent = are_fields_consistent,
    common_fields = common_fields,
    field_analysis = field_analysis
  ))
}

#check wd
file.exists("../../../RC_2015-01 - 15k")

# usage:
results <- analyze_reddit_comments("../../../RC_2015-01 - 15k")

# Print results
cat("Total number of records:", results$total_records, "\n")
cat("Structure consistent across all records:", results$structure_consistent, "\n\n")
cat("Field analysis:\n")
print(results$field_analysis)
```

## Transform data into a CSV

```{r eval=FALSE, echo=FALSE}
# Function to convert Reddit comments to CSV
convert_reddit_to_csv <- function(input_file, output_file) {
  # Read and parse all JSON lines
  data <- stream_in(file(input_file), verbose = FALSE)
  
  # Select and transform relevant columns
  cleaned_data <- data %>%
    select(
      id,                    # unique identifier
      author,                # username
      subreddit,             # community
      body,                  # comment text
      score,                 # net upvotes
      created_utc,           # timestamp
      controversiality,      # controversy flag
      parent_id,            # parent comment/post ID
      gilded,               # times gilded
      edited                 # whether edited
    ) %>%
    # Convert UTC timestamp to datetime
    mutate(
      created_utc = as.POSIXct(as.numeric(created_utc), origin = "1970-01-01", tz = "UTC"),
      # Clean body text (remove newlines and quotes)
      body = gsub("\\n", " ", body),
      body = gsub("\"", "'", body)
    )
  
  # Write to CSV
  write.csv(cleaned_data, output_file, row.names = FALSE, quote = TRUE)
  
  # Return summary of the conversion
  return(list(
    total_rows = nrow(cleaned_data),
    columns = colnames(cleaned_data),
    file_size = file.size(output_file)
  ))
}

# Execute the conversion
results <- convert_reddit_to_csv("../../../RC_2015-01 - 15k", "reddit_comments_15k.csv")

# Print summary
cat("Conversion completed!\n")
cat("Total rows processed:", results$total_rows, "\n")
cat("Columns saved:", paste(results$columns, collapse = ", "), "\n")
cat("Output file size:", round(results$file_size/1024/1024, 2), "MB\n")
```

## check the CSV

```{r load_data}
#load csv with here
df <- read.csv(here("data/reddit_comments_15k.csv"))
(head(df, 1000))
summary(df)
```

## Clean the data

```{r clean_data}
#show column that contains 'deleted'
df[df$body == '[deleted]',]

#remove rows where 'deleted' in body or author
df <- df[!(df$body == '[deleted]' | df$author == '[deleted]'),]

# Remove any series of 'x' characters from the 'body' column
df$body <- gsub("xx+", "", df$body)
df$body <- gsub("XX+", "", df$body)

#show number of rows
nrow(df)

#remove 3 first character of the col parent_id and create a new column
df$parent_id_small <- substr(df$parent_id, 4, nchar(df$parent_id))

#remove weblink
df$body <- gsub("http\\S+", "", df$body)

#remove all quotes or special characters in text fields.
df$body <- gsub("[^[:alnum:][:space:]]", "", df$body)

#show random sample of df
set.seed(123)
df[sample(nrow(df), 1000),]
summary(df)
#save csv as cleaned (a reecrire qd on clean +)
write.csv(df, here("data/reddit_comments_15k_cleaned_v1.csv"), row.names = TRUE)

#create new dataset as csv but without col body
#write.csv(df[, -4], here("data/reddit_comments_15k_cleaned_NOBODY.csv"), row.names = TRUE)

```


