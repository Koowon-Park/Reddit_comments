```{r viridis, eval=TRUE, echo=FALSE}
#save some viridis colors
viridis_1 <- viridis(10)[1]
viridis_2 <- viridis(10)[2]
viridis_3 <- viridis(10)[3]
viridis_4 <- viridis(10)[4]
viridis_5 <- viridis(10)[5]
viridis_6 <- viridis(10)[6]
viridis_7 <- viridis(10)[7]
viridis_8 <- viridis(10)[8]
viridis_9 <- viridis(10)[9]
viridis_10 <- viridis(10)[10]
```

# EDA

## Intro: General exploration

### Subreddits as Documents

For further discussions: a subreddit = A document

Subreddits are treated as "documents," where all comments within a subreddit form a single document for text mining.

```{r top_10_sub, eval=TRUE, echo=TRUE}
#take the df data and group all comments by subreddit
df.subreddit <- df %>%
  group_by(subreddit) %>%
  summarise(body = paste(body, collapse = " "))

# show number of subreddits
cat("Number of subreddits:", nrow(df.subreddit), "\n")

# show top 10 subreddits by length in a bar plot
df.subreddit %>%
  mutate(subreddit = reorder(subreddit, nchar(body), fill = nchar(body))) %>%
  top_n(10, nchar(body)) %>%
  ggplot(aes(x = subreddit, y = nchar(body))) +
  geom_col(fill = viridis_1) +
  coord_flip() +
  labs(title = "Top 10 Subreddits by Length", x = "Subreddit", y = "Length") +
  #increase text size
  theme(axis.text.y = element_text(size = 14),
        axis.text.x = element_text(size = 14))
```

We observe that AskReddit is by far the largest Subreddit.
Half of the top 10 subreddits are for large audiences (i.e. news/worldnews, funny, etc.), while the other half is for more niche subjects.

### Words Analysis
#### Common Words

Here, we have a look at the most used words within comments while controlling for "meaningless" words, such as common stopwords (the, it, etc.) and reddit-related redundant words (reddit, subreddit, etc.)

```{r word_freq, eval=TRUE, echo=TRUE, cache=TRUE}
# preprocess the text data
df.subreddit.cp <- corpus(df.subreddit$body)
docnames(df.subreddit.cp) <- df.subreddit$subreddit  # Assign subreddit names as document identifiers

df.subreddit.tk <- tokens(
  df.subreddit.cp,
  remove_numbers = TRUE,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_url = TRUE,
  remove_hyphens = TRUE,
  remove_separators = TRUE
)
df.subreddit.tk <- df.subreddit.tk %>%
  tokens_tolower() %>%
  tokens_remove(stop_words$word) %>%
  tokens_remove(c("reddit", "subreddit", "amp", "gt", "deleted", "x+"))

# tf and tf-idf analysis
#tf
df.subreddit.dfm <- dfm(df.subreddit.tk)
tf <- rowSums(t(df.subreddit.dfm))
tf <- data.frame(term = names(tf), count = tf) %>%
  tibble() %>%
  arrange(desc(count))

#freq per documents
tidy(df.subreddit.dfm) %>%
  arrange(desc(count))

#freq per term
df.subreddit.freq <- textstat_frequency(df.subreddit.dfm)
head(df.subreddit.freq,20)

#plot 20 most frequent words
df.subreddit.freq %>%
  top_n(20, frequency) %>%
  ggplot(aes(
    x = reorder(feature, frequency),
    y = frequency)) +
  geom_bar(stat = "identity", fill = viridis_2) +
  coord_flip() +
  xlab("Frequency") +
  ylab("term") +
  #increase size of text
  theme(axis.text.y = element_text(size = 14),
        axis.text.x = element_text(size = 14))

```

We observe here a lot of abbreviations that are not taken into account by the stop words.
Indeed, a lot of word are still meaningless without context (I'm, You're, etc.)

#### Term-frequency - Inverse document frequency (TF-IDF)

See Annex for more details on the TF-IDF method.

This methods allows for an exploration of unique word usage per subreddits.


```{r tf_idf, eval=TRUE, echo=TRUE}
#tf-idf
df.subreddit.tfidf <- dfm_tfidf(df.subreddit.dfm)
df.subreddit.tfidf.tidy <- tidy(df.subreddit.tfidf) %>%
  bind_tf_idf(term = term, document = document, n = count) %>%
  arrange(desc(tf_idf))
```

So we have here lots of 'small' and 'rare' subreddit that have specific words that are not used in other subreddit.
This is interesting as it shows us the specificity of each subreddit.
(check the Annex for a representative table)

```{r tf_idf_plot, eval=TRUE, echo=TRUE}
#plot per document
df.subreddit.dfm %>%
  tidy() %>%
  top_n(12, count) %>%
  ggplot(aes(x = term, y = count)) +
  geom_bar(stat = "identity", fill = viridis_3) +
  coord_flip() +
  theme(axis.text.y = element_text(size = 14),
        axis.ticks.y = element_blank()) +
  facet_wrap(~document, ncol = 3)
```

### Word Analysis Summary

- We observe that AskReddit has lots of words frequency which makes sense as it is the largest subreddit.

-   We also observe that the term 'game' is associated in term of frequency with sports subreddits like CFB (College Football) and Hockey.

-   Interestingly the subreddit for the game League of Legends is associated with the term 'happy', which show us that the community may be the happiest, careful bias here, it is just a word frequency analysis.

-   In the subreddits 'pics' we observe a lot of 'gem' term which indicates treasure or something valuable.
    This is interesting as it is a subreddit for sharing pictures.

-   The subreddits 'news' is associated in terms of frequency with the term 'people' which is logical as news is about people.

## Topic Modelling I

#### Similarity Analysis
We analyze similarities and dissimilarities between the documents (through words) in the data.

We will use the Jaccard index. (see Annex `Similarities` section for more details)

```{r similarity_subreddits, eval=TRUE, echo=TRUE, cache=TRUE}
#We restrict ourselves to a small subset of the highest tf-idf of the data to avoid memory issues and readability issues in the plots.
# df.subreddit.tfidf.small <- df.subreddit.tfidf[1:1000,]
# df.subreddit.tfidf.small
# 
# #compute the jaccard index
# #comment out the next line to avoid recomputing the jaccard index
# df.subreddit.jac <- textstat_simil(
#   df.subreddit.tfidf.small, method = "jaccard", margin = "documents")
# 
# # Heatmap representation of similarities between subreddits
# df.subreddit.jac.mat <- melt(as.matrix(df.subreddit.jac))

#save
# write.csv(df.subreddit.jac.mat, "../../data/df.subreddit.jac.mat_1000.csv")
# 
# #load using here()
df.subreddit.jac.mat <- read.csv(here("data/df.subreddit.jac.mat_1000.csv"))

# Heatmap representation of similarities between subreddits
## Jaccard
p_similarities <- ggplot(data = df.subreddit.jac.mat, 
            mapping = aes(x = Var1, y = Var2, fill = value)) +
  scale_fill_gradient2(
    low = viridis_1, 
    high = viridis_5, 
    mid = viridis_10, 
    midpoint = 0.5, 
    limit = c(0, 1), 
    name = "Jaccard") +
  geom_tile() + 
  xlab("") + 
  ylab("") +
  theme(axis.text.x = element_blank(),  # Remove x-axis text
        axis.text.y = element_blank(),  # Remove y-axis text
        axis.ticks = element_blank())   # Remove axis ticks

```

The similarity using Jaccard index on the top 1000 tf-idf words is inconclusive.

This is due to the fact that we have a lot of words that are specific to each subreddit.

We have also tried to cluster the subreddits to see if we can regroup them in a more meaningful way, but the results are not very clear. (see Annex for more details)

```{r clustering_subreddits, eval=TRUE, echo=TRUE}
df.subreddit.tfidf.small <- df.subreddit.tfidf[1:50,]
df.subreddit.eucl <- textstat_dist(
  df.subreddit.tfidf.small, method = "euclidean", margin = "documents")
#hierarchical
df.subreddit.hc <- hclust(as.dist(df.subreddit.eucl))
## df.subreddit.hc <- hclust(as.dist(1 - df.subreddit.jac)) # use this line for Jaccard
## df.subreddit.hc <- hclust(as.dist(1 - df.subreddit.cos)) # use this line for Cosine
```

Therefore, we will try to use topic modeling to see if we can regroup the subreddits in a more meaningful way.

### Latent Semantic Analysis

LSA is a core technique in topic modelling, it uses matrices to separate topics and terms.

(see code for more detail on the results)

```{r lsa_subreddit, eval=TRUE, echo=TRUE}
#select subset of data of the top 10 subreddits for readability purposes
df.top10_subreddit <- df.subreddit %>%
  arrange(desc(nchar(body))) %>%
  filter(row_number() <= 10)

#corpus
df.top10_subreddit.cp <- corpus(df.top10_subreddit$body)
# Assign subreddit names as document identifiers
docnames(df.top10_subreddit.cp) <- df.top10_subreddit$subreddit

#token
df.top10_subreddit.tk <- tokens(
  corpus(df.top10_subreddit$body),
  remove_numbers = TRUE,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_url = TRUE,
  remove_separators = TRUE
)
df.top10_subreddit.tk <- df.top10_subreddit.tk %>%
  tokens_tolower() %>%
  tokens_remove(stop_words$word) %>%
  tokens_remove(c("reddit", "subreddit", "amp", "gt", "deleted", "x+")) %>%
  tokens_replace(
    pattern = hash_lemmas$token,
    replacement = hash_lemmas$lemma)

# Create Document-Feature Matrix (DFM)
df.top10_subreddit.dfm <- dfm(df.top10_subreddit.tk)

# Explicitly set document names in the DFM to the subreddit names
docnames(df.top10_subreddit.dfm) <- df.top10_subreddit$subreddit
df.top10_subreddit.dfm

#use textmodel_lsa() on dfm using n dimensions
df.subreddit.lsa <- textmodel_lsa(
  x = df.top10_subreddit.dfm,
  nd = 5)

#head(df.subreddit.lsa$docs)
#interpretation
## we look at the five terms with the largest values and the five ones with the lowest value (i.e., largest negative value)

n.terms <- 5
##for dim 2
w.order <- sort(df.subreddit.lsa$features[, 2], decreasing = TRUE)
w.top2 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))
##for dim 3
w.order <- sort(df.subreddit.lsa$features[, 3], decreasing = TRUE)
w.top3 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))

w.top2
w.top3
```

For readability, we apply it first on the top 10 subreddits to assess its application in our context.

-   AskReddit is strongly associated with terms like "people" and "job," indicating discussions about personal experiences and employment.
-   funny has a moderate association with "people" and "post," suggesting content related to social interactions and posts.
-   CFB and league of legends are associated with terms like "team" and "game," indicating discussions about sports and gaming.

The terms are associated with different dimensions, indicating their importance in the context of the documents.
Here are some key terms and their associations:

-   Dimension 1: "people" (0.1511), "question" (0.1346), "post" (0.1174)
-   Dimension 2: "team" (-0.2862), "play" (-0.2991), "game" (-0.5395)
-   Dimension 3: "fuck" (0.1837), "team" (0.1602), "call" (0.1255)
-   Dimension 4: "bowl" (0.1155), "boise" (0.0894), "steam" (-0.1784)
-   Dimension 5: "awesome" (-0.2124), "card" (-0.2553), "pc" (-0.2608)

However, we can see that it is not precise in our use-case, dim 4 and 2 and 3 seems quite similar..

We therefore inspect further with Singular Value Decomposition (SVD). The SVD coordinates help visualize the positioning of documents in the reduced-dimensional space, showing how closely related they are based on the terms they contain.


```{r pca, cache=TRUE, eval=TRUE, echo=TRUE}
set.seed(123)
# Restrict chart to terms that are mostly related to dim2 and dim3 
w.subset <- df.subreddit.lsa$features[ c(unique(c(names(w.top2), names(w.top3)))), 2:3]
# Create data frames for documents and features
docs_df <- as.data.frame(df.subreddit.lsa$docs[, 2:3])
features_df <- as.data.frame(w.subset) # to show all features (words) use instead : #features_df <- as.data.frame(df.subreddit.lsa$features[, 2:3])

# Rename columns for clarity
colnames(docs_df) <- c("Dim2", "Dim3")
colnames(features_df) <- c("Dim2", "Dim3")

# Create the ggplot biplot
p <- ggplot() +
  geom_segment(data = docs_df, 
               aes(x = 0, y = 0, xend = Dim2, yend = Dim3, text = rownames(docs_df)), 
               arrow = arrow(length = unit(0.2, "cm")), 
               color = "red") +
  geom_point(data = features_df, 
             aes(x = Dim2, y = Dim3, text = rownames(features_df)), 
             color = "black", size = 1) +
  xlab("Dim 2") +
  ylab("Dim 3") +
  theme_minimal()

# Convert ggplot to plotly for interactivity
p_interactive_pca <- ggplotly(p, tooltip = "text", width = 800, height = 600)
```

The subreddits are indeed better regrouped, in a more meaningful way. (see Annex for more details and a visual representation of the PCA plot)

### Latent Dirichlet Allocation (LDA)

LDA is a Bayesian version of the probabilistic Latent Semantic Analysis.
It answers the question "*Given this type of distribution, what are some actual probability distributions I am likely to see?*"

(see code for more details on the results)

```{r lda_subreddit, cache=TRUE, eval=TRUE, echo=TRUE}
docnames(df.top10_subreddit.dfm) <- df.top10_subreddit$subreddit
set.seed(1234) #To create reproducible results
df.top10_subbredit.lda <- textmodel_lda(
  x = df.top10_subreddit.dfm,
  k = 10)

#extract top 5 terms per topic
top_terms <- seededlda::terms(df.top10_subbredit.lda, 5)
top_terms
# Extract subreddits per topic and rename output to match subreddit names
top_subreddits <- seededlda::topics(df.top10_subbredit.lda)
names(top_subreddits) <- df.top10_subreddit$subreddit
top_subreddits

#count the number of documents per topic
topic_counts <- seededlda::topics(df.top10_subbredit.lda) %>% table
topic_counts
```

Topic 7 with  subreddits such as AskReddit, funny, pics and news is the most popular topic with words like "people", "time", "im", "youre" which may represent human experience and emotions. They touch on aspects of daily life, feelings, and the passage of time, with a bit of raw expression thrown in. It's a mix of the mundane and the profound, capturing the essence of what it means to be human.

However, the distinction between topics is still not very clear. We observe a lot of overlapping terms within the different topics. (see Annex for more details on the diagnostic of the LDA)

##### LDA diagnostics

(See the code output for more details analysis)

```{r topic_diagnostics, eval=TRUE, echo=TRUE}
#topic prevalence
rev(sort(colSums(df.top10_subbredit.lda$theta)/sum(df.top10_subbredit.lda$theta)))
```

Topic 4 most prevalent by far which makes sense as it is related to feelings and human experience that people often use in their comments.

### Topic-term Analysis

```{r topic_term_analysis, eval=TRUE, echo=TRUE, cache=TRUE}
#transform into a long df
phi.long <- melt(
  df.top10_subbredit.lda$phi,
  varnames = c("Topic", "Term"),
  value.name = "Phi") 

#plot 10 largest prob terms within each subject
plot_topic_term_1 <- phi.long %>% 
  group_by(Topic) %>% 
  top_n(10, Phi) %>% 
  ggplot(aes(reorder_within(Term, Phi, Topic), Phi, fill = as.factor(Topic))) + 
  geom_col(show.legend = FALSE) +
  coord_flip() +
  facet_wrap(~ Topic, scales = "free_y") +
  scale_x_reordered() + 
  scale_fill_viridis_d() +
  xlab("Term") + 
  theme(
    axis.text.y = element_text(size = 10),
    strip.text = element_text(size = 10))

docnames(df.top10_subreddit.dfm) <- df.top10_subreddit$subreddit
set.seed(1234) #To create reproducible results
df.top10_subbredit.lda <- textmodel_lda(
  x = df.top10_subreddit.dfm,
  k = 7)

#extract top 5 terms per topic
seededlda::terms(df.top10_subbredit.lda, 5)

# Extract subreddits per topic and rename output to match subreddit names
top_subreddits <- seededlda::topics(df.top10_subbredit.lda)
names(top_subreddits) <- df.top10_subreddit$subreddit
top_subreddits

#count the number of documents per topic
seededlda::topics(df.top10_subbredit.lda) %>% table()
#transform into a long df
phi.long <- melt(
  df.top10_subbredit.lda$phi,
  varnames = c("Topic", "Term"),
  value.name = "Phi") 

#plot 10 largest prob terms within each subject
phi.long %>% 
  group_by(Topic) %>% 
  top_n(10, Phi) %>% 
  ggplot(aes(reorder_within(Term, Phi, Topic), Phi, fill = as.factor(Topic))) + 
  geom_col(show.legend = FALSE) +
  coord_flip() +
  facet_wrap(~ Topic, scales = "free_y") +
  scale_x_reordered() + 
  scale_fill_viridis_d() +
  xlab("Term") + 
  theme(
    axis.text.y = element_text(size = 10),
    strip.text = element_text(size = 10))
```

The distinction between topics is clearer now, we observe that:

- Topic 1 may be related to 'pc games' ("game", "pc", "steam", "play", etc.)

- Topic 2 seems to also be related to pc games, but more precisely to the game League of Legends, where terms such as "champions", "ping", "riot" and "team" are prevalent.

- Topic 3 is also related to some sorts of games, we can maybe infer that it related to sports more specifically, with the presence of words such as "fan".

- Topic 4 is harder to distinguish with the presence of conflicting words such as "play", "kill", "run", and "raid".

- Topic 5 seems to be related to feelings with words like "im", time", "day", "feel", etc.

- Topic 6 seems to be related to Crime and Justice with the presence of words such as "murder", "evidence", and "interview".

- Topic 7 seems to be related to news and world event with the presence of words such as "country", "american", "russia," "war".
  
We see that they reflect our subreddit which makes sense. It would be interesting to take more that 10 subreddit as we might uncover interesting grouping of subreddits but it is so computationally intensive that we refrain from doing that for now.

### Topic document Analysis

We also analyzed the distribution of topics across the documents (subreddits). This differs from the previous analysis, where we looked at the distribution of terms (words) across topics. Here, we look at the distribution of topics across documents (subreddits).

And it confirmed our previous analysis. We observe that topic 1 related to pc/games, topic 2 related to league of legends, etc. (see Annex for the plot and more details)

```{r topic_doc_analysis, eval=TRUE, echo=TRUE}
set.seed(1234)
theta.long <- melt(
  df.top10_subbredit.lda$theta,
  varnames = c("Doc", "Topic"),
  value.name = "Theta")

# Ensure `theta.long` has the same document order as `df.top10_subreddit`
theta.long$Doc <- rep(df.top10_subreddit$subreddit, each = ncol(df.top10_subreddit.dfm$theta))

plot_topic_document_analysis <- theta.long %>% 
  group_by(Topic) %>% 
  top_n(10, Theta) %>% 
  ggplot(aes(reorder_within(Doc, Theta, Topic), Theta, fill = Theta)) + 
  geom_col(show.legend = FALSE) +
  coord_flip() +
  facet_wrap(~ Topic, scales = "free_y") +
  scale_x_reordered() + 
  scale_fill_viridis_c() + 
  xlab("Document") + 
  theme(
    axis.text.y = element_text(size = 10),
    strip.text = element_text(size = 10))
```

### Topic modelling summary

For our first analysis, we regrouped ALL comments per subreddit. From the results we obtained, we can argue that the LDA produced the best results:

  - It was more efficient that the clustering and similarity analysis.
  - The subreddits are well regrouped, in a meaningful way.
  - The topics are well defined and they reflect our subreddits.
  - The topics are well dsitributed across the documents (subreddits), which is a good sign.

However, regrouping ALL the comments per subreddits is not the best way to analyze the data. Indeed, we have a lot of specific words that are not taken into account by the stop words. Therefore, we will try to group in a more meaningful way, using the 'description' and 'display name' of each subreddit, obtained  via web scraping.

## Topic Modelling II

(See annex for the web-scrapping code )

Cleaning the data:

```{r, message=FALSE}
subreddit_first1000 <- read_csv(here("data", "subreddit_metadata_first1000.csv"))
subreddit_first1000 <- na.omit(subreddit_first1000)

subreddit_next2500 <- read_csv(here("data", "subreddit_metadata_next2500.csv"))
subreddit_next2500 <- na.omit(subreddit_next2500)

subreddit_last <- read_csv(here("data", "subreddit_metadata_last2500.csv"))

subreddit_last <- na.omit(subreddit_last)
# Combine the two dataframes
combined_subreddit_theme <- rbind(subreddit_first1000, subreddit_next2500,subreddit_last)
# Replace "/r/" with "subreddit" in the 'description' column of the 'combined_subreddit_theme' dataframe
combined_subreddit_theme$description <- gsub("/r/", "subreddit ", combined_subreddit_theme$description)

# Remove emojis and special characters from the 'description' column
combined_subreddit_theme$description <- gsub("[^[:alnum:][:space:]]", "", 
                                             iconv(combined_subreddit_theme$description, 
                                                   from = "UTF-8", to = "ASCII", sub = ""))
# Remove observations with empty or whitespace-only descriptions
combined_subreddit_theme <- combined_subreddit_theme %>%
  filter(str_trim(description) != "")

# Remove observations with "weird" descriptions
combined_subreddit_theme <- combined_subreddit_theme %>%
  filter(
    str_trim(description) != "" &            # Remove empty or whitespace-only descriptions
    nchar(str_trim(description)) > 3 &       # Keep descriptions longer than 3 characters
    str_detect(description, "[A-Za-z0-9]")   # Ensure descriptions contain alphanumeric characters
  )

# View the cleaned dataframe
head(combined_subreddit_theme)
```

### Clusters

Testing the text similarities through clusters

#### K-means

```{r, warning=FALSE, cache=TRUE}
corpus <- Corpus(VectorSource(combined_subreddit_theme$description))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("en"))

dtm <- DocumentTermMatrix(corpus)
set.seed(123)
k <- 30  # Choose the number of clusters
clusters <- kmeans(as.matrix(dtm), centers = k)
combined_subreddit_theme$theme <- as.factor(clusters$cluster)
# Rename the column
combined_subreddit_theme <- combined_subreddit_theme %>%
  rename(`theme (cluster number)` = theme)
# Remove duplicates based on the "subreddit" column
combined_subreddit_theme <- combined_subreddit_theme[!duplicated(combined_subreddit_theme$subreddit), ]

#plot
combined_subreddit_theme %>%
  ggplot(aes(x = `theme (cluster number)`)) +
  geom_bar(fill = viridis_1, color = "black") +
  labs(title = "Number of Subreddits per Cluster",
       x = "Cluster Number",
       y = "Number of Subreddits") +
  theme_minimal()
```

The k-means clustering method tend to classify the majority of the subreddits in only 5-6 clusters. We therefore test with another method.

#### LDA method
Based on the data we scrapped, we test our most promising method (LDA) on small scale of our new, more precise, dataset.

```{r, cache=TRUE, include=FALSE}
# Define irrelevant words
irrelevant_words <- c("reddit","reddits", "reddit's","www.reddit.com","https","http", "subreddit", "redditors","related", "community", "dedicated", "discussion", "discuss", "discussed","post", "content", "questions", "discussions", "share", "stories", "httpswwwredditcomsubreddit")

# Remove irrelevant words during the cleaning process
data_clean <- combined_subreddit_theme %>%
  unnest_tokens(word, description) %>%    # Tokenize the text into words
  anti_join(stop_words, by = "word") %>%  # Remove stopwords
  filter(!word %in% irrelevant_words) %>% # Exclude specific words
  filter(!str_detect(word, "^[0-9]+$"))   # Remove numbers

# Create a document-term matrix
dtm <- data_clean %>%
  count(subreddit, word) %>%                     # Count words per subreddit
  cast_dtm(subreddit, word, n)                  # Create the DTM
# Fit the LDA model
num_topics <- 5  
lda_model <- LDA(dtm, k = num_topics, control = list(seed = 1234))

# Get the terms associated with each topic
topics <- tidy(lda_model, matrix = "beta")

# Top terms for each topic
top_terms <- topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  arrange(topic, -beta)

print(top_terms)

# Get the topic distribution for each subreddit
subreddit_topics <- tidy(lda_model, matrix = "gamma")

print(subreddit_topics)

#list unique in topic
unique(subreddit_topics$topic)

# Visualize the top terms in each topic
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  scale_fill_viridis_d() +
  #increase font size
  theme(axis.text.x = element_text(size = 10),
        strip.text = element_text(size = 10))
```

##### Twenty topics
We now test with 20 topics. We use this incremental method because of the heavy computational resources needed for this method.

```{r, cache=TRUE}

num_topics <- 20  # Choose the number of topics you want to extract
lda_model <- LDA(dtm, k = num_topics, control = list(seed = 1234))


# Extract term-topic probabilities
topics <- tidy(lda_model, matrix = "beta")

# Top terms for each topic
top_terms <- topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 15) %>%
  ungroup() %>%
  arrange(topic, -beta)

print(top_terms)

# Extract subreddit-topic distributions
subreddit_topics <- tidy(lda_model, matrix = "gamma")

print(subreddit_topics)

```

Visualize:

```{r, fig.width=20, fig.height=15, warning=FALSE}

top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "Top Terms in Each Topic",
       x = "Terms",
       y = "Beta") +
  scale_fill_viridis_d() +
  #increase font size
  theme(axis.text.x = element_text(size = 10),
        strip.text = element_text(size = 10))
```

Feel free to zoom on the graph to see which terms are most used for each topic.
The LDA method has yielded the best results in our various tests. Based on the most commun words, we can infer that the topics are defined and that the subreddits are better regrouped. We therefore use it for our analysis.

```{r, eval=FALSE, include=FALSE}
# Assuming your data is stored in a dataframe called `top_terms`
top_terms_text <- top_terms %>%
  group_by(topic) %>%                # Group by topic
  slice_max(order_by = beta, n = 15) %>% # Select top 15 terms by beta for each topic
  arrange(topic, desc(beta)) %>%     # Arrange terms by topic and beta
  summarise(top_words = paste(term, collapse = ", ")) %>% # Combine the top 10 terms into a single string
  ungroup()

# View the result
#print(top_terms_text)
```

#### Extract dominant topic

To have a more granular view, we now try to assign a topic with the highest probability to each subreddit.

```{r}
# Extract subreddit-topic probabilities
subreddit_topics <- tidy(lda_model, matrix = "gamma")

# Find the topic with the highest probability for each subreddit
dominant_topics <- subreddit_topics %>%
  group_by(document) %>%
  slice_max(gamma, n = 1) %>%
  ungroup() %>%
  rename(subreddit = document, dominant_topic = topic)

# View dominant topics
print(dominant_topics)
```

The majority of the subreddits are not well associated with their dominant topic, Gamma can be seen as a probability (from 0 to 1) that a subreddit belongs to a topic. We can see that some with higher gamma (close to 1 ) are better classified than with lower gamma (close to 0) but there is still work to do.

#### Naming the topics

We now feed the dominant topics to ChatGPT to name them.

```{r}
# Add dominant topic to the original dataframe
combined_subreddit_theme <- combined_subreddit_theme %>%
  left_join(dominant_topics, by = c("subreddit" = "subreddit"))

# Define meaningful names for each topic
topic_names <- c(
  "Rules, Information, and Wiki Communities", 
  "Language, Sports, and Club Communities", 
  "Discord, Servers, and Safe Spaces", 
  "Gaming, TV Shows, and Strategy", 
  "University, Work, and Support Groups", 
  "NSFW, Visual Media, and Photography", 
  "City, Policy, and Political Communities", 
  "Food, Environment, and Social Selling", 
  "Sports, Teams, and Player Discussions", 
  "RPG, Fire, and Online Roleplaying", 
  "Advice, Forums, and Technical Help", 
  "Music, Tech, and Enthusiast Groups", 
  "Fans, Comics, and Book Discussions", 
  "Advice, PokÃ©mon, and Basic Tips", 
  "Art, Social Media, and Tactics", 
  "Learning, Projects, and Flair Sharing", 
  "Politics, Science, and Star Wars", 
  "Sharing Tips, Events, and Hardcore Music", 
  "Visual Media, Fans, and Rock Music", 
  "Gaming Systems, PC, and Friends"
)

# Add a new column with topic names corresponding to dominant_topic
combined_subreddit_theme <- combined_subreddit_theme %>%
  mutate(dominant_topic_name = topic_names[dominant_topic])

combined_subreddit_theme <- combined_subreddit_theme %>%
  rename(theme = dominant_topic)

combined_subreddit_theme <- combined_subreddit_theme %>%
  rename(theme_name = dominant_topic_name)

# Display the updated dataframe as a kable table (showing first few rows)
combined_subreddit_theme %>%
  head(3) %>% # Display only the first 10 rows for brevity
  kable(caption = "Combined Subreddit Theme Table")
```

In the table above, we see that names are better assigned when the beta is closer to 1. But it still far from perfect, the best method is probably to manually assign a theme to each subreddit.

```{r,echo=FALSE, warning=FALSE}
#save data
# write.csv(combined_subreddit_theme, "combined_subreddit_theme.csv", row.names = TRUE)

#read data
# combined_subreddit_theme <- read.csv("../../data/combined_subreddit_theme.csv")

# Perform a left join to add the columns to 'df'
df <- df %>%
  left_join(combined_subreddit_theme, by = "subreddit")

#head(df)
```

## Sentiment Analysis

### Sentiment by Subreddit

Analyzing the top *30* subreddits (30 for vizualisation purposes) as a *whole*, focusing on average sentiment scores for each subreddit to get a general idea of the sentiment of the comments in each subreddit.
We will use the AFINN, NRC, and BING lexicons to assign sentiment scores to words in the comments.
We will then calculate the average sentiment score for each subreddit based on the sentiment scores of the words in the comments.

#### AFINN

The AFINN lexicon is a list of English words rated for valence with an integer between -5 (negative) and +5 (positive).

-   Weaknessess : It may not capture the sentiment of slang, sarcasm, or other informal language. Also limited to the words in the lexicon.
-   Strengths : It is simple and easy to use.

```{r sentiment_afinn, eval=TRUE, echo=TRUE}
#get top 50 subreddits
df.top30_subreddit <- df.subreddit %>%
  arrange(desc(nchar(body))) %>%
  filter(row_number() <= 30)

df.top30_subreddit.tokens <- df.top30_subreddit %>% unnest_tokens(word, body)

df.top30_subreddit.affin <- inner_join(df.top30_subreddit.tokens, get_sentiments("afinn"),
 by = c("word" = "word")) %>% group_by(subreddit) %>%
 summarize(Sentiment = mean(value)) %>% ungroup()

ggplot(df.top30_subreddit.affin, aes(x = Sentiment, y = reorder(subreddit, Sentiment), fill = Sentiment)) +
  geom_col() +
  scale_fill_viridis_c() +
  ylab("") + 
  #increase text size
  theme(axis.text.y = element_text(size = 14),
        axis.text.x = element_text(size = 14))
```

There seems to be more positive subreddits than negative ones.

We interestingly observe that based on the affin dictionnary the `news` and `politics` subreddit are the most negative, which could reflect a state of the world.

And `pc` Subreddits like `pcmasterrace` or `builapc` are the most positive.

#### BING

The Bing sentiment lexicon categorizes words as either "positive" or "negative."

-   Weaknesses: It lacks the granularity of numerical scores and may not capture the intensity of sentiment. It also does not account for neutral words.
-   Strengths: Easy to use and understand. It provides a straightforward classification of words.

```{r bing_sentiment, eval=TRUE, echo=TRUE}
df.top30_subreddit.bing <- inner_join(df.top30_subreddit.tokens, get_sentiments("bing"),
 by = c("word" = "word")) %>% group_by(subreddit) %>%
 summarize(Sentiment = mean(ifelse(sentiment == "positive", 1, ifelse(sentiment == "negative", -1, 0)))) %>%
 select(subreddit, Sentiment)
 
ggplot(df.top30_subreddit.bing, aes(x = Sentiment, y = reorder(subreddit, Sentiment), fill = Sentiment)) +
  geom_col() +
  scale_fill_viridis_c() +
  ylab("") + 
  #increase text size
  theme(axis.text.y = element_text(size = 14),
        axis.text.x = element_text(size = 14))
```

BING seems to reflect the same proportion of positive and negative as the AFINN dictionary.
Indeed, we observe the same pattern with `pcmasterrace` at the top and `news` at the bottom, but there is slight variation when compared to the AFINN dictionary.

#### NRC

The NRC Emotion Lexicon (EmoLex) associates words with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (positive and negative).

-   Weaknesses: More complex to implement and interpret. It may require more computational resources.
-   Strengths: Provides a more detailed analysis by categorizing words into multiple emotions. It can capture a wider range of sentiments and emotions.

```{r nrc_sentiment, eval=TRUE, echo=TRUE}
df.top30_subreddit.nrc <- inner_join(df.top30_subreddit.tokens, get_sentiments("nrc"),
 by = c("word" = "word")) %>% group_by(subreddit) %>%
 summarize(Sentiment = mean(ifelse(sentiment == "positive", 1, ifelse(sentiment == "negative", -1, 0)))) %>%
 select(subreddit, Sentiment)

ggplot(df.top30_subreddit.nrc, aes(x = Sentiment, y = reorder(subreddit, Sentiment), fill = Sentiment)) +
  geom_col() +
  scale_fill_viridis_c() +
  ylab("") + 
  #increase text size
  theme(axis.text.y = element_text(size = 14),
        axis.text.x = element_text(size = 14))
```

Through the NRC dictionary we observe far more positive subreddits than negative ones.
This imbalance could be explain by the fact that the NRC dictionnary is more biased towards positive words.

We still observe quite the same patterns though with `pcmasterrace` at the top and `news` near the bottom.

#### Summary

To summarize it is interesting to observe that the NRC classification is totally different from the AFINN and BING classification.
Indeed, the NRC classification seems to be more biased towards positive words but on the other hand it shall be more precise as it has more categories.

Which one to choose ?
It depends on the goal of the analysis.
If we want a simple and easy to use classification we can use AFINN or BING.
If we want a more precise classification we can use NRC.

### Sentiment by Comment

(see the Annex for interactive Tables where you can read the comments and the attributed sentiment based on each sentiment dictionary)

To have a more precise understanding of the sentiment of the comments, we will analyze the sentiment per comment for each dictionary (AFINN, BING, NRC). So that we can choose the best dictionary for our analysis.

We will analyze a sample of small comments to have a better understanding.

```{r sample_afinn_comment, eval=TRUE, echo=TRUE}
#define a random sample of 100 comments
set.seed(1234)
df.sample <- df %>% sample_n(100) %>%
  #select only columns 'id' and 'body'
  select(id, body, subreddit)

# use afinn to analyse sentiment all the rows based on column body
df.sentiment <- inner_join(df.sample %>% unnest_tokens(word, body),
 get_sentiments("afinn"), by = c("word" = "word")) %>% group_by(id) %>%
 summarize(Sentiment = mean(value)) %>% inner_join(df.sample, by = "id")

# Normalize the sentiment scores
df.sentiment.a <- df.sentiment %>%
 mutate(Normalized_Sentiment = (Sentiment - min(Sentiment)) / (max(Sentiment) - min(Sentiment)))

table_a <- reactable(df.sentiment.a,
          resizable = TRUE,
          defaultPageSize = 2,
          sortable = TRUE,
          searchable = TRUE,
          filterable = TRUE,
          pagination = TRUE,
          highlight = TRUE
          )

df.sentiment <- inner_join(df.sample %>% unnest_tokens(word, body),
 get_sentiments("bing"), by = c("word" = "word")) %>% group_by(id) %>%
 summarize(Sentiment = mean(ifelse(sentiment == "positive", 1, ifelse(sentiment == "negative", -1, 0)))) %>%
 inner_join(df.sample, by = "id")

# Normalize the sentiment scores
df.sentiment.b <- df.sentiment %>%
 mutate(Normalized_Sentiment = (Sentiment - min(Sentiment)) / (max(Sentiment) - min(Sentiment)))

table_b <- reactable(df.sentiment.b,
          resizable = TRUE,
          defaultPageSize = 2,
          sortable = TRUE,
          searchable = TRUE,
          filterable = TRUE,
          pagination = TRUE,
          highlight = TRUE)


#use NRC to analyse sentiment all the rows based on column body
df.sentiment <- inner_join(df.sample %>% unnest_tokens(word, body),
 get_sentiments("nrc"), by = c("word" = "word")) %>% group_by(id) %>%
 summarize(Sentiment = mean(ifelse(sentiment == "positive", 1, ifelse(sentiment == "negative", -1, 0)))) %>%
 inner_join(df.sample, by = "id")

# Normalize the sentiment scores
df.sentiment.n <- df.sentiment %>%
 mutate(Normalized_Sentiment = (Sentiment - min(Sentiment)) / (max(Sentiment) - min(Sentiment)))

table_n <- reactable(df.sentiment.n,
          resizable = TRUE,
          defaultPageSize = 2,
          sortable = TRUE,
          searchable = TRUE,
          filterable = TRUE,
          pagination = TRUE,
          highlight = TRUE)
```

We use here the normalized sentiment scores to have a better understanding of the distribution of the sentiment scores.

```{r sent_dist, cache=FALSE}
# use afinn to analyse sentiment all the rows based on column body
df.sentiment.a <- inner_join(df %>% unnest_tokens(word, body),
 get_sentiments("afinn"), by = c("word" = "word")) %>% group_by(id) %>%
 summarize(Sentiment = mean(value)) %>% inner_join(df.sample, by = "id")

# use bing to analyse sentiment all the rows based on column body
df.sentiment.b <- inner_join(df %>% unnest_tokens(word, body),
 get_sentiments("bing"), by = c("word" = "word")) %>% group_by(id) %>%
 summarize(Sentiment = mean(ifelse(sentiment == "positive", 1, ifelse(sentiment == "negative", -1, 0)))) %>%
 inner_join(df, by = "id")

#use NRC to analyse sentiment all the rows based on column body
df.sentiment.n <- inner_join(df %>% unnest_tokens(word, body),
 get_sentiments("nrc"), by = c("word" = "word")) %>% group_by(id) %>%
 summarize(Sentiment = mean(ifelse(sentiment == "positive", 1, ifelse(sentiment == "negative", -1, 0)))) %>%
 inner_join(df, by = "id")

# Normalize the sentiment scores
df.sentiment.a <- df.sentiment.a %>%
  mutate(Normalized_Sentiment = (Sentiment - min(Sentiment)) / (max(Sentiment) - min(Sentiment)))

df.sentiment.b <- df.sentiment.b %>%
  mutate(Normalized_Sentiment = (Sentiment - min(Sentiment)) / (max(Sentiment) - min(Sentiment)))

df.sentiment.n <- df.sentiment.n %>%
  mutate(Normalized_Sentiment = (Sentiment - min(Sentiment)) / (max(Sentiment) - min(Sentiment)))

# Create the ggplot object
p <- ggplot() +
  geom_density(data = df.sentiment.a, aes(x = Normalized_Sentiment, fill = "AFINN"), alpha = 0.3) +
  geom_density(data = df.sentiment.b, aes(x = Normalized_Sentiment, fill = "BING"), alpha = 0.5) +
  geom_density(data = df.sentiment.n, aes(x = Normalized_Sentiment, fill = "NRC"), alpha = 0.5) +
  scale_fill_viridis_d() +
  theme_minimal()

# Convert the ggplot object to an interactive plotly object
interactive_plot_sentiment <- ggplotly(p, 
                                       width = 600, 
                                       height = 400,)

# Adjust the height and width 
interactive_plot_sentiment <- interactive_plot_sentiment %>% layout(
  margin = list(l = 0, r = 0, t = 0, b = 0)
)
# Display the interactive plot
interactive_plot_sentiment
```

-   The AFINN method shows a relatively flat distribution with a slight peak around 0.
    This suggests that the sentiment scores are spread out, with a slight tendency towards neutrality.

-   The Bing method displays a bimodal distribution with two distinct peaks, one around -1 and another around 1.
    This indicates that the sentiment scores are polarized, with many comments being classified as either quite negative or quite positive.
    Which is logical based on the previous analysis

-   The NRC method has a sharp peak at 0, suggesting that most of the sentiment scores are neutral.
    This implies that the NRC method tends to classify a large number of comments as neutral, with fewer comments being classified as strongly positive or negative.
    But we note that the NRC is also quite condensed
    
#### Summary

We might choose one method over the others.
For example, if we need a method that captures strong sentiments, Bing might be more suitable.
For a more balanced view, AFINN could be the way to go.
With more neutral comments, NRC might be the best choice.

We will go with the Bing method as it seems to capture the sentiment of the comments more accurately.

### Apply The findings on the whole dataset

```{r}
df.sentiment <- inner_join(df %>% unnest_tokens(word, body),
 get_sentiments("bing"), by = c("word" = "word")) %>% group_by(id) %>%
 summarize(Sentiment = mean(ifelse(sentiment == "positive", 1, ifelse(sentiment == "negative", -1, 0)))) %>%
 inner_join(df, by = "id")

#barplot
ggplot(df.sentiment, aes(x = Sentiment)) +
  geom_bar(fill = viridis_3, color = "black", alpha = 0.7, width = 0.05) +
  labs(title = "Distribution of Sentiment Scores",
       x = "Sentiment Score",
       y = "Frequency") +
  theme_minimal() +
  theme(panel.grid.major = element_line(color = "grey", linetype = "dashed", size = 0.5)) +
  #tilt 45 degree x axis
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

We will 'cut' the sentiment score into 5 categories: 

- extremely_positive, 1
- positive, 0.2 to 0.99
- somewhat_positive, neutral, -0.2 to 0.2
- somewhat_negative, negative, -0.99 to -0.2
- negative, -1 to -0.99


```{r}
# Normalize the sentiment scores
df.sentiment <- df.sentiment %>%
 mutate(Normalized_Sentiment = (Sentiment - min(Sentiment)) / (max(Sentiment) - min(Sentiment)))


# Categorize sentiment into five groups 
df.sentiment <- df.sentiment %>% mutate(sentiment_category = case_when( 
  Sentiment >= 0.99 ~ "extremely_positive", 
  Sentiment > 0.2 & Sentiment < 0.99 ~ "somewhat_positive", 
  Sentiment >= -0.2 & Sentiment <= 0.2 ~ "neutral",
  Sentiment > -0.99 & Sentiment < -0.2 ~ "somewhat_negative",
  Sentiment <= -0.99 ~ "extremely_negative"
))

df <- df.sentiment 
head(df)

#count each unique value in sentiment col
table(df.sentiment$sentiment_category)
# save the data as a .csv file
#write.csv(df.sentiment, "../../data/BING_sentiment.csv", row.names = TRUE)
```

## Score Analysis

### Score distribution

To complement our sentiment analysis, we perform a quick exploration of the score (Upvotes and Downvotes). First, we compute the total amount of comment with:


- No interactions --> Score = 1
- Only 1 interactions --> Score = 0 or 2

```{r}
df_hugo <- df

score_2 <- sum(df_hugo$score==2)
score_1 <- sum(df_hugo$score==1)
score_0 <- sum(df_hugo$score==0)

print(paste0("Total number of comments without Up or Downvotes is ", score_1))
print(paste0("Total number of comments with only one Up or Downvote is ", score_0 + score_2))
print(paste0(round((100*(score_0 + score_1 + score_2)/138070),digits = 2), "% of the comments have 0 or 1 upvote/downvote"))
```

We find that 44,12% of our comments have been interacted with 0 times or only once.

We then focus on comments that received average interactions (Scores between -10 and 20).

```{r}
# Filtering scores to focus on the range from -10 to 100
range_filtered <- df_hugo %>%
  filter(score >= -10 & score <= 20)

ggplot(range_filtered, aes(x = score)) +
  geom_histogram(bins = 30, fill = viridis_3, color = "black", alpha = 0.7) +
  labs(title = "Distribution of Score Column from -10 to 20",
       x = "Score",
       y = "Frequency") +
  theme_minimal() +
  theme(
    panel.grid.major = element_line(color = "grey", linetype = "dashed", size = 0.5),
    text = element_text(size = 16), # Increases overall text size
    axis.title = element_text(size = 14), # Increases axis title text size
    axis.text = element_text(size = 12), # Increases axis labels text size
    plot.title = element_text(size = 18, face = "bold") # Increases title text size
  )
```

We see here that indeed, most comments in our dataset have a score of 1 or 2. Other exploratory graphs can be found in Annex, where the distribution of very popular (positively or negatively) comments are displayed. Overall, we find that most comments mostly follow a somewhat normal distribution, with most comments receiving little to no attention.

### Score categories
With the distribution seen above (and the ones in Annex), we then categorize the scores between "Extremely positive" (Score > 99) and "Very negative" (Score < -20).

```{r}
df <- df %>%
  mutate(score_category = case_when(
    score >= 100 ~ "extremely_positive",
    score >= 20 & score < 100 ~ "very_positive",
    score >= 10 & score < 20 ~ "positive",
    score >= 2 & score < 10 ~ "somewhat_positive",
    score == 1 ~ "untouched",
    score > -10 & score <= 0 ~ "somewhat_negative",
    score >= -20 & score <= -10 ~ "negative",
    score < -20 ~ "very_negative"
  ))
```

### Score per subreddits

#### High-scores

We now want to explore whether different subreddits have different behaviors in terms of scoring.

```{r}
# Counting unique subreddits in the dataset
unique_subreddits <- n_distinct(df_high_score$subreddit)
print(paste0("Total number of unique subreddits in the High-score dataset: ", unique_subreddits))
```

We filter our dataset to only keep the subreddits with 5 or more comments. And then compute the average score of a comment in each subreddit.

```{r}
# Analyzing subreddit-wise tendency of high scores
df_subreddit_high_score <- df_high_score %>%
  group_by(subreddit) %>%
  summarise(avg_score = mean(score), median_score = median(score), comment_count = n()) %>%
  filter(comment_count >= 5) %>%
  arrange(desc(avg_score))

# Printing subreddit analysis
df_subreddit_high_score
```

We see that IAmA subreddit has the highest average score per comment, with an average score of 227,1 and a median score of 120, for a total of 47 comments.

We now look at the distribution of the average score per subreddit. We only display the first 10 results for readability.

```{r}
df_top10 <- df_subreddit_high_score %>%
  arrange(desc(avg_score)) %>%  # Ensure data is sorted by avg_score
  slice_head(n = 10)

# Plotting average score by subreddit for high scoring comments
ggplot(df_top10, aes(x = reorder(subreddit, avg_score), y = avg_score)) +
  geom_bar(stat = "identity", fill = viridis_7, alpha = 0.8) +
  coord_flip() +
  labs(title = "Top 10 Subreddits by Average Score for High Scoring Comments",
       x = "Subreddit",
       y = "Average Score") +
  theme_minimal()
```

#### Low-scores

We now perform the same analysis for low-scoring comments and subreddits. However, we keep all subreddits with 2 or more comments, to have a significant number of subreddits to analyze (given the rarer nature of low-scoring comments).

```{r}
# Analyzing subreddit-wise tendency of low scores
df_subreddit_low_score <- df_low_score %>%
  group_by(subreddit) %>%
  summarise(avg_score = mean(score), median_score = median(score), comment_count = n()) %>%
  filter(comment_count >= 2) %>%
  arrange(avg_score)

# Printing subreddit analysis for low scores
df_subreddit_low_score
```

We then plot the distribution of the average score of low-scoring subreddits.

```{r}
# Keep only the 10 lowest scores
df_subreddit_low_score <- df_subreddit_low_score %>%
  arrange(avg_score) %>%      # Sort by avg_score (ascending)
  slice_head(n = 10)          # Select the first 10 rows

# Plot the data
ggplot(df_subreddit_low_score, aes(x = avg_score, y = reorder(subreddit, avg_score))) +
  geom_bar(stat = "identity", fill = viridis(9)[1], alpha = 0.8) +
  scale_x_reverse() +
  labs(title = "Average Score by Subreddit for Low Scoring Comments (5 or more comments)",
       x = "Average Score",
       y = "Subreddit") +
  theme_minimal()
```
Funnily enough, the "cincinnati" subreddit has the lowest average score per comment, closely followed by explainlikeimfive and worldnews.

Some further exploration about scores and their relationships with subreddits can be found in the Annex. It is however not directly relevant for the rest of the report.

## Additional Variable

We now refine our dataset to explore some more relationships, as well as plotting a first knowledge graph.

### Scrap related subreddits

We use the following code to scrape relevant data from Reddit pages.

```{r, eval=FALSE}
# Function to scrape related subreddits
get_related_subreddits <- function(subreddit) {
  # Construct the URL
  url <- paste0("https://www.reddit.com/r/", subreddit, "/")
  
  tryCatch({
    # Make the GET request with a User-Agent
    response <- GET(url, add_headers("User-Agent" = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"))
    
    # Check HTTP status code
    if (status_code(response) != 200) {
      cat(paste("Failed to fetch:", subreddit, "Status Code:", status_code(response), "\n"))
      return(data.frame(
        subreddit = subreddit,
        related_subreddits = NA,
        stringsAsFactors = FALSE
      ))
    }
     # Parse the HTML content
    page <- read_html(content(response, "text"))
    
    # Extract related subreddit elements
    related_subreddits <- page %>%
      html_nodes("li .overflow-ellipsis") %>%  # Target divs with the specific class inside list items
      html_text(trim = TRUE)                  # Extract the visible text
    
    # Check if any related subreddits were found
    if (length(related_subreddits) == 0) {
      cat(paste("No related subreddits found for:", subreddit, "\n"))
      related_subreddits <- NA
    }
    
    # Return the results
    return(data.frame(
      subreddit = subreddit,
      related_subreddits = paste(related_subreddits, collapse = ", "), # Join as a single string
      stringsAsFactors = FALSE
    ))
  }, error = function(e) {
    cat(paste("Error for subreddit:", subreddit, "with error:", e$message, "\n"))
    return(data.frame(
      subreddit = subreddit,
      related_subreddits = NA,
      stringsAsFactors = FALSE
    ))
  })
}

# Example usage with a few subreddit names
subreddit_names <- combined_subreddit_theme$subreddit[3501:nrow(combined_subreddit_theme)]
# Loop through subreddits and fetch related subreddits
results <- do.call(rbind, lapply(subreddit_names, function(subreddit) {
  cat(paste("Processing:", subreddit, "\n"))
  Sys.sleep(runif(1, min = 2, max = 3.5))  # Randomized delay
  get_related_subreddits(subreddit)
}))

# View the results
#print(results)

```   


```{r, echo=FALSE, warning=FALSE, message=FALSE}
subreddit_related_part1 <- read_csv(here("data", "subreddit_relatedsub_part1.csv"))
# Remove r/ in column related subreddit
subreddit_related_part1$related_subreddits <- gsub("r/", "", subreddit_related_part1$related_subreddits)

subreddit_related_part2 <- read_csv(here("data", "subreddit_relatedsub_part2.csv"))
# Remove r/ in column related subreddit
subreddit_related_part2$related_subreddits <- gsub("r/", "", subreddit_related_part2$related_subreddits)

subreddit_related_part3 <- read_csv(here("data", "subreddit_relatedsub_part3.csv"))

#remove r/ in column related subreddit
subreddit_related_part3$related_subreddits <- gsub("r/", "", subreddit_related_part3$related_subreddits)

combined_subreddit_related <- rbind(subreddit_related_part1, subreddit_related_part2,subreddit_related_part3)

#remove Megathread and Announcement from related subreddits
combined_subreddit_related$related_subreddits <- gsub("Megathread", "", combined_subreddit_related$related_subreddits)
combined_subreddit_related$related_subreddits <- gsub("Announcement", "", combined_subreddit_related$related_subreddits)

#remove ", ", ", , ", ", , , " in the beginning of the string
combined_subreddit_related$related_subreddits <- gsub("^,\\s*", "", combined_subreddit_related$related_subreddits)
combined_subreddit_related$related_subreddits <- gsub("^,\\s*", "", combined_subreddit_related$related_subreddits)
combined_subreddit_related$related_subreddits <- gsub("^,\\s*", "", combined_subreddit_related$related_subreddits)
combined_subreddit_related$related_subreddits <- gsub("^,\\s*", "", combined_subreddit_related$related_subreddits)
combined_subreddit_related$related_subreddits <- gsub("^,\\s*", "", combined_subreddit_related$related_subreddits)

#yes the code could be more efficient than copy paste 5 times the line above 
```


```{r, echo=FALSE}
#add column to match related subreddit with the subreddit in our database

# Split related subreddits into individual components
combined_subreddit_related$matching_related_subreddits <- sapply(
  strsplit(combined_subreddit_related$related_subreddits, ",\\s*"), 
  function(related) {
    # Return only related subreddits that are in the 'subreddit' column
    related[related %in% combined_subreddit_related$subreddit]
  }
)

# Convert the list back into a string for easy viewing
combined_subreddit_related$matching_related_subreddits <- sapply(
  combined_subreddit_related$matching_related_subreddits, 
  paste, collapse = ", "
)

#we remove the NA values
combined_subreddit_related <- na.omit(combined_subreddit_related)

# remove empty related_subreddit in combined_subreddit_related
combined_subreddit_related <- combined_subreddit_related %>%
  filter(related_subreddits != "")
```

```{r, echo=FALSE, warning=FALSE}

# Perform a left join to add the columns to 'df'
df <- df %>%
  left_join(combined_subreddit_related, by = "subreddit")

#head(df)
```

After cleaning and updating and merging with the main database, we plot graphs of matching related subreddits.

```{r, fig.width=7, fig.height=5, echo=FALSE}
# Filter rows where `matching_related_subreddits` is not empty
filtered_data <- combined_subreddit_related[
  combined_subreddit_related$matching_related_subreddits != "", 
]

# Take only the first 700 rows
filtered_data <- head(filtered_data, 700)

# Prepare the edges for the graph
edges <- do.call(rbind, lapply(1:nrow(filtered_data), function(i) {
  source <- filtered_data$subreddit[i]
  targets <- unlist(strsplit(filtered_data$matching_related_subreddits[i], ",\\s*"))
  if (length(targets) > 0) {
    data.frame(from = source, to = targets, stringsAsFactors = FALSE)
  }
}))

# Ensure no duplicates in edges
edges <- unique(edges)

# Create nodes
nodes <- data.frame(
  id = unique(c(edges$from, edges$to)), # Unique nodes from edges
  label = unique(c(edges$from, edges$to)), # Labels are the same as IDs
  stringsAsFactors = FALSE
)

# Create the interactive graph
visNetwork(nodes, edges) %>%
  visNodes(size = 10) %>%
  visEdges(arrows = "to") %>%
  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %>%
  visInteraction(navigationButtons = TRUE) %>%
  visLayout(randomSeed = 42) %>% # Consistent layout
  visOptions(width="800px", height = "600px") # Control the height of the graph in the output


```

This graph show how subreddits are connected to each other. We can see that some subreddits are more related to others. More analysis will be done later to understand the relationships between subreddits.

```{r, echo=FALSE}
#other graph with everything
# 
# # Filter rows where `matching_related_subreddits` is not empty
# filtered_data <- combined_subreddit_related[
#   combined_subreddit_related$matching_related_subreddits != "", 
# ]
# 
# # Take the first 1000 rows
# filtered_data <- head(filtered_data, 1000)
# 
# # Prepare the edges for the graph
# edges <- do.call(rbind, lapply(1:nrow(filtered_data), function(i) {
#   source <- filtered_data$subreddit[i]
#   targets <- unlist(strsplit(filtered_data$matching_related_subreddits[i], ",\\s*"))
#   if (length(targets) > 0) {
#     data.frame(from = source, to = targets, stringsAsFactors = FALSE)
#   }
# }))
# 
# # Ensure no duplicates in edges
# edges <- unique(edges)
# 
# # Create the igraph object
# graph <- graph_from_data_frame(
#   d = edges,  # Edge list
#   directed = TRUE # Directional relationships
# )
# 
# # Use Kamada-Kawai layout for better node spacing
# set.seed(42) # For consistent layout
# layout <- layout_with_kk(graph)
# 
# # Scale layout to further spread nodes
# layout <- layout * 2
# # Plot the graph with spacing
# plot(
#   graph,
#   vertex.size = 0.2,                      # Node size
#   vertex.label = NA,                    # Disable labels
#   # vertex.label.cex = 0.1,               # Reduced label text size
#   # vertex.label.color = "black",         # Label color
#   # vertex.label.dist = 1,                # Distance of label from node
#   edge.arrow.size = 0.05,                # Smaller arrow size
#   edge.color = "gray50",                # Edge color
#   layout = layout,                      # Use the scaled layout
#   main = "Subreddit Relationship Graph" # Add a title
# )
# 

```


```{r, echo=FALSE}

# top 20 most related subreddits
# Filter rows where `matching_related_subreddits` is not empty
filtered_data <- combined_subreddit_related[
  combined_subreddit_related$matching_related_subreddits != "", 
]

# Take only the first 1000 rows (for performance reasons)
filtered_data <- head(filtered_data, 1000)

# Prepare the edges for the graph
edges <- do.call(rbind, lapply(1:nrow(filtered_data), function(i) {
  source <- filtered_data$subreddit[i]
  targets <- unlist(strsplit(filtered_data$matching_related_subreddits[i], ",\\s*"))
  if (length(targets) > 0) {
    data.frame(from = source, to = targets, stringsAsFactors = FALSE)
  }
}))

# Ensure no duplicates in edges
edges <- unique(edges)

# Count the number of links to each subreddit
link_counts <- table(edges$to)
link_counts <- sort(link_counts, decreasing = TRUE) # Sort by most links

# Extract the top 20 subreddits with the most links
top_20_subreddits <- head(link_counts, 20)

# Convert to a data frame for better readability
top_20_df <- as.data.frame(top_20_subreddits)
colnames(top_20_df) <- c("Subreddit", "Number_of_Links")

# Display the top 20 subreddits
#print(top_20_df)

# Optionally, create a barplot
barplot(
  top_20_df$Number_of_Links, 
  names.arg = top_20_df$Subreddit, 
  las = 2, # Rotate labels for better readability
  col = viridis_3, 
  main = "Top 20 Subreddits with Most Links to Them",
  ylab = "Number of Links",
  cex.names = 0.8 # Adjust label size
)

```
This graph allows us to identify the most interconnected subreddits in our dataset. Feel free to play with the interactive graph to see which subreddits are the most connected to each others.
Itâs important to note that this does not imply they are the most significant subreddits; rather, it indicates they are the most closely related to other subreddits based on the data we scraped from Reddit.

### Adding Length of comment as a feature

```{r, eval=TRUE, echo=TRUE}
#show length of one random comment
df$body[1] %>% nchar() #char shows the number of characters in a string
#to show the number of words in a comment we can use str_count
df$body[1] %>% str_count("\\w+")
df$body[1]
#add length of comment as a feature
df$length <- str_count(df$body, "\\w+")
head(df)

#get summary statistics on column length
summary(df$length)
```

## Final Dataset

```{r, eval=FALSE}
head(df)


df.final <- df %>%
  select(id, author, subreddit, display_name, body, score, sentiment_category, length, description, theme, theme_name, matching_related_subreddits, score_category)

head(df.final)

nrow(df.final)

#count NA in related subreddits
sum(is.na(df.final$matching_related_subreddits))

# Perform a left join to add the columns to 'df'
# df.final <- df %>%
#   left_join(combined_subreddit_theme, by = "subreddit")

#dataset without observations with missing values in some columns
# d_leo <- df %>%
#   drop_na()
# write.csv(d_leo, "../../data/reddit_comments_leo.csv", row.names = TRUE)

#write.csv(df.final, "../../data/reddit_comments_15k_v2.csv", row.names = TRUE)
#load the data using here
df.final <- read_csv(here("data", "reddit_comments_15k_v2.csv"))
(head(df.final))


# Create a table with theme names, their counts, and the theme column
theme_counts <- as.data.frame(table(df.final$theme_name))
colnames(theme_counts) <- c("Theme Name", "Count")
theme_counts$Theme <- df.final$theme[match(theme_counts$`Theme Name`, df.final$theme_name)]

#move theme column at the begining
theme_counts <- theme_counts[, c(3, 1, 2)]
# Display the table using kable
kable(theme_counts, col.names = c("Theme Number", "Theme Name", "Count"), caption = "Theme Name Counts with Theme Column")
```

```{r, eval=FALSE, echo=FALSE}
#show five row of df.final using reactable
reactable(head(df.final, 1000), 
          resizable = TRUE,
          defaultPageSize = 2,
          sortable = TRUE,
          searchable = TRUE,
          filterable = TRUE,
          pagination = TRUE,
          highlight = TRUE
          )
```
