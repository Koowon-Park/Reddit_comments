# EDA

## Text Mining

### Attempt 1

```{r}
# Step 1: Basic text cleaning and preprocessing
# Remove punctuation, convert to lowercase, remove stopwords, etc.
clean_text <- df %>%#
  mutate(body_clean = body %>%
           str_to_lower() %>%                  # Convert text to lowercase
           str_replace_all("[^[:alnum:]\\s]", " ") %>% # Remove punctuation
           str_squish())                       # Remove extra whitespace

# Remove stop words
data(stop_words)  # Load stopwords from tidytext package
clean_text <- clean_text %>%
  unnest_tokens(word, body_clean) %>%
  anti_join(stop_words, by = "word")

# Step 3: Exploratory Data Analysis (EDA)
# Word frequency analysis on training set
word_counts <- clean_text %>%
  count(word, sort = TRUE)

# Visualize top 20 most frequent words
word_counts %>%
  top_n(20, n) %>%
  ggplot(aes(x = reorder(word, n), y = n)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Top 20 Most Frequent Words", x = "Word", y = "Frequency")
```

```{r}
# Prepare document-term matrix (DTM) for LDA
dtm <- clean_text %>%
  count(id, word) %>%                       # Count word frequencies per comment (id)
  cast_dtm(id, word, n)                     # Create a document-term matrix

# Apply LDA to discover topics (time consumming)
#lda_model <- LDA(dtm, k = 5, control = list(seed = 1234))  # Choose 5 topics

# Save the LDA model to an R file
#save(lda_model, file = "lda_model.RData")

#load using here()
load(here("docs/sections/lda_model.RData"))
# Get the terms per topic
topics <- tidy(lda_model, matrix = "beta")

# Visualize the top 10 words per topic
top_terms <- topics %>%
  group_by(topic) %>%
  top_n(8, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ggplot(top_terms, aes(x = reorder_within(term, beta, topic), y = beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol = 5) +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "Top Terms per Topic", x = "Terms", y = "Beta")
```

### Second Attempt - A comment = A document or text?

#### Preprocessing

```{r}
#tokenize text
tokens <- df %>% unnest_tokens(word, body)
head(tokens)
df %>% unnest_tokens(word, body)  %>% 
  count(word, sort = TRUE) %>% 
  mutate(word = reorder(word, n)) %>% 
  filter(n >= 15000) %>% ggplot(aes(n, word)) + geom_col()
```

The `n` value in the `filter` function can be adjusted to show more or fewer words in the plot. This analysis can help identify the most common words in the text data, which can provide insights into the main topics or themes present in the text. Here `n`is set to 15000, which is enormous, because the dataset is large. We see here the most common words are words like "the", "and", "a", etc., which are common stopwords in English text. These words are not very informative for topic modeling or sentiment analysis, so they are typically removed in the preprocessing steps.

##### Creating a corpus

For consistency and a organized structure, we will create a corpus from the text data. A corpus is a collection of text documents that can be used for text analysis and natural language processing tasks. In this case, we will create a corpus where each document corresponds to a comment in the dataset. We will also add metadata to the corpus, such as the author, subreddit, score, controversiality, parent_id, gilded, and edited fields from the original dataset.

```{r}
# Create a corpus from the text data where the text field is "body" but the document field is "subreddit"
corpus <- corpus(df, text_field = "body", docid_field = "id") 

# Add metadata to the corpus
docvars(corpus, "author") <- df$author
docvars(corpus, "subreddit") <- df$subreddit
docvars(corpus, "score") <- df$score
docvars(corpus, "controversiality") <- df$controversiality
docvars(corpus, "parent_id") <- df$parent_id
docvars(corpus, "gilded") <- df$gilded
docvars(corpus, "edited") <- df$edited

summary(corpus)
```

##### Process the data

1.  Tokenization of the commment in `body` column into individual words. Which was already done before

2.  Preprocessing these tokens by converting them to lowercase, stemming, and removing common stop words. For this particular dataset, we know that the articles are coming from Reddit platform, hence the word ‘reddit’ is not useful to us can be removed as a stop word.

```{r}
df.tk <- tokens(
  corpus,
  remove_numbers = TRUE,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_url = TRUE,
  remove_hyphens = TRUE,
  remove_separators = TRUE,
)
df.tk <- df.tk %>%
  tokens_tolower() %>%
  tokens_remove(stop_words$word) %>%
  tokens_remove("reddit")
```

##### See again the most common words

```{r}
df.tk %>%
  dfm() %>%
  topfeatures( n = 20) %>%
  as.data.frame()
```

It looks like some of the words we’re seeing, such as “amp” and “gt”, are artifacts from HTML encoding or other text formatting issues.

There also seems to be a lot of curse words

##### Removing HTML encoding artifacts

```{r}
# Remove HTML tags
corpus <- str_remove_all(corpus, "<[^>]+>")

# Tokenize and preprocess
df.tk <- tokens(
  corpus,
  remove_numbers = TRUE,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_url = TRUE,
  remove_hyphens = TRUE,
  remove_separators = TRUE
)

df.tk <- df.tk %>%
  tokens_tolower() %>%
  tokens_remove(stop_words$word) %>%
  tokens_remove("reddit") %>%
  tokens_remove("subreddit") %>%
  tokens_remove("amp") %>%
  tokens_remove("gt") %>%
  tokens_remove("deleted") %>%
  # remove any 'xxxx' characters
  tokens_remove("x+")
```

##### See again the most common words

```{r}
df.tk %>%
  dfm() %>%
  topfeatures( n = 20) %>%
  as.data.frame()
```

#### TF and TF-IDF

##### Term Frequency (TF)

Now, let us compute the word frequencies (TF) and arrange them by decreasing frequencies. For the Document-Term Matrix, quanteda uses dfm() objects like below (for document frequency matrix).

```{r}
df.dfm <- dfm(df.tk)
df.dfm

#extract the TF
tf <- rowSums(t(df.dfm))
tf <- data.frame(term = names(tf), count = tf) %>%
  tibble() %>%
  arrange(desc(count))
head(tf)
```

##### Frequency per document (DFM)

Let us compute the frequencies by documents. We can use tidy function directly on the crude.dfm object to turn it into a dataframe and view the results.

```{r}
tidy(df.dfm) %>%
  arrange(desc(count))

## compute global term frequency
df.freq <- textstat_frequency(df.dfm)
head(df.freq,20)
```

##### TF-IDF

Now, we repeat the same analysis using the tf-idf formula. We use function bind_tf_idf() from tidytext. The results are ordered by decreasing TF_IDF. Note that the TF_IDF is calculated per document.

```{r}
df.tfidf <- dfm_tfidf(df.dfm)
df.tfidf.tidy <- tidy(df.tfidf) %>%
  bind_tf_idf(term = term, document = document, n = count) %>%
  arrange(desc(tf_idf))
head(df.tfidf.tidy, n = 10) %>% flextable() %>% autofit()
```

Now, we take the max TF_IDF per words (over all documents):

```{r}
df.tfidf.max <- df.tfidf.tidy %>%
  group_by(term) %>%
  summarise(tf_idf = max(tf_idf)) %>%
  ungroup() %>%
  arrange(desc(tf_idf))
head(df.tfidf.max, n = 10) %>% flextable() %>% autofit()
```

##### Plotting the Results

```{r}
df.freq %>%
  top_n(20, frequency) %>%
  ggplot(aes(
    x = reorder(feature, frequency),
    y = frequency)) + 
  geom_bar(stat = "identity") + 
  coord_flip() +
  xlab("Frequency") + 
  ylab("term")

textplot_wordcloud(df.dfm)

df.freq %>%
  top_n(50, frequency) %>%
  ggplot(aes(label = feature, size = frequency)) +
  geom_text_wordcloud() +
  scale_size_area() +
  theme_minimal()

#plot per documents
df.dfm %>%
  tidy() %>%
  top_n(10, count) %>%
  ggplot(aes(x = term, y = count)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  theme(axis.text.y = element_text(size = 8), 
        axis.ticks.y = element_blank()) +
  facet_wrap(~document, ncol = 5)
```

Maybe not really relevant as we have a lot of documents/comments ?

### Attempt 3 - A subreddit = A document

Here i take the idea that a 'document' is a subreddit. It is like all comments that are part of one subreddit will be a document. It is a bit an analysis per subreddit

```{r fig.width=10, fig.height=10}
#take the df data and group all comments by subreddit
df.subreddit <- df %>%
  group_by(subreddit) %>%
  summarise(body = paste(body, collapse = " "))

# show number of subreddits
cat("Number of subreddits:", nrow(df.subreddit), "\n")

# show top 10 subreddits by length in a bar plot
df.subreddit %>%
  mutate(subreddit = reorder(subreddit, nchar(body))) %>%
  top_n(10, nchar(body)) %>%
  ggplot(aes(x = subreddit, y = nchar(body))) +
  geom_col() +
  coord_flip() +
  labs(title = "Top 10 Subreddits by Length", x = "Subreddit", y = "Length")

# preprocess the text data
df.subreddit.cp <- corpus(df.subreddit$body)
docnames(df.subreddit.cp) <- df.subreddit$subreddit  # Assign subreddit names as document identifiers

df.subreddit.tk <- tokens(
  df.subreddit.cp,
  remove_numbers = TRUE,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_url = TRUE,
  remove_hyphens = TRUE,
  remove_separators = TRUE
)
df.subreddit.tk <- df.subreddit.tk %>%
  tokens_tolower() %>%
  tokens_remove(stop_words$word) %>%
  tokens_remove(c("reddit", "subreddit", "amp", "gt", "deleted", "x+"))

# tf and tf-idf analysis
#tf
df.subreddit.dfm <- dfm(df.subreddit.tk)
tf <- rowSums(t(df.subreddit.dfm))
tf <- data.frame(term = names(tf), count = tf) %>%
  tibble() %>%
  arrange(desc(count))

#freq per documents
tidy(df.subreddit.dfm) %>%
  arrange(desc(count))

#freq per term
df.subreddit.freq <- textstat_frequency(df.subreddit.dfm)
head(df.subreddit.freq,20)

#plot 20 most frequent words
df.subreddit.freq %>%
  top_n(20, frequency) %>%
  ggplot(aes(
    x = reorder(feature, frequency),
    y = frequency)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  xlab("Frequency") +
  ylab("term")

textplot_wordcloud(df.subreddit.dfm)

df.subreddit.freq %>%
  top_n(20, frequency) %>%
  ggplot(aes(label = feature, size = frequency)) +
  geom_text_wordcloud() +
  scale_size_area(max_size = 20) +
  theme_minimal()

#tf-idf
df.subreddit.tfidf <- dfm_tfidf(df.subreddit.dfm)
df.subreddit.tfidf.tidy <- tidy(df.subreddit.tfidf) %>%
  bind_tf_idf(term = term, document = document, n = count) %>%
  arrange(desc(tf_idf))
head(df.subreddit.tfidf.tidy, n = 20) %>% flextable() %>% autofit()

#plot per document
df.subreddit.dfm %>%
  tidy() %>%
  top_n(30, count) %>%
  ggplot(aes(x = term, y = count)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  theme(axis.text.y = element_text(size = 6),
        axis.ticks.y = element_blank()) +
  facet_wrap(~document, ncol = 5)
```

-   We observe that AskReddit has lots of words frequency which makes sense as it is the biggest subreddit by lenght.
-   We also observe that the term 'game' is associated in term of frequency with sports subreddits like CFB (College Football) and Hockey which makes sense.
-   Interestingly the subreddits LoL (League of Legends) is associated with the term 'happy', which show us that the community may be the happiest, careful bias here, it is just a word frequency analysis.
- In the subreddits 'pics' we observe a lot of 'gem' term which indicates treasure or something valuable. This is interesting as it is a subreddit for sharing pictures.
- The subreddits 'news' is associated in terms of frequency with the term 'people' which is logical as news is about people.

#### Similarity between Subreddits/Documents

we analyze similarities and dissimilarities between the documents (through words) in the data. We use quanteda extensively. We’ll use the objects created previously in the exercises.

Then use the functions textstat_simil() and textstat_dist() to compute the Jaccard index matrix, the cosine matrix, and the Euclidean distances matrix.

```{r}
#We restrict ourselves to a small subset of the highest tf-idf of the data to avoid memory issues and readability issues in the plots.
df.subreddit.tfidf.small <- df.subreddit.tfidf[1:50,]
df.subreddit.tfidf.small

df.subreddit.jac <- textstat_simil(
  df.subreddit.tfidf.small, method = "jaccard", margin = "documents")

df.subreddit.cos <- textstat_simil(
  df.subreddit.tfidf.small, method = "cosine", margin = "documents")

df.subreddit.eucl <- textstat_dist(
  df.subreddit.tfidf.small, method = "euclidean", margin = "documents")

#heatmap representation of similariteis between subreddits
## jaccard
df.subreddit.jac.mat <- melt(as.matrix(df.subreddit.jac))
ggplot( data = df.subreddit.jac.mat, 
        mapping = aes(x = Var1, y = Var2, fill = value)) +
  scale_fill_gradient2(
    low = "blue", 
    high = "red", 
    mid = 'white', 
    midpoint = 0.5, 
    limit = c(0,1), 
    name = "Jaccard") +
  geom_tile() + xlab("") + ylab("") +
  #incline x-axis labels
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

## cosine
df.subreddit.cos.mat <- melt(as.matrix(df.subreddit.cos))
ggplot( data = df.subreddit.cos.mat, 
        mapping = aes(x = Var1, y = Var2, fill = value)) +
  scale_fill_gradient2(
    low = "blue", 
    high = "red", 
    mid = 'white', 
    midpoint = 0.5, 
    limit = c(0,1), 
    name = "Cosine") +
  geom_tile() + xlab("") + ylab("") +
  #incline x-axis labels
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

## euclidean
df.subreddit.eucl.mat <- melt(as.matrix(df.subreddit.eucl))
ggplot( data = df.subreddit.eucl.mat, 
        mapping = aes(x = Var1, y = Var2, fill = value)) +
  scale_fill_gradient2(
    low = "blue", 
    high = "red", 
    mid = 'white', 
    midpoint = 0.5, 
    limit = c(0,1), 
    name = "Euclidean") +
  geom_tile() + xlab("") + ylab("") +
  #incline x-axis labels
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

#### Clustering of Documents

Hierarchical clustering and K-means. The first one is applied on the dissimilarities (Euclidean, inverted Jaccard, and inverted cosine). The second one is applied on the features, here, TF-IDF. To illustrate the methods, we decide to create five clusters.

```{r}
#hierarchical
df.subreddit.hc <- hclust(as.dist(df.subreddit.eucl))
## df.subreddit.hc <- hclust(as.dist(1 - df.subreddit.jac)) # use this line for Jaccard
## df.subreddit.hc <- hclust(as.dist(1 - df.subreddit.cos)) # use this line for Cosine
plot(df.subreddit.hc)
df.subreddit.clust <- cutree(df.subreddit.hc, k = 3)
df.subreddit.clust
#k-means

df.subreddit.km <- kmeans(df.subreddit.tfidf.small, centers = 3)
df.subreddit.km$cluster
```

#### Similarities between words

In this part we analyze similarities between words (through documents). We restrict ourselves to a subset corresponding to words with frequency rank less than 40 (it should correspond to the 40 most frequent words but several words have the same frequency rank). We use the cosine similarity and plot the heatmap.

```{r}
# crude.feat <- textstat_frequency(crude.dfm) %>%
#   filter(rank <= 40) 
# crude.feat$feature
# 
# crude.cos <- textstat_simil(
#   crude.dfm[, crude.feat$feature],
#   method = "cosine",
#   margin = "feature")
# crude.cos.mat <- melt(as.matrix(crude.cos)) # Convert the object to matrix then to data frame 
# 
# ggplot(data = crude.cos.mat, aes(x=Var1, y=Var2, fill=value)) +
#   scale_fill_gradient2(
#     low = "blue",
#     high = "red",
#     mid = "white",
#     midpoint = 0.5,
#     limit = c(0, 1),
#     name = "Cosine") +
#   geom_tile() + 
#   theme(
#     axis.text.x = element_text(angle = 45, hjust = 1, size = 5),
#     axis.text.y = element_text(size = 5)) +
#   xlab("") + 
#   ylab("")

df.subreddit.freq <- textstat_frequency(df.subreddit.dfm) %>%
  filter(rank <= 40)
df.subreddit.freq$feature

df.subreddit.cos <- textstat_simil(
  df.subreddit.dfm[, df.subreddit.freq$feature],
  method = "cosine",
  margin = "feature")
df.subreddit.cos.mat <- melt(as.matrix(df.subreddit.cos)) # Convert the object to matrix then to data frame

ggplot(data = df.subreddit.cos.mat, aes(x=Var1, y=Var2, fill=value)) +
  scale_fill_gradient2(
    low = "blue",
    high = "red",
    mid = "white",
    midpoint = 0.5,
    limit = c(0, 1),
    name = "Cosine") +
  geom_tile() + 
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 5),
    axis.text.y = element_text(size = 5)) +
  xlab("") + 
  ylab("")
```

Lots of words are similar in terms of cosine similarity. this is peculiar which means that they are used in a similar proportion through subreddits.

#### Clustering Words

```{r}
#cluster and rotate labels
df.subreddit.hc <- hclust(as.dist(1 - df.subreddit.cos))
plot(df.subreddit.hc)
```

### Attempt 4 - Topic Modelling per subrreddit
#### LSA

```{r}
#select subset of data of the top 10 subreddits for readability purposes
df.top10_subreddit <- df.subreddit %>%
  arrange(desc(nchar(body))) %>%
  filter(row_number() <= 10)

#corpus
df.top10_subreddit.cp <- corpus(df.top10_subreddit$body)
# Assign subreddit names as document identifiers
docnames(df.top10_subreddit.cp) <- df.top10_subreddit$subreddit

#token
df.top10_subreddit.tk <- tokens(
  corpus(df.top10_subreddit$body),
  remove_numbers = TRUE,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_url = TRUE,
  remove_hyphens = TRUE,
  remove_separators = TRUE
)
df.top10_subreddit.tk <- df.top10_subreddit.tk %>%
  tokens_tolower() %>%
  tokens_remove(stop_words$word) %>%
  tokens_remove(c("reddit", "subreddit", "amp", "gt", "deleted", "x+")) %>%
  tokens_replace(
    pattern = hash_lemmas$token,
    replacement = hash_lemmas$lemma)

# Create Document-Feature Matrix (DFM)
df.top10_subreddit.dfm <- dfm(df.top10_subreddit.tk)

# Explicitly set document names in the DFM to the subreddit names
docnames(df.top10_subreddit.dfm) <- df.top10_subreddit$subreddit
df.top10_subreddit.dfm

#use textmodel_lsa() on dfm using n dimensions
df.subreddit.lsa <- textmodel_lsa(
  x = df.top10_subreddit.dfm,
  nd = 5)

head(df.subreddit.lsa$docs)
#interpretation
## we look at the five terms with the largest values and the five ones with the lowest value (i.e., largest negative value)

n.terms <- 5
##for dim 2
w.order <- sort(df.subreddit.lsa$features[, 2], decreasing = TRUE)
w.top2 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))
##for dim 3
w.order <- sort(df.subreddit.lsa$features[, 3], decreasing = TRUE)
w.top3 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))

w.top2
w.top3
```

Topic 2 is associated positively with topic like  (game, team, play, games, fuck, ) and negatively with topics like (questions, question, comments, post, people)

```{r}
#PCA bibplots for links between topics and documents and topcis with terms
w.subset <-
  ##restric chart to terms that are mostly related to dim2 and dim3 so w.top2 and w.top3
  df.subreddit.lsa$features[
    c(unique(c(names(w.top2), names(w.top3)))), 2:3]

biplot(
  y = df.subreddit.lsa$docs[, 2:3],
  x = df.subreddit.lsa$features[,2:3],
  col = c("black","red"),
  cex = c(0.5, 0.5),
  xlab = "Dim 2",
  ylab = "Dim 3")
```
We observe that the word `fuck` is associated with dim 3 and is interestingly negatively associated to `happy` which makes sense.

#### LDA

```{r}
docnames(df.top10_subreddit.dfm) <- df.top10_subreddit$subreddit
set.seed(1234) #To create reproducible results
df.top10_subbredit.lda <- textmodel_lda(
  x = df.top10_subbredit.dfm,
  k = 10)

#extract top 5 terms per topic
seededlda::terms(df.top10_subbredit.lda, 5)

# Extract subreddits per topic and rename output to match subreddit names
top_subreddits <- seededlda::topics(df.top10_subbredit.lda)
names(top_subreddits) <- df.top10_subreddit$subreddit
top_subreddits

#count the number of documents per topic
seededlda::topics(df.top10_subbredit.lda) %>% table()
```

Topic 8 with 3 subreddits ( AskReddit, funny and news) is the most popular topic with words like people, time, fuck, day, feel which may represent human experience and emotions. They touch on aspects of daily life, feelings, and the passage of time, with a bit of raw expression thrown in. It's a mix of the mundane and the profound, capturing the essence of what it means to be human. Indeed, what reddit is about haha

#### Topic-term Analysis

```{r}
#transform into a long df
phi.long <- melt(
  df.top10_subbredit.lda$phi,
  varnames = c("Topic", "Term"),
  value.name = "Phi") 

#plot 10 largest prob terms within each subject
phi.long %>% 
  group_by(Topic) %>% 
  top_n(10, Phi) %>% 
  ggplot(aes(reorder_within(Term, Phi, Topic), Phi)) + 
  geom_col(show.legend = FALSE) +
  coord_flip() +
  facet_wrap(~ Topic, scales = "free_y") +
  scale_x_reordered() + 
  xlab("Term") + 
  theme(
    axis.text.y = element_text(size = 5),
    strip.text = element_text(size = 5))
```
Similar analysis as the to the previous but now we see the probs so more precise

But we observe that lots of topics have the same terms. Therefore, adjusting k topics may be necessary.

We try with 7 topics

```{r}
docnames(df.top10_subreddit.dfm) <- df.top10_subreddit$subreddit
set.seed(1234) #To create reproducible results
df.top10_subbredit.lda <- textmodel_lda(
  x = df.top10_subbredit.dfm,
  k = 7)

#extract top 5 terms per topic
seededlda::terms(df.top10_subbredit.lda, 5)

# Extract subreddits per topic and rename output to match subreddit names
top_subreddits <- seededlda::topics(df.top10_subbredit.lda)
names(top_subreddits) <- df.top10_subreddit$subreddit
top_subreddits

#count the number of documents per topic
seededlda::topics(df.top10_subbredit.lda) %>% table()
#transform into a long df
phi.long <- melt(
  df.top10_subbredit.lda$phi,
  varnames = c("Topic", "Term"),
  value.name = "Phi") 

#plot 10 largest prob terms within each subject
phi.long %>% 
  group_by(Topic) %>% 
  top_n(10, Phi) %>% 
  ggplot(aes(reorder_within(Term, Phi, Topic), Phi)) + 
  geom_col(show.legend = FALSE) +
  coord_flip() +
  facet_wrap(~ Topic, scales = "free_y") +
  scale_x_reordered() + 
  xlab("Term") + 
  theme(
    axis.text.y = element_text(size = 5),
    strip.text = element_text(size = 5))
```

the distinction between topics is more clear now. We observe that topic1 may be related to 'hockey' and topic 2 to 'CFB' both sports so they have similar words expect specific ones. Topic 3 might more be related to 'news' as it has words like link (to refer to a source), interview, story, murder. Topic 4 is more related to feelings with words like time, day, feel, shit, pretty,... but harder to distinguish. Topic 5 might be related to government, nations or world maybe ? with words like police, american, russia, war, country. Topic 6 might well be a question topic with words like comment, contact,.... And topic 7 might more be a PC or PC gaming type of topic with words like game, steam, ram, build. we see that they reflect our subreddit which makes sense, now it would be interesting to take more that 10 subreddit as we might have a interesting regrouping of subreddits.

#### Topic document Analysis

```{r}
set.seed(1234)
theta.long <- melt(
  df.top10_subbredit.lda$theta,
  varnames = c("Doc", "Topic"),
  value.name = "Theta")

# Ensure `theta.long` has the same document order as `df.top10_subreddit`
theta.long$Doc <- rep(df.top10_subreddit$subreddit, each = ncol(df.top10_subreddit.dfm$theta))

theta.long %>% 
  group_by(Topic) %>% 
  top_n(10, Theta) %>% 
  ggplot(aes(reorder_within(Doc, Theta, Topic), Theta)) + 
  geom_col(show.legend = FALSE) +
  coord_flip()+
  facet_wrap(~ Topic, scales = "free_y") +
  scale_x_reordered() + 
  xlab("Document") + 
  theme(
    axis.text.y = element_text(size = 5),
    strip.text = element_text(size = 5))
```
This confirms my previous analysis. we see topic one related to hockey, topic 2 related to CFB but interestingly lol is also here. Topic 3 was in fact reports, topic 4 was indeed feelings as it encompass a large range of subreddits. Topic 5 was indeed related to world news, news in general, topic 6 was indeed questions and topic 7 was indeed PC.

#### LDA diagnostics

```{r}
#topic prevalence
rev(sort(colSums(df.top10_subbredit.lda$theta)/sum(df.top10_subbredit.lda$theta)))
```
Topic 4 most prevalent.

```{r}
#using topicmodels
df.top10_subbredit.LDA <- LDA(
  convert(df.top10_subbredit.dfm, to = "topicmodels"),
  k = 7
)

topicmodels::terms(df.top10_subbredit.LDA, 5)
topicmodels::topics(df.top10_subbredit.LDA)
topicmodels::topics(df.top10_subbredit.LDA) %>% table()

topic_diagnostics(
  topic_model = df.top10_subbredit.LDA, 
  dtm_data = convert(df.top10_subbredit.dfm, to = "topicmodels"))
```

```{r}
#reproduce  term-topic analysis with this package
beta.long <- tidy(
  df.top10_subbredit.LDA,
  matrix = "beta") # equivalent to melt (with this package)

beta.long %>% 
  group_by(topic) %>% 
  top_n(10, beta) %>% 
  ggplot(aes(reorder_within(term, beta, topic), beta)) + 
  geom_col(show.legend = FALSE) +
  coord_flip()+
  facet_wrap(~ topic, scales = "free_y") +
  scale_x_reordered() + 
  xlab("Term") +
  theme(
    axis.text.y = element_text(size = 5),
    axis.text.x = element_text(size = 5),
    strip.text = element_text(size = 5))
```

### Attempt 5 - Regression

We will try to predict the rating of a comment.

```{r}
#focus on col 'body' which contains the written comment and 'score' which is the rating

#statistics of ratings/scores
df %>%
  group_by(score) %>%
  summarise(n = n()) %>%
  ungroup() %>%
  arrange(score)

#print max and min scores
cat("max score", max(df$score), "\n")
cat("min score", min(df$score), "\n")
```

Not very well balanced, goes from 3967 to -302

```{r}
#build a regression model using SVM
##splitting
###select only the columns of interest
df_split <- df %>%
  select(score, body) %>%
  rename(text = body) %>%
  initial_split()

#create training and test sets
df_train <- training(df_split)
df_test <- testing(df_split)

#preprocess the text data
df_rec <- recipe(score ~ text, data = df_train) %>%
  step_tokenize(text) %>%
  step_tokenfilter(text, max_tokens = 1000) %>%
  #remove stopwords
  step_stopwords(text) %>%
  step_tfidf(text)

#create workflow
df_wf <- workflow() %>%
  add_recipe(df_rec)

#define model
svm_spec <- svm_linear() %>%
  set_engine("LiblineaR") %>%
  set_mode("regression")

#fit
svm_fit <- df_wf %>%
  add_model(svm_spec) %>%
  fit(data = df_train)

#cv
set.seed(1234)
df_folds <- vfold_cv(df_train, v = 10)

#fitting resamples
svm_res <- fit_resamples(
  df_wf %>%
    add_model(svm_spec),
  df_folds,
  control = control_resamples(save_pred = TRUE)
)

##collecting metrics
collect_metrics(svm_res)

#visualize the results
svm_res %>%
  collect_predictions() %>%
  ggplot(aes(score, .pred, color = id)) +
  geom_abline(lty = 2, color = "gray80", linewidth = 1.5) +
  geom_point(alpha = 0.3) +
  ylim(c(1, 5)) +
  labs(
    x = "Truth",
    y = "Predicted Score",
    color = NULL,
    title = "Predicted and true comments grades",
    subtitle = "Each cross-validation fold is shown in a different color"
  )
```

Only predict from a range 1-5
- Inbalance in data thus model fail to capture the full range of scores

To improve the performance, experimenting with alternative algorithms that can handle extreme outliers, or resampling techniques to balance the data, may help achieve better predictive accuracy across the entire score range.
