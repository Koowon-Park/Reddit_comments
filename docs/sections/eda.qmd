# EDA

## Text Mining

### Attempt 1

```{r}
# Step 1: Basic text cleaning and preprocessing
# Remove punctuation, convert to lowercase, remove stopwords, etc.
clean_text <- df %>%#
  mutate(body_clean = body %>%
           str_to_lower() %>%                  # Convert text to lowercase
           str_replace_all("[^[:alnum:]\\s]", " ") %>% # Remove punctuation
           str_squish())                       # Remove extra whitespace

# Remove stop words
data(stop_words)  # Load stopwords from tidytext package
clean_text <- clean_text %>%
  unnest_tokens(word, body_clean) %>%
  anti_join(stop_words, by = "word")

# Step 3: Exploratory Data Analysis (EDA)
# Word frequency analysis on training set
word_counts <- clean_text %>%
  count(word, sort = TRUE)

# Visualize top 20 most frequent words
word_counts %>%
  top_n(20, n) %>%
  ggplot(aes(x = reorder(word, n), y = n)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Top 20 Most Frequent Words", x = "Word", y = "Frequency")
```

```{r}
# Prepare document-term matrix (DTM) for LDA
dtm <- clean_text %>%
  count(id, word) %>%                       # Count word frequencies per comment (id)
  cast_dtm(id, word, n)                     # Create a document-term matrix

# Apply LDA to discover topics (time consumming)
#lda_model <- LDA(dtm, k = 5, control = list(seed = 1234))  # Choose 5 topics

# Save the LDA model to an R file
#save(lda_model, file = "lda_model.RData")

#load using here()
load(here("docs/sections/lda_model.RData"))
# Get the terms per topic
topics <- tidy(lda_model, matrix = "beta")

# Visualize the top 10 words per topic
top_terms <- topics %>%
  group_by(topic) %>%
  top_n(8, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ggplot(top_terms, aes(x = reorder_within(term, beta, topic), y = beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol = 5) +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "Top Terms per Topic", x = "Terms", y = "Beta")
```

### Second Attempt - A comment = A document or text?

#### Preprocessing

```{r}
#tokenize text
tokens <- df %>% unnest_tokens(word, body)
head(tokens)
df %>% unnest_tokens(word, body)  %>% 
  count(word, sort = TRUE) %>% 
  mutate(word = reorder(word, n)) %>% 
  filter(n >= 15000) %>% ggplot(aes(n, word)) + geom_col()
```

The `n` value in the `filter` function can be adjusted to show more or fewer words in the plot. This analysis can help identify the most common words in the text data, which can provide insights into the main topics or themes present in the text. Here `n`is set to 15000, which is enormous, because the dataset is large. We see here the most common words are words like "the", "and", "a", etc., which are common stopwords in English text. These words are not very informative for topic modeling or sentiment analysis, so they are typically removed in the preprocessing steps.

##### Creating a corpus

For consistency and a organized structure, we will create a corpus from the text data. A corpus is a collection of text documents that can be used for text analysis and natural language processing tasks. In this case, we will create a corpus where each document corresponds to a comment in the dataset. We will also add metadata to the corpus, such as the author, subreddit, score, controversiality, parent_id, gilded, and edited fields from the original dataset.

```{r}
# Create a corpus from the text data where the text field is "body" but the document field is "subreddit"
corpus <- corpus(df, text_field = "body", docid_field = "id") 

# Add metadata to the corpus
docvars(corpus, "author") <- df$author
docvars(corpus, "subreddit") <- df$subreddit
docvars(corpus, "score") <- df$score
docvars(corpus, "controversiality") <- df$controversiality
docvars(corpus, "parent_id") <- df$parent_id
docvars(corpus, "gilded") <- df$gilded
docvars(corpus, "edited") <- df$edited

summary(corpus)
```

##### Process the data

1.  Tokenization of the commment in `body` column into individual words. Which was already done before

2.  Preprocessing these tokens by converting them to lowercase, stemming, and removing common stop words. For this particular dataset, we know that the articles are coming from Reddit platform, hence the word ‘reddit’ is not useful to us can be removed as a stop word.

```{r}
df.tk <- tokens(
  corpus,
  remove_numbers = TRUE,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_url = TRUE,
  remove_hyphens = TRUE,
  remove_separators = TRUE,
)
df.tk <- df.tk %>%
  tokens_tolower() %>%
  tokens_remove(stop_words$word) %>%
  tokens_remove("reddit")
```

##### See again the most common words

```{r}
df.tk %>%
  dfm() %>%
  topfeatures( n = 20) %>%
  as.data.frame()
```

It looks like some of the words we’re seeing, such as “amp” and “gt”, are artifacts from HTML encoding or other text formatting issues.

There also seems to be a lot of curse words

##### Removing HTML encoding artifacts

```{r}
# Remove HTML tags
corpus <- str_remove_all(corpus, "<[^>]+>")

# Tokenize and preprocess
df.tk <- tokens(
  corpus,
  remove_numbers = TRUE,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_url = TRUE,
  remove_hyphens = TRUE,
  remove_separators = TRUE
)

df.tk <- df.tk %>%
  tokens_tolower() %>%
  tokens_remove(stop_words$word) %>%
  tokens_remove("reddit") %>%
  tokens_remove("subreddit") %>%
  tokens_remove("amp") %>%
  tokens_remove("gt") %>%
  tokens_remove("deleted") %>%
  # remove any 'xxxx' characters
  tokens_remove("x+")
```

##### See again the most common words

```{r}
df.tk %>%
  dfm() %>%
  topfeatures( n = 20) %>%
  as.data.frame()
```

#### TF and TF-IDF

##### Term Frequency (TF)

Now, let us compute the word frequencies (TF) and arrange them by decreasing frequencies. For the Document-Term Matrix, quanteda uses dfm() objects like below (for document frequency matrix).

```{r}
df.dfm <- dfm(df.tk)
df.dfm

#extract the TF
tf <- rowSums(t(df.dfm))
tf <- data.frame(term = names(tf), count = tf) %>%
  tibble() %>%
  arrange(desc(count))
head(tf)
```

##### Frequency per document (DFM)

Let us compute the frequencies by documents. We can use tidy function directly on the crude.dfm object to turn it into a dataframe and view the results.

```{r}
tidy(df.dfm) %>%
  arrange(desc(count))

## compute global term frequency
df.freq <- textstat_frequency(df.dfm)
head(df.freq,20)
```

##### TF-IDF

Now, we repeat the same analysis using the tf-idf formula. We use function bind_tf_idf() from tidytext. The results are ordered by decreasing TF_IDF. Note that the TF_IDF is calculated per document.

```{r}
df.tfidf <- dfm_tfidf(df.dfm)
df.tfidf.tidy <- tidy(df.tfidf) %>%
  bind_tf_idf(term = term, document = document, n = count) %>%
  arrange(desc(tf_idf))
head(df.tfidf.tidy, n = 10) %>% flextable() %>% autofit()
```

Now, we take the max TF_IDF per words (over all documents):

```{r}
df.tfidf.max <- df.tfidf.tidy %>%
  group_by(term) %>%
  summarise(tf_idf = max(tf_idf)) %>%
  ungroup() %>%
  arrange(desc(tf_idf))
head(df.tfidf.max, n = 10) %>% flextable() %>% autofit()
```

##### Plotting the Results

```{r}
df.freq %>%
  top_n(20, frequency) %>%
  ggplot(aes(
    x = reorder(feature, frequency),
    y = frequency)) + 
  geom_bar(stat = "identity") + 
  coord_flip() +
  xlab("Frequency") + 
  ylab("term")

textplot_wordcloud(df.dfm)

df.freq %>%
  top_n(50, frequency) %>%
  ggplot(aes(label = feature, size = frequency)) +
  geom_text_wordcloud() +
  scale_size_area() +
  theme_minimal()

#plot per documents
df.dfm %>%
  tidy() %>%
  top_n(10, count) %>%
  ggplot(aes(x = term, y = count)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  theme(axis.text.y = element_text(size = 8), 
        axis.ticks.y = element_blank()) +
  facet_wrap(~document, ncol = 5)
```

Maybe not really relevant as we have a lot of documents/comments ?

### Attempt 3 - A subreddit = A document

Here i take the idea that a 'document' is a subbredit. It is like all comments that are part of one subreddit will be a document. It is a bit an analysis per subreddit

```{r fig.width=10, fig.height=10}}
#take the df data and group all comments by subreddit
df.subreddit <- df %>%
  group_by(subreddit) %>%
  summarise(body = paste(body, collapse = " "))

# show number of subreddits
cat("Number of subreddits:", nrow(df.subreddit), "\n")

# show top 10 subreddits by length in a bar plot
df.subreddit %>%
  mutate(subreddit = reorder(subreddit, nchar(body))) %>%
  top_n(10, nchar(body)) %>%
  ggplot(aes(x = subreddit, y = nchar(body))) +
  geom_col() +
  coord_flip() +
  labs(title = "Top 10 Subreddits by Length", x = "Subreddit", y = "Length")

# preprocess the text data
df.subreddit.cp <- corpus(df.subreddit$body)
docnames(df.subreddit.cp) <- df.subreddit$subreddit  # Assign subreddit names as document identifiers

df.subreddit.tk <- tokens(
  df.subreddit.cp,
  remove_numbers = TRUE,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_url = TRUE,
  remove_hyphens = TRUE,
  remove_separators = TRUE
)
df.subreddit.tk <- df.subreddit.tk %>%
  tokens_tolower() %>%
  tokens_remove(stop_words$word) %>%
  tokens_remove(c("reddit", "subreddit", "amp", "gt", "deleted", "x+"))

# tf and tf-idf analysis
#tf
df.subreddit.dfm <- dfm(df.subreddit.tk)
tf <- rowSums(t(df.subreddit.dfm))
tf <- data.frame(term = names(tf), count = tf) %>%
  tibble() %>%
  arrange(desc(count))

#freq per documents
tidy(df.subreddit.dfm) %>%
  arrange(desc(count))

#freq per term
df.subreddit.freq <- textstat_frequency(df.subreddit.dfm)
head(df.subreddit.freq,20)

#plot 20 most frequent words
df.subreddit.freq %>%
  top_n(20, frequency) %>%
  ggplot(aes(
    x = reorder(feature, frequency),
    y = frequency)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  xlab("Frequency") +
  ylab("term")

textplot_wordcloud(df.subreddit.dfm)

df.subreddit.freq %>%
  top_n(20, frequency) %>%
  ggplot(aes(label = feature, size = frequency)) +
  geom_text_wordcloud() +
  scale_size_area(max_size = 20) +
  theme_minimal()

#tf-idf
df.subreddit.tfidf <- dfm_tfidf(df.subreddit.dfm)
df.subreddit.tfidf.tidy <- tidy(df.subreddit.tfidf) %>%
  bind_tf_idf(term = term, document = document, n = count) %>%
  arrange(desc(tf_idf))
head(df.subreddit.tfidf.tidy, n = 20) %>% flextable() %>% autofit()

#plot per document
df.subreddit.dfm %>%
  tidy() %>%
  top_n(30, count) %>%
  ggplot(aes(x = term, y = count)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  theme(axis.text.y = element_text(size = 6),
        axis.ticks.y = element_blank()) +
  facet_wrap(~document, ncol = 5)
```

-   We observe that AskReddit has lots of words frequency which makes sense as it is the biggest subbredit by lenght.
-   We also observe that the term 'game' is associated in term of frequency with sports Subbredits like CFB (College Football) and Hockey which makes sense.
-   Interestingly the subreddits LoL (League of Legends) is associated with the term 'happy', which show us that the community may be the happiest, careful bias here, it is just a word frequency analysis.
- In the subreddits 'pics' we observe a lot of 'gem' term which indicates treasure or something valuable. This is interesting as it is a subreddit for sharing pictures.
- The subbredits 'news' is associated in terms of frequency with the term 'people' which is logical as news is about people.

#### Similarity between Subreddits/Documents

we analyze similarities and dissimilarities between the documents (through words) in the data. We use quanteda extensively. We’ll use the objects created previously in the exercises.

Then use the functions textstat_simil() and textstat_dist() to compute the Jaccard index matrix, the cosine matrix, and the Euclidean distances matrix.

```{r}
#We restrict ourselves to a small subset of the highest tf-idf of the data to avoid memory issues and readability issues in the plots.
df.subreddit.tfidf.small <- df.subreddit.tfidf[1:50,]
df.subreddit.tfidf.small

df.subbredit.jac <- textstat_simil(
  df.subreddit.tfidf.small, method = "jaccard", margin = "documents")

df.subbredit.cos <- textstat_simil(
  df.subreddit.tfidf.small, method = "cosine", margin = "documents")

df.subbredit.eucl <- textstat_dist(
  df.subreddit.tfidf.small, method = "euclidean", margin = "documents")

#heatmap representation of similariteis between subreddits
## jaccard
df.subbredit.jac.mat <- melt(as.matrix(df.subbredit.jac))
ggplot( data = df.subbredit.jac.mat, 
        mapping = aes(x = Var1, y = Var2, fill = value)) +
  scale_fill_gradient2(
    low = "blue", 
    high = "red", 
    mid = 'white', 
    midpoint = 0.5, 
    limit = c(0,1), 
    name = "Jaccard") +
  geom_tile() + xlab("") + ylab("") +
  #incline x-axis labels
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

## cosine
df.subbredit.cos.mat <- melt(as.matrix(df.subbredit.cos))
ggplot( data = df.subbredit.cos.mat, 
        mapping = aes(x = Var1, y = Var2, fill = value)) +
  scale_fill_gradient2(
    low = "blue", 
    high = "red", 
    mid = 'white', 
    midpoint = 0.5, 
    limit = c(0,1), 
    name = "Cosine") +
  geom_tile() + xlab("") + ylab("") +
  #incline x-axis labels
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

## euclidean
df.subbredit.eucl.mat <- melt(as.matrix(df.subbredit.eucl))
ggplot( data = df.subbredit.eucl.mat, 
        mapping = aes(x = Var1, y = Var2, fill = value)) +
  scale_fill_gradient2(
    low = "blue", 
    high = "red", 
    mid = 'white', 
    midpoint = 0.5, 
    limit = c(0,1), 
    name = "Euclidean") +
  geom_tile() + xlab("") + ylab("") +
  #incline x-axis labels
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

#### Clustering of Documents

Hierarchical clustering and K-means. The first one is applied on the dissimilarities (Euclidean, inverted Jaccard, and inverted cosine). The second one is applied on the features, here, TF-IDF. To illustrate the methods, we decide to create five clusters.

```{r}
#hierarchical
df.subbredit.hc <- hclust(as.dist(df.subbredit.eucl))
## df.subbredit.hc <- hclust(as.dist(1 - df.subbredit.jac)) # use this line for Jaccard
## df.subbredit.hc <- hclust(as.dist(1 - df.subbredit.cos)) # use this line for Cosine
plot(df.subbredit.hc)
df.subbredit.clust <- cutree(df.subbredit.hc, k = 3)
df.subbredit.clust
#k-means

df.subbredit.km <- kmeans(df.subreddit.tfidf.small, centers = 3)
df.subbredit.km$cluster
```

#### Similarities between words

In this part we analyze similarities between words (through documents). We restrict ourselves to a subset corresponding to words with frequency rank less than 40 (it should correspond to the 40 most frequent words but several words have the same frequency rank). We use the cosine similarity and plot the heatmap.

```{r}
# crude.feat <- textstat_frequency(crude.dfm) %>%
#   filter(rank <= 40) 
# crude.feat$feature
# 
# crude.cos <- textstat_simil(
#   crude.dfm[, crude.feat$feature],
#   method = "cosine",
#   margin = "feature")
# crude.cos.mat <- melt(as.matrix(crude.cos)) # Convert the object to matrix then to data frame 
# 
# ggplot(data = crude.cos.mat, aes(x=Var1, y=Var2, fill=value)) +
#   scale_fill_gradient2(
#     low = "blue",
#     high = "red",
#     mid = "white",
#     midpoint = 0.5,
#     limit = c(0, 1),
#     name = "Cosine") +
#   geom_tile() + 
#   theme(
#     axis.text.x = element_text(angle = 45, hjust = 1, size = 5),
#     axis.text.y = element_text(size = 5)) +
#   xlab("") + 
#   ylab("")

df.subreddit.freq <- textstat_frequency(df.subreddit.dfm) %>%
  filter(rank <= 40)
df.subreddit.freq$feature

df.subreddit.cos <- textstat_simil(
  df.subreddit.dfm[, df.subreddit.freq$feature],
  method = "cosine",
  margin = "feature")
df.subreddit.cos.mat <- melt(as.matrix(df.subreddit.cos)) # Convert the object to matrix then to data frame

ggplot(data = df.subreddit.cos.mat, aes(x=Var1, y=Var2, fill=value)) +
  scale_fill_gradient2(
    low = "blue",
    high = "red",
    mid = "white",
    midpoint = 0.5,
    limit = c(0, 1),
    name = "Cosine") +
  geom_tile() + 
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 5),
    axis.text.y = element_text(size = 5)) +
  xlab("") + 
  ylab("")
```

Lots of words are similar in terms of cosine similarity. this is peculiar which means that they are used in a similar proportion through subreddits.

#### Clustering Words

```{r}
#cluster and rotate labels
df.subreddit.hc <- hclust(as.dist(1 - df.subreddit.cos))
plot(df.subreddit.hc)
```

#### Co-occurences

The procedure below is quite similar to the one with cosine. We first compute the co-occurrence (that we make symmetrical), then represent this matrix on a heatmap, then run the clustering algorithm, and finally try to represent it on a map.

The function below makes a co-occurrence on a (half-)window of 3. Note that the object needs to be the tokens because co-occurrences needs the token order and thus cannot be computed on a BOW object (like DTM).

```{r}
# crude.fcm <- fcm(crude.tk, 
#                  window = 3, 
#                  tri = FALSE)
# crude.fcm <- (crude.fcm + t(crude.fcm))/2 ## make the co-occurrence matrix symmetrical

df.subreddit.fcm <- fcm(df.subreddit.tk, 
                 window = 3, 
                 tri = FALSE)
df.subreddit.fcm <- (df.subreddit.fcm + t(df.subreddit.fcm))/2 ## make the co-occurrence matrix symmetrical
```

