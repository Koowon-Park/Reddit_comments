```{r viridis, eval=FALSE, echo=FALSE}
#save some viridis colors
viridis_1 <- viridis(10)[1]
viridis_2 <- viridis(10)[2]
viridis_3 <- viridis(10)[3]
viridis_4 <- viridis(10)[4]
viridis_5 <- viridis(10)[5]
viridis_6 <- viridis(10)[6]
viridis_7 <- viridis(10)[7]
viridis_8 <- viridis(10)[8]
viridis_9 <- viridis(10)[9]
viridis_10 <- viridis(10)[10]

#plot each color side by side to observe them
plot(1:10, rep(1, 10), col = viridis(10), pch = 19, cex = 2, xlab = "Color", ylab = "Color", xlim = c(0, 11), ylim = c(0, 2), xaxt = "n", yaxt = "n")
```

# EDA
## Text Mining
### A subreddit = A document

Here i take the idea that a 'document' is a subreddit. It is like all comments that are part of one subreddit will be a document. It is a bit an analysis per subreddit

```{r top_10_sub, eval=TRUE, echo=TRUE}
#take the df data and group all comments by subreddit
df.subreddit <- df %>%
  group_by(subreddit) %>%
  summarise(body = paste(body, collapse = " "))

# show number of subreddits
cat("Number of subreddits:", nrow(df.subreddit), "\n")

# show top 10 subreddits by length in a bar plot
df.subreddit %>%
  mutate(subreddit = reorder(subreddit, nchar(body), fill = nchar(body))) %>%
  top_n(10, nchar(body)) %>%
  ggplot(aes(x = subreddit, y = nchar(body))) +
  geom_col(fill = viridis_1) +
  coord_flip() +
  labs(title = "Top 10 Subreddits by Length", x = "Subreddit", y = "Length") +
  #increase text size
  theme(axis.text.y = element_text(size = 10),
        axis.text.x = element_text(size = 10))
```

We observe that AskReddit is by far the largest Subreddit

```{r word_freq, eval=TRUE, echo=TRUE, cache=TRUE}
# preprocess the text data
df.subreddit.cp <- corpus(df.subreddit$body)
docnames(df.subreddit.cp) <- df.subreddit$subreddit  # Assign subreddit names as document identifiers

df.subreddit.tk <- tokens(
  df.subreddit.cp,
  remove_numbers = TRUE,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_url = TRUE,
  remove_hyphens = TRUE,
  remove_separators = TRUE
)
df.subreddit.tk <- df.subreddit.tk %>%
  tokens_tolower() %>%
  tokens_remove(stop_words$word) %>%
  tokens_remove(c("reddit", "subreddit", "amp", "gt", "deleted", "x+"))

# tf and tf-idf analysis
#tf
df.subreddit.dfm <- dfm(df.subreddit.tk)
tf <- rowSums(t(df.subreddit.dfm))
tf <- data.frame(term = names(tf), count = tf) %>%
  tibble() %>%
  arrange(desc(count))

#freq per documents
tidy(df.subreddit.dfm) %>%
  arrange(desc(count))

#freq per term
df.subreddit.freq <- textstat_frequency(df.subreddit.dfm)
head(df.subreddit.freq,20)

#plot 20 most frequent words
df.subreddit.freq %>%
  top_n(20, frequency) %>%
  ggplot(aes(
    x = reorder(feature, frequency),
    y = frequency)) +
  geom_bar(stat = "identity", fill = viridis_2) +
  coord_flip() +
  xlab("Frequency") +
  ylab("term") +
  #increase size of text
  theme(axis.text.y = element_text(size = 10),
        axis.text.x = element_text(size = 10))

```

We observe here a lot of abbreviations that are not taken into account by the stop words. Indeed, lots of word are just because the author removed the '. So for example. i'm becomes im or you're becomes youre.

```{r textplot_wordcloud, eval=TRUE, echo=TRUE}
#textplot_wordcloud(df.subreddit.dfm)

df.subreddit.freq %>%
  top_n(30, frequency) %>%
  ggplot(aes(label = feature, size = frequency, color = frequency)) +
  geom_text_wordcloud() +
  scale_size_area(max_size = 14) +
  scale_color_viridis_c() +
  theme_minimal()
```

We can observe that the most frequent words are 'people', 'time', 'game'.

```{r tf_idf, eval=TRUE, echo=TRUE}
#tf-idf
df.subreddit.tfidf <- dfm_tfidf(df.subreddit.dfm)
df.subreddit.tfidf.tidy <- tidy(df.subreddit.tfidf) %>%
  bind_tf_idf(term = term, document = document, n = count) %>%
  arrange(desc(tf_idf))
head(df.subreddit.tfidf.tidy, n = 20) %>% flextable() %>% autofit()
```

So tf_idf shows us rare words that are used in a specific subreddit. So we have here lot's of 'small' and 'rare' subreddit that have specific words that are not used in other subreddit. This is interesting as it shows us the specificity of each subreddit.

```{r tf_idf_plot, eval=TRUE, echo=TRUE}
#plot per document
df.subreddit.dfm %>%
  tidy() %>%
  top_n(12, count) %>%
  ggplot(aes(x = term, y = count)) +
  geom_bar(stat = "identity", fill = viridis_3) +
  coord_flip() +
  theme(axis.text.y = element_text(size = 10),
        axis.ticks.y = element_blank()) +
  facet_wrap(~document, ncol = 3)
```

So in summary: 
-   We observe that AskReddit has lots of words frequency which makes sense as it is the biggest subreddit by lenght.
-   We also observe that the term 'game' is associated in term of frequency with sports subreddits like CFB (College Football) and Hockey which makes sense.
-   Interestingly the subreddits LoL (League of Legends) is associated with the term 'happy', which show us that the community may be the happiest, careful bias here, it is just a word frequency analysis.
- In the subreddits 'pics' we observe a lot of 'gem' term which indicates treasure or something valuable. This is interesting as it is a subreddit for sharing pictures.
- The subreddits 'news' is associated in terms of frequency with the term 'people' which is logical as news is about people.

#### Similarity between Subreddits/Documents

we analyze similarities and dissimilarities between the documents (through words) in the data. We use quanteda extensively. Weâ€™ll use the objects created previously in the exercises.

Then use the functions textstat_simil() and textstat_dist() to compute the Jaccard index matrix. We choose the Jaccard index  because it is a good measure of similarity between two documents.

```{r similarity_subreddits, eval=TRUE, echo=TRUE, cache=TRUE}
#We restrict ourselves to a small subset of the highest tf-idf of the data to avoid memory issues and readability issues in the plots.
df.subreddit.tfidf.small <- df.subreddit.tfidf[1:1000,]
df.subreddit.tfidf.small

#compute the jaccard index
#comment out the next line to avoid recomputing the jaccard index
# df.subreddit.jac <- textstat_simil(
#   df.subreddit.tfidf.small, method = "jaccard", margin = "documents")

#save the resulted jac so for the next time we don't have to recompute it
#write.csv(df.subreddit.jac, "../../data/jac_1000.csv")

#LOAD THE DATA
df.subreddit.jac <- read.csv("../../data/jac_1000.csv")

#heatmap representation of similariteis between subreddits
## jaccard
p <- ggplot(data = df.subreddit.jac.mat, 
            mapping = aes(x = Var1, y = Var2, fill = value, text = paste("Subreddit 1:", Var1, "<br>Subreddit 2:", Var2, "<br>Jaccard:", round(value, 2)))) +
  scale_fill_gradient2(
    low = viridis_1, 
    high = viridis_5, 
    mid = viridis_10, 
    midpoint = 0.5, 
    limit = c(0, 1), 
    name = "Jaccard") +
  geom_tile() + 
  xlab("") + 
  ylab("") +
  theme(axis.text.x = element_blank(),  # Remove x-axis text
        axis.text.y = element_blank(),  # Remove y-axis text
        axis.ticks = element_blank())   # Remove axis ticks

# Convert ggplot to plotly for interactivity
p_interactive <- ggplotly(p, tooltip = "text")

# Display the interactive plot
p_interactive
```

We observe that similarity using Jaccard index on the top 1000 tf-idf words is inconclusive. Indeed, we do not observe any clear pattern. This is due to the fact that we have a lot of words that are specific to each subreddit. Therefore, we will try to cluster the subreddits to see if we can regroup them in a more meaningful way.

#### Clustering of Documents

Hierarchical clustering and K-means. The first one is applied on the dissimilarities (Euclidean). The second one is applied on the features, here, TF-IDF. To illustrate the methods, we decide to create 3 clusters.

```{r clustering_subreddits, eval=TRUE, echo=TRUE}
#hierarchical
df.subreddit.hc <- hclust(as.dist(df.subreddit.eucl))
## df.subreddit.hc <- hclust(as.dist(1 - df.subreddit.jac)) # use this line for Jaccard
## df.subreddit.hc <- hclust(as.dist(1 - df.subreddit.cos)) # use this line for Cosine
plot(df.subreddit.hc)
df.subreddit.clust <- cutree(df.subreddit.hc, k = 3)
df.subreddit.clust
#k-means

df.subreddit.km <- kmeans(df.subreddit.tfidf.small, centers = 3)
df.subreddit.km$cluster
```

Again, we observe that the clustering is not very clear. We have a lot of subreddits that are not well regrouped. This is due to the fact that we have a lot of specific words in each subreddit. Therefore, we will try to use topic modeling to see if we can regroup the subreddits in a more meaningful way.

### Topic Modelling per subrreddit
#### LSA

```{r lsa_subreddit, eval=TRUE, echo=TRUE}
#select subset of data of the top 10 subreddits for readability purposes
df.top10_subreddit <- df.subreddit %>%
  arrange(desc(nchar(body))) %>%
  filter(row_number() <= 10)

#corpus
df.top10_subreddit.cp <- corpus(df.top10_subreddit$body)
# Assign subreddit names as document identifiers
docnames(df.top10_subreddit.cp) <- df.top10_subreddit$subreddit

#token
df.top10_subreddit.tk <- tokens(
  corpus(df.top10_subreddit$body),
  remove_numbers = TRUE,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_url = TRUE,
  remove_hyphens = TRUE,
  remove_separators = TRUE
)
df.top10_subreddit.tk <- df.top10_subreddit.tk %>%
  tokens_tolower() %>%
  tokens_remove(stop_words$word) %>%
  tokens_remove(c("reddit", "subreddit", "amp", "gt", "deleted", "x+")) %>%
  tokens_replace(
    pattern = hash_lemmas$token,
    replacement = hash_lemmas$lemma)

# Create Document-Feature Matrix (DFM)
df.top10_subreddit.dfm <- dfm(df.top10_subreddit.tk)

# Explicitly set document names in the DFM to the subreddit names
docnames(df.top10_subreddit.dfm) <- df.top10_subreddit$subreddit
df.top10_subreddit.dfm

#use textmodel_lsa() on dfm using n dimensions
df.subreddit.lsa <- textmodel_lsa(
  x = df.top10_subreddit.dfm,
  nd = 5)

head(df.subreddit.lsa$docs)
#interpretation
## we look at the five terms with the largest values and the five ones with the lowest value (i.e., largest negative value)

n.terms <- 5
##for dim 2
w.order <- sort(df.subreddit.lsa$features[, 2], decreasing = TRUE)
w.top2 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))
##for dim 3
w.order <- sort(df.subreddit.lsa$features[, 3], decreasing = TRUE)
w.top3 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))

w.top2
w.top3
```

Topic 2 is associated positively with topic like  (game, team, play, games, fuck, ) and negatively with topics like (questions, question, comments, post, people)

We restrict here the PCA for links between subreddits and topics to only dimension 2 and 3 to obersve the efficacy of it.

```{r pca, cache=TRUE, eval=TRUE, echo=TRUE}
# Restrict chart to terms that are mostly related to dim2 and dim3 
w.subset <- df.subreddit.lsa$features[ c(unique(c(names(w.top2), names(w.top3)))), 2:3]
# Create data frames for documents and features
docs_df <- as.data.frame(df.subreddit.lsa$docs[, 2:3])
features_df <- as.data.frame(w.subset) # to show all features (words) use instead : #features_df <- as.data.frame(df.subreddit.lsa$features[, 2:3])

# Rename columns for clarity
colnames(docs_df) <- c("Dim2", "Dim3")
colnames(features_df) <- c("Dim2", "Dim3")

# Create the ggplot biplot
p <- ggplot() +
  geom_segment(data = docs_df, 
               aes(x = 0, y = 0, xend = Dim2, yend = Dim3, text = rownames(docs_df)), 
               arrow = arrow(length = unit(0.2, "cm")), 
               color = "red") +
  geom_point(data = features_df, 
             aes(x = Dim2, y = Dim3, text = rownames(features_df)), 
             color = "black", size = 1) +
  xlab("Dim 2") +
  ylab("Dim 3") +
  theme_minimal()

# Convert ggplot to plotly for interactivity
p_interactive <- ggplotly(p, tooltip = "text")

# Display the interactive plot
p_interactive
```

We only show the features (words) that are mostly related to dim2 and dim3 so that the chart is not overcrowded.

We observe that the word `fuck` on the top right is associated with dim 3 and is interestingly negatively associated to `happy` on the bottom left which makes sense in a sense.

This is more convincing than the similarity and clustering analysis we did before. We observe that the subreddits are well regrouped in a more meaningful way.

#### LDA

```{r lda_subreddit, cache=TRUE, eval=TRUE, echo=TRUE}
docnames(df.top10_subreddit.dfm) <- df.top10_subreddit$subreddit
set.seed(1234) #To create reproducible results
df.top10_subbredit.lda <- textmodel_lda(
  x = df.top10_subreddit.dfm,
  k = 10)

#extract top 5 terms per topic
top_terms <- seededlda::terms(df.top10_subbredit.lda, 5)
top_terms
# Extract subreddits per topic and rename output to match subreddit names
top_subreddits <- seededlda::topics(df.top10_subbredit.lda)
names(top_subreddits) <- df.top10_subreddit$subreddit
top_subreddits

#count the number of documents per topic
topic_counts <- seededlda::topics(df.top10_subbredit.lda) %>% table
topic_counts
```

Topic 5 with 4 subreddits ( AskReddit, funny, pics and news) is the most popular topic with words like people, time, im, youre which may represent human experience and emotions. They touch on aspects of daily life, feelings, and the passage of time, with a bit of raw expression thrown in. It's a mix of the mundane and the profound, capturing the essence of what it means to be human. 
Lol
However, the distinction between topics is not very clear. We observe that lots of topics have the same terms.

#### Topic-term Analysis

```{r topic_term_analysis, eval=TRUE, echo=TRUE}
#transform into a long df
phi.long <- melt(
  df.top10_subbredit.lda$phi,
  varnames = c("Topic", "Term"),
  value.name = "Phi") 

#plot 10 largest prob terms within each subject
phi.long %>% 
  group_by(Topic) %>% 
  top_n(10, Phi) %>% 
  ggplot(aes(reorder_within(Term, Phi, Topic), Phi)) + 
  geom_col(show.legend = FALSE) +
  coord_flip() +
  facet_wrap(~ Topic, scales = "free_y") +
  scale_x_reordered() + 
  xlab("Term") + 
  theme(
    axis.text.y = element_text(size = 5),
    strip.text = element_text(size = 5))
```

Similar analysis as the to the previous but now we see the probs so more precise

But we observe that lots of topics have the same terms. Therefore, adjusting k topics may be necessary.

We try with 7 topics

```{r topic_term_with_7, cache=TRUE, eval=TRUE, echo=TRUE}
docnames(df.top10_subreddit.dfm) <- df.top10_subreddit$subreddit
set.seed(1234) #To create reproducible results
df.top10_subbredit.lda <- textmodel_lda(
  x = df.top10_subreddit.dfm,
  k = 7)

#extract top 5 terms per topic
seededlda::terms(df.top10_subbredit.lda, 5)

# Extract subreddits per topic and rename output to match subreddit names
top_subreddits <- seededlda::topics(df.top10_subbredit.lda)
names(top_subreddits) <- df.top10_subreddit$subreddit
top_subreddits

#count the number of documents per topic
seededlda::topics(df.top10_subbredit.lda) %>% table()
#transform into a long df
phi.long <- melt(
  df.top10_subbredit.lda$phi,
  varnames = c("Topic", "Term"),
  value.name = "Phi") 

#plot 10 largest prob terms within each subject
phi.long %>% 
  group_by(Topic) %>% 
  top_n(10, Phi) %>% 
  ggplot(aes(reorder_within(Term, Phi, Topic), Phi, fill = as.factor(Topic))) + 
  geom_col(show.legend = FALSE) +
  coord_flip() +
  facet_wrap(~ Topic, scales = "free_y") +
  scale_x_reordered() + 
  scale_fill_viridis_d() +
  xlab("Term") + 
  theme(
    axis.text.y = element_text(size = 5),
    strip.text = element_text(size = 5))
```

the distinction between topics is more clear now. We observe that topic 1 may be related to 'pc games'. Topic 2 is quite difficult to name, might be something related to Lifestyle or Achievements. Topic 5 might more be related to crime, societal issues, or law enforcement so something like 'Crime and Justice' as it has words like country, polics, story, law, or  world maybe ?  Topic 4 is more related to feelings with words like time, day, feel,... but harder to distinguish. Topic 6 might well be a question topic with words like question, contact, moderator, answer. And topic 7 and 3 might be more related to sports like hockey or CFB.

We see that they reflect our subreddit which makes sense, now it would be interesting to take more that 10 subreddit as we might have a interesting regrouping of subreddits but it wouldn't be feasible as it is quite computational intensive

#### Topic document Analysis

Now we will analyze the distribution of topics across the documents (subreddits). This differs from the previous analysis, where we looked at the distribution of terms (words) across topics. Here, we look at the distribution of topics across documents (subreddits).

```{r topic_doc_analysis, eval=TRUE, echo=TRUE}
set.seed(1234)
theta.long <- melt(
  df.top10_subbredit.lda$theta,
  varnames = c("Doc", "Topic"),
  value.name = "Theta")

# Ensure `theta.long` has the same document order as `df.top10_subreddit`
theta.long$Doc <- rep(df.top10_subreddit$subreddit, each = ncol(df.top10_subreddit.dfm$theta))

theta.long %>% 
  group_by(Topic) %>% 
  top_n(10, Theta) %>% 
  ggplot(aes(reorder_within(Doc, Theta, Topic), Theta)) + 
  geom_col(show.legend = FALSE) +
  coord_flip()+
  facet_wrap(~ Topic, scales = "free_y") +
  scale_x_reordered() + 
  xlab("Document") + 
  theme(
    axis.text.y = element_text(size = 5),
    strip.text = element_text(size = 5))
```

This confirms my previous analysis. we see topic one related to pc / games, topic 2 related to funny and pics. Topic 3 and 7 to sports, topic 4 was indeed feelings as it encompass a large range of subreddits. Topic 5 was indeed related to world, news in general, topic 6 was indeed questions.

#### LDA diagnostics

```{r topic_diagnostics, eval=TRUE, echo=TRUE}
#topic prevalence
rev(sort(colSums(df.top10_subbredit.lda$theta)/sum(df.top10_subbredit.lda$theta)))
```
Topic 4 most prevalent by far which makes sense as it is related to feelings and human experience that people often use in their comments.

#### Summary

So so far we can fairly say that the LDA is more efficient than the clustering and similarity analysis we did before. We observe that the subreddits are well regrouped in a more meaningful way. We observe that the topics are well defined and that they reflect our subreddit which makes sense. We can see that the topics are well distributed across the documents (subreddits) which is a good sign.

However, regrouping ALL the coments per Subreddit is not the best way to analyze the data. Indeed, we have a lot of specific words that are not taken into account by the stop words. 

Therefore, we will try to regroup the comments in a more meaningful way.

Indeed, we have observed a 'description' and 'display name' of the subreddit. We will use this information to regroup the subreddits in a more meaningful way via web scraping.

### Extract description and display name of subreddit

better method to scrape data
```{r, eval=FALSE}
# Function to scrape display-name and description
get_subreddit_metadata <- function(subreddit) {
  # Construct the URL
  url <- paste0("https://www.reddit.com/r/", subreddit, "/")
  
  tryCatch({
    # Make the GET request with a User-Agent
    response <- GET(url, add_headers("User-Agent" = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"))
    
    # Check HTTP status code
    if (status_code(response) != 200) {
      cat(paste("Failed to fetch:", subreddit, "Status Code:", status_code(response), "\n"))
      return(data.frame(
        subreddit = subreddit,
        display_name = NA,
        description = NA,
        stringsAsFactors = FALSE
      ))
    }
    
    # Parse the HTML content
    page <- read_html(content(response, "text"))
    
    # Extract the <shreddit-subreddit-header> element
    header_element <- page %>%
      html_node("shreddit-subreddit-header")
    
    # Check if the element exists
    if (is.null(header_element)) {
      cat(paste("No header element found for subreddit:", subreddit, "\n"))
      return(data.frame(
        subreddit = subreddit,
        display_name = NA,
        description = NA,
        stringsAsFactors = FALSE
      ))
    }
    
    # Extract the attributes
    display_name <- header_element %>% html_attr("display-name")
    description <- header_element %>% html_attr("description")
    
    # Return the metadata
    return(data.frame(
      subreddit = subreddit,
      display_name = display_name,
      description = description,
      stringsAsFactors = FALSE
    ))
  }, error = function(e) {
    cat(paste("Error for subreddit:", subreddit, "with error:", e$message, "\n"))
    return(data.frame(
      subreddit = subreddit,
      display_name = NA,
      description = NA,
      stringsAsFactors = FALSE
    ))
  })
}

# Example usage with delays
subreddit_names <- tail(df.subreddit$subreddit, 2190)
#print(subreddit_names)

request_counter <- 0

# Introduce random delays between requests
results <- do.call(rbind, lapply(subreddit_names, function(subreddit) {
  # Increment the request counter
  request_counter <<- request_counter + 1
  
  # Print the current request number
  cat(paste("Request number:", request_counter, "\n"))
  
  # Randomize sleep time between 2 and 4 seconds
  sleep_time <- runif(1, min = 2, max = 3.8)
  Sys.sleep(sleep_time) # Delay between requests
  
  # Fetch metadata
  get_subreddit_metadata(subreddit)
}))

# View results
print(results)


```

old method to scrape data
```{r, eval=FALSE}
# Example list of subreddit names (replace this with your dataframe column)
#subreddit_names <- head(df.subreddit$subreddit, 1000)
# Select rows 1001 to 3500
subreddit_names <- df.subreddit$subreddit[5715:nrow(df.subreddit)]
#subreddit_names <- c("100yearsago", "AskReddit", "nonexistent_subreddit", "private_subreddit")

# Function to scrape display-name and description
get_subreddit_metadata <- function(subreddit) {
  # Construct the URL
  url <- paste0("https://www.reddit.com/r/", subreddit, "/")
  
  # Try to scrape the metadata
  tryCatch({
    # Read the HTML content of the subreddit page
    page <- read_html(url)
    
    # Extract the <shreddit-subreddit-header> element
    header_element <- page %>%
      html_node("shreddit-subreddit-header")  # Target the specific element
    
    # Extract the attributes
    display_name <- header_element %>% html_attr("display-name")
    description <- header_element %>% html_attr("description")
    
    # Return the results
    return(data.frame(
      subreddit = subreddit,
      print("sub"),
      display_name = display_name,
      description = description,
      stringsAsFactors = FALSE
    ))
  }, error = function(e) {
    # If an error occurs (e.g., subreddit doesn't exist), return NA
    return(data.frame(
      subreddit = subreddit,
      display_name = NA,
      description = NA,
      stringsAsFactors = FALSE
    ))
  })
}

# Introduce a delay between requests
subreddit_metadata <- do.call(rbind, lapply(subreddit_names, function(subreddit) {
  # Randomize sleep time between 3 and 5 seconds
  sleep_time <- runif(1, min = 2, max = 4)

# Sleep for the random duration
  Sys.sleep(sleep_time) # Wait 2 seconds between requests
  get_subreddit_metadata(subreddit)
}))

# Save the cleaned dataset to a new CSV file
#write_csv(subreddit_metadata, "subreddit_metadata.csv")
# View the results
print(subreddit_metadata)
```

put data in a csv file

```{r}
#write_csv(subreddit_metadata, "subreddit_metadata_first1000.csv")
#write_csv(results, "subreddit_metadata_last2500.csv")
```

for computational purpose we don't run the code above as it takes a lot of time to scrape the data. We will use the data that we have already scraped.

remove NA
```{r, message=FALSE}
subreddit_first1000 <- read_csv("../../data/subreddit_metadata_first1000.csv")
subreddit_first1000 <- na.omit(subreddit_first1000)
subreddit_next2500 <- read_csv("../../data/subreddit_metadata_next2500.csv")
subreddit_next2500 <- na.omit(subreddit_next2500)
subreddit_last <-read_csv("../../data/subreddit_metadata_last2500.csv")
subreddit_last <- na.omit(subreddit_last)
# Combine the two dataframes
combined_subreddit_theme <- rbind(subreddit_first1000, subreddit_next2500,subreddit_last)
```

clustering test similarity

```{r, warning=FALSE}
corpus <- Corpus(VectorSource(combined_subreddit_theme$description))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("en"))
dtm <- DocumentTermMatrix(corpus)
```

### clusters

```{r, cache=TRUE}
set.seed(123)
k <- 30  # Choose the number of clusters
clusters <- kmeans(as.matrix(dtm), centers = k)
combined_subreddit_theme$theme <- as.factor(clusters$cluster)
# Rename the column
combined_subreddit_theme <- combined_subreddit_theme %>%
  rename(`theme (cluster number)` = theme)
# Remove duplicates based on the "subreddit" column
combined_subreddit_theme <- combined_subreddit_theme[!duplicated(combined_subreddit_theme$subreddit), ]
```

merge df with combined_subreddit_theme

```{r}
# Perform a left join to add the columns to 'df'
df <- df %>%
  left_join(combined_subreddit_theme, by = "subreddit")
```

### other way to cluster
topic modeling with LDA / similarity

### scrap related subreddits
```{r, eval=FALSE}

# Function to scrape related subreddits
get_related_subreddits <- function(subreddit) {
  # Construct the URL
  url <- paste0("https://www.reddit.com/r/", subreddit, "/")
  
  tryCatch({
    # Make the GET request with a User-Agent
    response <- GET(url, add_headers("User-Agent" = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"))
    
    # Check HTTP status code
    if (status_code(response) != 200) {
      cat(paste("Failed to fetch:", subreddit, "Status Code:", status_code(response), "\n"))
      return(data.frame(
        subreddit = subreddit,
        related_subreddits = NA,
        stringsAsFactors = FALSE
      ))
    }
    
    # Parse the HTML content
    page <- read_html(content(response, "text"))
    
    # Extract related subreddit elements
    related_subreddits <- page %>%
      html_nodes("li .overflow-ellipsis") %>%  # Target divs with the specific class inside list items
      html_text(trim = TRUE)                  # Extract the visible text
    
    # Check if any related subreddits were found
    if (length(related_subreddits) == 0) {
      cat(paste("No related subreddits found for:", subreddit, "\n"))
      related_subreddits <- NA
    }
    
    # Return the results
    return(data.frame(
      subreddit = subreddit,
      related_subreddits = paste(related_subreddits, collapse = ", "), # Join as a single string
      stringsAsFactors = FALSE
    ))
  }, error = function(e) {
    cat(paste("Error for subreddit:", subreddit, "with error:", e$message, "\n"))
    return(data.frame(
      subreddit = subreddit,
      related_subreddits = NA,
      stringsAsFactors = FALSE
    ))
  })
}

# Example usage with a few subreddit names
subreddit_names <- combined_subreddit_theme$subreddit[3501:nrow(combined_subreddit_theme)]


# Loop through subreddits and fetch related subreddits
results <- do.call(rbind, lapply(subreddit_names, function(subreddit) {
  cat(paste("Processing:", subreddit, "\n"))
  Sys.sleep(runif(1, min = 2, max = 3.5))  # Randomized delay
  get_related_subreddits(subreddit)
}))

# View the results
print(results)

```
```{r}
#write_csv(results, "subreddit_relatedsub_part3.csv")

```

```{r, warning=FALSE, message=FALSE}
subreddit_related_part1 <- read_csv("../../data/subreddit_relatedsub_part1.csv")
#remove r/ in column related subreddit
subreddit_related_part1$related_subreddits <- gsub("r/", "", subreddit_related_part1$related_subreddits)

subreddit_related_part2 <- read_csv("../../data/subreddit_relatedsub_part2.csv")
#remove r/ in column related subreddit
subreddit_related_part2$related_subreddits <- gsub("r/", "", subreddit_related_part2$related_subreddits)

subreddit_related_part3 <- read_csv("../../data/subreddit_relatedsub_part3.csv")
#remove r/ in column related subreddit
subreddit_related_part3$related_subreddits <- gsub("r/", "", subreddit_related_part3$related_subreddits)

combined_subreddit_related <- rbind(subreddit_related_part1, subreddit_related_part2,subreddit_related_part3)

#remove Megathread and Announcement from related subreddits
combined_subreddit_related$related_subreddits <- gsub("Megathread", "", combined_subreddit_related$related_subreddits)
combined_subreddit_related$related_subreddits <- gsub("Announcement", "", combined_subreddit_related$related_subreddits)

#remove ", ", ", , ", ", , , " in the beginning of the string
combined_subreddit_related$related_subreddits <- gsub("^,\\s*", "", combined_subreddit_related$related_subreddits)
combined_subreddit_related$related_subreddits <- gsub("^,\\s*", "", combined_subreddit_related$related_subreddits)
combined_subreddit_related$related_subreddits <- gsub("^,\\s*", "", combined_subreddit_related$related_subreddits)
combined_subreddit_related$related_subreddits <- gsub("^,\\s*", "", combined_subreddit_related$related_subreddits)
combined_subreddit_related$related_subreddits <- gsub("^,\\s*", "", combined_subreddit_related$related_subreddits)

#yes the code could be more efficient than copy paste 5 times the line above 


```

add column to match related subreddit with the subreddit in our database
```{r}
# Split related subreddits into individual components
combined_subreddit_related$matching_related_subreddits <- sapply(
  strsplit(combined_subreddit_related$related_subreddits, ",\\s*"), 
  function(related) {
    # Return only related subreddits that are in the 'subreddit' column
    related[related %in% combined_subreddit_related$subreddit]
  }
)

# Convert the list back into a string for easy viewing
combined_subreddit_related$matching_related_subreddits <- sapply(
  combined_subreddit_related$matching_related_subreddits, 
  paste, collapse = ", "
)

#we remove the NA values
combined_subreddit_related <- na.omit(combined_subreddit_related)


```

```{r}
# Perform a left join to add the columns to 'df'
df <- df %>%
  left_join(combined_subreddit_related, by = "subreddit")
```

### plot graph of matching related subreddits
```{r}

# Filter rows where `matching_related_subreddits` is not empty
filtered_data <- combined_subreddit_related[
  combined_subreddit_related$matching_related_subreddits != "", 
]

# Take only the first 100 rows
filtered_data <- head(filtered_data, 200)

# Prepare the edges for the graph
edges <- do.call(rbind, lapply(1:nrow(filtered_data), function(i) {
  source <- filtered_data$subreddit[i]
  targets <- unlist(strsplit(filtered_data$matching_related_subreddits[i], ",\\s*"))
  if (length(targets) > 0) {
    data.frame(from = source, to = targets, stringsAsFactors = FALSE)
  }
}))

# Ensure no duplicates in edges
edges <- unique(edges)

# Create nodes
nodes <- data.frame(
  id = unique(c(edges$from, edges$to)), # Unique nodes from edges
  label = unique(c(edges$from, edges$to)), # Labels are the same as IDs
  stringsAsFactors = FALSE
)

# Create the interactive graph
visNetwork(nodes, edges) %>%
  visNodes(size = 10) %>%
  visEdges(arrows = "to") %>%
  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %>%
  visInteraction(navigationButtons = TRUE) %>%
  visLayout(randomSeed = 42) # Consistent layout

```

other graph with everything
```{r}

# Filter rows where `matching_related_subreddits` is not empty
filtered_data <- combined_subreddit_related[
  combined_subreddit_related$matching_related_subreddits != "", 
]

# Take the first 1000 rows
filtered_data <- head(filtered_data, 1000)

# Prepare the edges for the graph
edges <- do.call(rbind, lapply(1:nrow(filtered_data), function(i) {
  source <- filtered_data$subreddit[i]
  targets <- unlist(strsplit(filtered_data$matching_related_subreddits[i], ",\\s*"))
  if (length(targets) > 0) {
    data.frame(from = source, to = targets, stringsAsFactors = FALSE)
  }
}))

# Ensure no duplicates in edges
edges <- unique(edges)

# Create the igraph object
graph <- graph_from_data_frame(
  d = edges,  # Edge list
  directed = TRUE # Directional relationships
)

# Use Kamada-Kawai layout for better node spacing
set.seed(42) # For consistent layout
layout <- layout_with_kk(graph)

# Scale layout to further spread nodes
layout <- layout * 2
# Plot the graph with spacing
plot(
  graph,
  vertex.size = 0.2,                      # Node size
  vertex.label = NA,                    # Disable labels
  # vertex.label.cex = 0.1,               # Reduced label text size
  # vertex.label.color = "black",         # Label color
  # vertex.label.dist = 1,                # Distance of label from node
  edge.arrow.size = 0.05,                # Smaller arrow size
  edge.color = "gray50",                # Edge color
  layout = layout,                      # Use the scaled layout
  main = "Subreddit Relationship Graph" # Add a title
)


```



using reddit api

```{r echo=FALSE}
# # Load necessary libraries
# library(httr)
# library(jsonlite)
# 
# # Replace these with your actual Reddit app's client ID, client secret, and redirect URI
# client_id <- "Qw1A2hzjh5bsUpKvvTSM-A"
# client_secret <- "_tk5YYxoVjiZqMmAFx3SfE9tF8rlZg"
# redirect_uri <- "http://localhost:8000"  # Make sure this matches the redirect URI you set in Reddit
# 
# # Step 1: Get the authorization URL
# authorize_url <- paste0("https://www.reddit.com/api/v1/authorize?client_id=", client_id,
#                        "&response_type=code&state=some_random_string&redirect_uri=", redirect_uri,
#                        "&scope=read")
# 
# # Print the URL and manually visit it to authorize the app
# cat("Visit this URL to authorize the app:\n", authorize_url, "\n")
# 
# # Once authorized, you will be redirected to the redirect_uri with a 'code' parameter in the URL
# # You need to capture the 'code' parameter, which you will use to obtain an access token
# 
# # Step 2: User visits the URL and grants permission, then gets the 'code' from the URL
# 
# # Prompt user to enter the code received after authorization
# auth_code <- readline(prompt = "Enter the code you received from Reddit: ")
# 
# # Step 3: Use the code to get an access token
# token_response <- POST("https://www.reddit.com/api/v1/access_token",
#                        authenticate(client_id, client_secret),
#                        body = list(
#                          grant_type = "authorization_code",
#                          code = auth_code,
#                          redirect_uri = redirect_uri
#                        ),
#                        encode = "form")
# 
# # Parse the response to get the access token
# token_data <- fromJSON(content(token_response, "text"))
# access_token <- token_data$access_token
# 
# # Step 4: Use the access token to make authorized API requests
# # Now you can make authenticated requests to Reddit's API
# 
# # Example: Get subreddit metadata (e.g., display name and description)
# get_subreddit_metadata <- function(subreddit) {
#   url <- paste0("https://www.reddit.com/r/", subreddit, "/about.json")
#   
#   # Send a GET request to Reddit API with the authorization header
#   response <- GET(url, add_headers(Authorization = paste("Bearer", access_token)))
#   
#   if (status_code(response) != 200) {
#     return(data.frame(
#       subreddit = subreddit,
#       display_name = NA,
#       description = NA,
#       stringsAsFactors = FALSE
#     ))
#   }
#   
#   # Parse the JSON response
#   data <- fromJSON(content(response, "text"))
#   
#   # Extract metadata
#   display_name <- data$data$display_name
#   description <- data$data$public_description
#   
#   # Return the result
#   return(data.frame(
#     subreddit = subreddit,
#     display_name = display_name,
#     description = description,
#     stringsAsFactors = FALSE
#   ))
# }
# 
# # Example list of subreddits to get metadata for
# subreddit_names <- c("r/programming", "r/learnprogramming")
# 
# # Get metadata for the subreddits
# subreddit_metadata <- do.call(rbind, lapply(subreddit_names, function(subreddit) {
#   Sys.sleep(2)  # Optional: add a delay between requests to avoid rate limiting
#   get_subreddit_metadata(subreddit)
# }))
# 
# # View the results
# print(subreddit_metadata)


```


## Sentiment Analysis

### Sentiment by Subreddit

Analyzing the top *30* subreddits (30 for vizualisation purposes) as a *whole*, focusing on average sentiment scores for each subreddit to get a general idea of the sentiment of the comments in each subreddit. We will use the AFINN, NRC, and BING lexicons to assign sentiment scores to words in the comments. We will then calculate the average sentiment score for each subreddit based on the sentiment scores of the words in the comments.

#### AFINN

The AFINN lexicon is a list of English words rated for valence with an integer between -5 (negative) and +5 (positive).

- Weaknessess : It may not capture the sentiment of slang, sarcasm, or other informal language. Also limited to the words in the lexicon.
- Strengths : It is simple and easy to use.

```{r sentiment_afinn, eval=TRUE, echo=TRUE}
#get top 50 subreddits
df.top30_subreddit <- df.subreddit %>%
  arrange(desc(nchar(body))) %>%
  filter(row_number() <= 30)

df.top30_subreddit.tokens <- df.top30_subreddit %>% unnest_tokens(word, body)

df.top30_subreddit.affin <- inner_join(df.top30_subreddit.tokens, get_sentiments("afinn"),
 by = c("word" = "word")) %>% group_by(subreddit) %>%
 summarize(Sentiment = mean(value)) %>% ungroup()

ggplot(df.top30_subreddit.affin, aes(x = Sentiment, y = reorder(subreddit, Sentiment), fill = Sentiment)) +
  geom_col() +
  scale_fill_viridis_c() +
  ylab("") + 
  #increase text size
  theme(axis.text.y = element_text(size = 10),
        axis.text.x = element_text(size = 10))
```

There seems to be more positive subreddits than negative ones.

We interestingly observe that based on the affin dictionnary the `news` and `politics` subreddit are the most negative, which could reflect a state of the world.

And `pc` Subreddits like  `pcmasterrace` or `builapc` are the most positive.

#### BING

The Bing sentiment lexicon categorizes words as either "positive" or "negative."

- Weaknesses: It lacks the granularity of numerical scores and may not capture the intensity of sentiment. It also does not account for neutral words.
- Strengths: Easy to use and understand. It provides a straightforward classification of words.

```{r bing_sentiment, eval=TRUE, echo=TRUE}
df.top30_subreddit.bing <- inner_join(df.top30_subreddit.tokens, get_sentiments("bing"),
 by = c("word" = "word")) %>% group_by(subreddit) %>%
 summarize(Sentiment = mean(ifelse(sentiment == "positive", 1, ifelse(sentiment == "negative", -1, 0)))) %>%
 select(subreddit, Sentiment)
 
ggplot(df.top30_subreddit.bing, aes(x = Sentiment, y = reorder(subreddit, Sentiment), fill = Sentiment)) +
  geom_col() +
  scale_fill_viridis_c() +
  ylab("") + 
  #increase text size
  theme(axis.text.y = element_text(size = 10),
        axis.text.x = element_text(size = 10))
```

BING seems to have a bit the same proporiton of positive and negative as the AFINN dictionnary. Indeed, we observe the same pattern with `pcmasterrace` at the top and `news` at the bottom, but there is slight variation when compared to the AFINN dictionnary.

#### NRC

The NRC Emotion Lexicon (EmoLex) associates words with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (positive and negative).

- Weaknesses: More complex to implement and interpret. It may require more computational resources.
- Strengths: Provides a more detailed analysis by categorizing words into multiple emotions. It can capture a wider range of sentiments and emotions.

```{r nrc_sentiment, eval=TRUE, echo=TRUE}
df.top30_subreddit.nrc <- inner_join(df.top30_subreddit.tokens, get_sentiments("nrc"),
 by = c("word" = "word")) %>% group_by(subreddit) %>%
 summarize(Sentiment = mean(ifelse(sentiment == "positive", 1, ifelse(sentiment == "negative", -1, 0)))) %>%
 select(subreddit, Sentiment)

ggplot(df.top30_subreddit.nrc, aes(x = Sentiment, y = reorder(subreddit, Sentiment), fill = Sentiment)) +
  geom_col() +
  scale_fill_viridis_c() +
  ylab("") + 
  #increase text size
  theme(axis.text.y = element_text(size = 10),
        axis.text.x = element_text(size = 10))
```

Through the NRC dictionnary we observe far more positive subreddits than negative ones. This inbalance could be explain by the fact that the NRC dictionnary is more biased towards positive words.

We still observe quite the same patterns though with `pcmasterrace`  at the top and `news` at the bottom.

#### Summary

To summarize it is interesting to observe that the NRC classification is totally different from the AFINN and BING classification. Indeed, the NRC classification seems to be more biased towards positive words but on the other hand it shall be more precise as it has more categories. 

Why one to choose ? It depends on the goal of the analysis. If we want a simple and easy to use classification we can use AFINN or BING. If we want a more precise classification we can use NRC.

### Sentiment by comment

Now to have a more precise understanding of the sentiment of the comments, we will analyze the sentiment per comment for each dictionary (AFINN, BING, NRC). So that we can choose the best dictionary for our analysis.

We will analyze a sample of small comments to have a better understanding.

#### Afinn

```{r sample_afinn_comment, eval=TRUE, echo=TRUE}
#define a random sample of 100 comments
set.seed(1234)
df.sample <- df %>% sample_n(100) %>%
  #select only columns 'id' and 'body'
  select(id, body, subreddit)

# use afinn to analyse sentiment all the rows based on column body
df.sentiment <- inner_join(df.sample %>% unnest_tokens(word, body),
 get_sentiments("afinn"), by = c("word" = "word")) %>% group_by(id) %>%
 summarize(Sentiment = mean(value)) %>% inner_join(df.sample, by = "id")

# Normalize the sentiment scores
df.sentiment.a <- df.sentiment %>%
 mutate(Normalized_Sentiment = (Sentiment - min(Sentiment)) / (max(Sentiment) - min(Sentiment)))

# show the sentiment of the comments
reactable(df.sentiment.a,
          resizable = TRUE,
          defaultPageSize = 2,
          sortable = TRUE,
          searchable = TRUE,
          filterable = TRUE,
          pagination = TRUE,
          highlight = TRUE,
          #add possibility to increase rows per page
          pageSizeOptions = c(5, 10, 15, 20)
          )
#save data as 'AFINN_sentiment_sample.csv'
#write_csv(df.sentiment, "../../data/AFINN_sentiment_sample.csv")
```

#### Bing

```{r sample_bing_comment, eval=TRUE, echo=TRUE}
df.sentiment <- inner_join(df.sample %>% unnest_tokens(word, body),
 get_sentiments("bing"), by = c("word" = "word")) %>% group_by(id) %>%
 summarize(Sentiment = mean(ifelse(sentiment == "positive", 1, ifelse(sentiment == "negative", -1, 0)))) %>%
 inner_join(df.sample, by = "id")

# Normalize the sentiment scores
df.sentiment.b <- df.sentiment %>%
 mutate(Normalized_Sentiment = (Sentiment - min(Sentiment)) / (max(Sentiment) - min(Sentiment)))

#show 
reactable(df.sentiment.b,
          resizable = TRUE,
          defaultPageSize = 2,
          sortable = TRUE,
          searchable = TRUE,
          filterable = TRUE,
          pagination = TRUE,
          highlight = TRUE,
          #add possibility to increase rows per page
          pageSizeOptions = c(5, 10, 15, 20)
          )

#save data as 'AFINN_sentiment_sample.csv'
#write_csv(df.sentiment, "../../data/BING_sentiment_sample.csv")
```

After reading the comments, we observe that they are mostly right. For example this comment was classified as neutral with a score of 0 : [Dude Congrats Im hoping to lose a large amount of weight in the same amount of time Huge inspiration here Keep kicking ass]

And this comment was classified as quite negative with a score of -2 : [Why did the chicken cross the road It didnt HHAHHAHAHAHHAHAHHA sorry that wasnt funny 472]. It seems, it did not understood the context

#### NRC

```{r sample_NRC_comment}
#use NRC to analyse sentiment all the rows based on column body
df.sentiment <- inner_join(df.sample %>% unnest_tokens(word, body),
 get_sentiments("nrc"), by = c("word" = "word")) %>% group_by(id) %>%
 summarize(Sentiment = mean(ifelse(sentiment == "positive", 1, ifelse(sentiment == "negative", -1, 0)))) %>%
 inner_join(df.sample, by = "id")

# Normalize the sentiment scores
df.sentiment.n <- df.sentiment %>%
 mutate(Normalized_Sentiment = (Sentiment - min(Sentiment)) / (max(Sentiment) - min(Sentiment)))

#show 
reactable(df.sentiment.n,
          resizable = TRUE,
          defaultPageSize = 2,
          sortable = TRUE,
          searchable = TRUE,
          filterable = TRUE,
          pagination = TRUE,
          highlight = TRUE,
          #add possibility to increase rows per page
          pageSizeOptions = c(5, 10, 15, 20)
          )

#save data as 'AFINN_sentiment_sample.csv'
#write_csv(df.sentiment, "../../data/NRC_sentiment_sample.csv")
```

We observe here that comments like "Night time moon light and a light snow The glow of the moon and the silence is amazing", which evokes a serene and appreciative tone for natural beauty suggest a strong positive emotion. The three dictionaries seem to have classified this comment as positive with the BING and AFINN dicitonnary attributing a higher score than the NRC.

#### Vizualising the Sentiment distribution based on a dictionnary

```{r sent_dist, cache=TRUE}
# use afinn to analyse sentiment all the rows based on column body
df.sentiment.a <- inner_join(df %>% unnest_tokens(word, body),
 get_sentiments("afinn"), by = c("word" = "word")) %>% group_by(id) %>%
 summarize(Sentiment = mean(value)) %>% inner_join(df.sample, by = "id")

# use bing to analyse sentiment all the rows based on column body
df.sentiment.b <- inner_join(df %>% unnest_tokens(word, body),
 get_sentiments("bing"), by = c("word" = "word")) %>% group_by(id) %>%
 summarize(Sentiment = mean(ifelse(sentiment == "positive", 1, ifelse(sentiment == "negative", -1, 0)))) %>%
 inner_join(df, by = "id")

#use NRC to analyse sentiment all the rows based on column body
df.sentiment.n <- inner_join(df %>% unnest_tokens(word, body),
 get_sentiments("nrc"), by = c("word" = "word")) %>% group_by(id) %>%
 summarize(Sentiment = mean(ifelse(sentiment == "positive", 1, ifelse(sentiment == "negative", -1, 0)))) %>%
 inner_join(df, by = "id")


#plot the three sentiment distribution on one using viridis color
ggplot() +
  geom_density(data = df.sentiment.a, aes(x = Sentiment, fill = "AFINN"), alpha = 0.5) +
  geom_density(data = df.sentiment.b, aes(x = Sentiment, fill = "BING"), alpha = 0.5) +
  geom_density(data = df.sentiment.n, aes(x = Sentiment, fill = "NRC"), alpha = 0.5) +
  scale_fill_viridis_d() +
  theme_minimal()
```

- The AFINN method shows a relatively flat distribution with a slight peak around 0. This suggests that the sentiment scores are spread out, with a slight tendency towards neutrality.

- The Bing method displays a bimodal distribution with two distinct peaks, one around -1 and another around 1. This indicates that the sentiment scores are polarized, with many comments being classified as either quite negative or quite positive. Which is logical based on the previous analysis

- The NRC method has a sharp peak at 0, suggesting that most of the sentiment scores are neutral. This implies that the NRC method tends to classify a large number of comments as neutral, with fewer comments being classified as strongly positive or negative. But we note that the NRC is also quite condensed

#### Normalization to correctly compare

```{r norm_sent_dist, eval=TRUE, echo=TRUE}
# Normalize the sentiment scores
df.sentiment.a <- df.sentiment.a %>%
  mutate(Normalized_Sentiment = (Sentiment - min(Sentiment)) / (max(Sentiment) - min(Sentiment)))

df.sentiment.b <- df.sentiment.b %>%
  mutate(Normalized_Sentiment = (Sentiment - min(Sentiment)) / (max(Sentiment) - min(Sentiment)))

df.sentiment.n <- df.sentiment.n %>%
  mutate(Normalized_Sentiment = (Sentiment - min(Sentiment)) / (max(Sentiment) - min(Sentiment)))

# Create the ggplot object
p <- ggplot() +
  geom_density(data = df.sentiment.a, aes(x = Normalized_Sentiment, fill = "AFINN"), alpha = 0.3) +
  geom_density(data = df.sentiment.b, aes(x = Normalized_Sentiment, fill = "BING"), alpha = 0.5) +
  geom_density(data = df.sentiment.n, aes(x = Normalized_Sentiment, fill = "NRC"), alpha = 0.5) +
  scale_fill_viridis_d() +
  theme_minimal()

# Convert the ggplot object to an interactive plotly object
interactive_plot <- ggplotly(p)

# Adjust the height and width 
interactive_plot <- layout(interactive_plot, width = 600, height = 400)
# Display the interactive plot
interactive_plot
```

#### Summary

We might choose one method over the others. For example, if we need a method that captures strong sentiments, Bing might be more suitable. For a more balanced view, AFINN could be the way to go. With more neutral comments, NRC might be the best choice.

#### LLM interpretation

We are going to use some LLM on a random sample to judge the accuracy of each dictionnary and judge which one to take.

prompt : 'Assign a sentiment score to this reddit comment from 0 (extremely negative) to 1 (extremely positive)'

Loop through the comments and assign a sentiment score 

## Analyzing the relationship between sentiment and score

There seem to be two outliers in this plot, let's examine them


There seems to be no relation between sentiment and score

### Apply The findings on the whole dataset

- investigate why it reduces the number of comment

```{r}
# use afinn to analyse sentiment all the rows based on column body
df.sentiment <- inner_join(df %>% unnest_tokens(word, body),
 get_sentiments("afinn"), by = c("word" = "word")) %>% group_by(id) %>%
 summarize(Sentiment = mean(value)) %>% inner_join(df, by = "id")

# Normalize the sentiment scores
df.sentiment <- df.sentiment %>%
 mutate(Normalized_Sentiment = (Sentiment - min(Sentiment)) / (max(Sentiment) - min(Sentiment)))

#new column for sentiment category
df.sentiment$sentiment_category <- ifelse(df.sentiment$Sentiment >= 1, "positive",
 ifelse(df.sentiment$Sentiment <= -1, "negative", "neutral"))

df <- df.sentiment 

# save the data as a .csv file
#write_csv(df.sentiment, "../../data/AFINN_sentiment.csv")
```

## Score

```{r}
df_hugo <- df %>%
  select(-c("X","author_flair_text","id"))

score_2 <- sum(df_hugo$score==2)
score_1 <- sum(df_hugo$score==1)
score_0 <- sum(df_hugo$score==0)

print(paste0("Total number of comments without Up or Downvotes is ", score_1))
print(paste0("Total number of comments with only one Up or Downvote is ", score_0 + score_2))
print(paste0(round((100*(score_0 + score_1 + score_2)/138070),digits = 2), "% of the comments have 0 or 1 upvote/downvote"))
```

```{r}
# Filtering scores to focus on the range from -10 to 100
range_filtered <- df_hugo %>%
  filter(score >= -10 & score <= 20)

# Plotting the distribution of the 'score' column from -10 to 100 using ggplot2
ggplot(range_filtered, aes(x = score)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Score Column from -10 to 20",
       x = "Score",
       y = "Frequency") +
  theme_minimal() +
  theme(panel.grid.major = element_line(color = "grey", linetype = "dashed", size = 0.5))
```
```{r}
print(summary(df_hugo$score))
```

```{r}
df_high_score <- df_hugo %>%
  filter(score %in% (10:3967))

ggplot(df_high_score, aes(x = score)) +
  geom_histogram(bins = 30, fill = "lightblue", color = "black", alpha = 0.7) +
  scale_y_log10() +
  scale_x_log10()+
  labs(title = "Log-Distribution of High Scoring (10+ upvotes) Comments",
       x = "Score",
       y = "Log(Frequency)") +
  theme_minimal() +
  scale_y_continuous(labels = scales::label_comma())
```


```{r}
df_low_score <- df_hugo %>%
  filter(score %in% (-302:-10))

# Plotting the distribution of low scoring comments using ggplot2 with log transformation on both axes
ggplot(df_low_score, aes(x = score)) +
  geom_histogram(bins = 30, fill = "lightblue", color = "black", alpha = 0.7) +
  scale_y_log10() +
  scale_x_reverse() +
  labs(title = "Log-Distribution of Low Scoring (-10 and below) Comments",
       x = "Score",
       y = "Log(Frequency)") +
  theme_minimal() +
  scale_y_continuous(labels = scales::label_comma())

```
```{r}
# Counting unique subreddits in the dataset
unique_subreddits <- n_distinct(df_high_score$subreddit)
print(paste0("Total number of unique subreddits in the High-score dataset: ", unique_subreddits))
```
Only keep the subreddits with 5 or more comments
```{r}
# Analyzing subreddit-wise tendency of high scores
df_subreddit_high_score <- df_high_score %>%
  group_by(subreddit) %>%
  summarise(avg_score = mean(score), median_score = median(score), comment_count = n()) %>%
  filter(comment_count >= 5) %>%
  arrange(desc(avg_score))

# Printing subreddit analysis
df_subreddit_high_score
```


```{r}
# Plotting histogram of the number of comments per subreddit for high scoring comments without subreddit names
count_distribution <- df_high_score %>%
  group_by(subreddit) %>%
  summarise(comment_count = n()) %>%
  filter(comment_count >= 5) %>%
  arrange(desc(comment_count))

# Plotting the distribution of number of comments per subreddit
ggplot(count_distribution, aes(x = comment_count)) +
  geom_histogram(bins = 30, fill = "darkorange", alpha = 0.8) +
  labs(title = "Distribution of Number of Comments per Subreddit for High Scoring Comments",
       x = "Number of Comments",
       y = "Frequency") +
  theme_minimal()
```
```{r}
# Plotting average score by subreddit for high scoring comments with 5 or more comments
ggplot(df_subreddit_high_score, aes(x = reorder(subreddit, avg_score), y = avg_score)) +
  geom_bar(stat = "identity", fill = "steelblue", alpha = 0.8) +
  coord_flip() +
  labs(title = "Average Score by Subreddit for High Scoring Comments (5 or more comments)",
       x = "Subreddit",
       y = "Average Score") +
  theme_minimal()
```
```{r}
# Analyzing subreddit-wise tendency of low scores
df_subreddit_low_score <- df_low_score %>%
  group_by(subreddit) %>%
  summarise(avg_score = mean(score), median_score = median(score), comment_count = n()) %>%
  filter(comment_count >= 2) %>%
  arrange(avg_score)

# Printing subreddit analysis for low scores
df_subreddit_low_score
```

```{r}
ggplot(df_subreddit_low_score, aes(x = avg_score, y = reorder(subreddit, avg_score))) +
  geom_bar(stat = "identity", fill = "red", alpha = 0.8) +
  scale_x_reverse(limits = c(0, min(df_subreddit_low_score$avg_score))) +
  labs(title = "Average Score by Subreddit for Low Scoring Comments (5 or more comments)",
       x = "Subreddit",
       y = "Average Score") +
  theme_minimal()
```

```{r}
# Plotting histogram of the number of comments per subreddit for low scoring comments without subreddit names
count_distribution_low <- df_low_score %>%
  group_by(subreddit) %>%
  summarise(comment_count = n()) %>%
  filter(comment_count >= 2) %>%
  arrange(desc(comment_count))

# Plotting the distribution of number of comments per subreddit for low scores
ggplot(count_distribution_low, aes(x = comment_count)) +
  geom_histogram(bins = 20, fill = "darkred", alpha = 0.8) +
  labs(title = "Distribution of Number of Comments per Subreddit for Low Scoring Comments",
       x = "Number of Comments",
       y = "Frequency") +
  theme_minimal()

```



```{r}
# Clustering analysis based on number of comments and average score for high scoring subreddits
df_subreddit_cluster <- df_high_score %>%
  group_by(subreddit) %>%
  summarise(avg_score = mean(score), comment_count = n())

# Scaling the data for clustering
cluster_data <- df_subreddit_cluster %>%
  select(comment_count, avg_score) %>%
  scale()

# Determining the optimal number of clusters using the elbow method
set.seed(123)
fviz_nbclust(cluster_data, kmeans, method = "wss") +
  labs(title = "Elbow Method for Determining Optimal Number of Clusters",
       x = "Number of Clusters",
       y = "Within-Cluster Sum of Squares") +
  theme_minimal()
```
We'll settle with 5 clusters

```{r}
# Running k-means clustering with 5 clusters
set.seed(123) # For reproducibility
kmeans_result <- kmeans(cluster_data, centers = 5)
df_subreddit_cluster$cluster <- as.factor(kmeans_result$cluster)

# Plotting the clusters
ggplot(df_subreddit_cluster, aes(x = comment_count, y = avg_score, color = cluster)) +
  geom_point(size = 3, alpha = 0.7) +
  labs(title = "Clustering of Subreddits Based on Number of Comments and Average Score",
       x = "Number of Comments",
       y = "Average Score") +
  theme_minimal() 
```

## Additional Variable
### Adding Length of comment as a feature

```{r, eval=TRUE, echo=TRUE}
#show length of one random comment
df$body[1] %>% nchar() #char shows the number of characters in a string
#to show the number of words in a comment we can use str_count
df$body[1] %>% str_count("\\w+")
df$body[1]
#add length of comment as a feature
df$length <- str_count(df$body, "\\w+")
head(df)

#get summary statistics on column length
summary(df$length)
```

### Adding 'related subreddit' 

Add columns, friends1, friends2, friends3, ... by scrapping it from the subreddit page

## Final Dataset

```{r}
head(df)
#rename column 'theme (cluster number)' to 'theme'
df <- df %>%
  rename(theme = `theme (cluster number)`)
# Save the final dataset to a CSV file
# select only columns ; 'id','author' ,'subreddit', 'display_name', 'body', 'score', 'sentiment_category', 'length', 'description', 'theme'
df <- df %>%
  select(id, author, subreddit, display_name, body, score, sentiment_category, length, description, theme)
#write_csv(df, "../../data/reddit_comments_15k_v2.csv")
#load the data
df <- read_csv("../../data/reddit_comments_15k_v2.csv")
#count nunber of rows
nrow(df)
```

