# EDA

## Text Mining
### A subreddit = A document

Here i take the idea that a 'document' is a subreddit. It is like all comments that are part of one subreddit will be a document. It is a bit an analysis per subreddit

```{r top_10_sub, eval=TRUE, echo=TRUE}
#take the df data and group all comments by subreddit
df.subreddit <- df %>%
  group_by(subreddit) %>%
  summarise(body = paste(body, collapse = " "))

# show number of subreddits
cat("Number of subreddits:", nrow(df.subreddit), "\n")

# show top 10 subreddits by length in a bar plot
df.subreddit %>%
  mutate(subreddit = reorder(subreddit, nchar(body))) %>%
  top_n(10, nchar(body)) %>%
  ggplot(aes(x = subreddit, y = nchar(body))) +
  geom_col() +
  coord_flip() +
  labs(title = "Top 10 Subreddits by Length", x = "Subreddit", y = "Length") +
  #increase text size
  theme(axis.text.y = element_text(size = 10),
        axis.text.x = element_text(size = 10))

```

We observe that AskReddit is by far the largest Subreddit

```{r word_freq, eval=TRUE, echo=TRUE, cache=TRUE}
# preprocess the text data
df.subreddit.cp <- corpus(df.subreddit$body)
docnames(df.subreddit.cp) <- df.subreddit$subreddit  # Assign subreddit names as document identifiers

df.subreddit.tk <- tokens(
  df.subreddit.cp,
  remove_numbers = TRUE,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_url = TRUE,
  remove_hyphens = TRUE,
  remove_separators = TRUE
)
df.subreddit.tk <- df.subreddit.tk %>%
  tokens_tolower() %>%
  tokens_remove(stop_words$word) %>%
  tokens_remove(c("reddit", "subreddit", "amp", "gt", "deleted", "x+"))

# tf and tf-idf analysis
#tf
df.subreddit.dfm <- dfm(df.subreddit.tk)
tf <- rowSums(t(df.subreddit.dfm))
tf <- data.frame(term = names(tf), count = tf) %>%
  tibble() %>%
  arrange(desc(count))

#freq per documents
tidy(df.subreddit.dfm) %>%
  arrange(desc(count))

#freq per term
df.subreddit.freq <- textstat_frequency(df.subreddit.dfm)
head(df.subreddit.freq,20)

#plot 20 most frequent words
df.subreddit.freq %>%
  top_n(20, frequency) %>%
  ggplot(aes(
    x = reorder(feature, frequency),
    y = frequency)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  xlab("Frequency") +
  ylab("term") +
  #increase size of text
  theme(axis.text.y = element_text(size = 10),
        axis.text.x = element_text(size = 10))

```

We observe here a lot of abbreiations that are not taken into account by the stop words. Indeed, lots of word are just because the author removed the '. So for example. i'm becomes im or you're becomes youre.

```{r textplot_wordcloud, eval=TRUE, echo=TRUE}
textplot_wordcloud(df.subreddit.dfm)

df.subreddit.freq %>%
  top_n(20, frequency) %>%
  ggplot(aes(label = feature, size = frequency)) +
  geom_text_wordcloud() +
  scale_size_area(max_size = 20) +
  theme_minimal()
```

We can observe that the most frequent words are 'people', 'time

```{r tf_idf, eval=TRUE, echo=TRUE}
#tf-idf
df.subreddit.tfidf <- dfm_tfidf(df.subreddit.dfm)
df.subreddit.tfidf.tidy <- tidy(df.subreddit.tfidf) %>%
  bind_tf_idf(term = term, document = document, n = count) %>%
  arrange(desc(tf_idf))
head(df.subreddit.tfidf.tidy, n = 20) %>% flextable() %>% autofit()
```

So tf_idf shows us rare words that are used in a specific subreddit. So we have here lot's of 'small' and 'rare' subreddit that have specific words that are not used in other subreddit. This is interesting as it shows us the specificity of each subreddit.

```{r tf_idf_plot, eval=TRUE, echo=TRUE}
#plot per document
df.subreddit.dfm %>%
  tidy() %>%
  top_n(12, count) %>%
  ggplot(aes(x = term, y = count)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  theme(axis.text.y = element_text(size = 10),
        axis.ticks.y = element_blank()) +
  facet_wrap(~document, ncol = 3)
```

So in summary: 
-   We observe that AskReddit has lots of words frequency which makes sense as it is the biggest subreddit by lenght.
-   We also observe that the term 'game' is associated in term of frequency with sports subreddits like CFB (College Football) and Hockey which makes sense.
-   Interestingly the subreddits LoL (League of Legends) is associated with the term 'happy', which show us that the community may be the happiest, careful bias here, it is just a word frequency analysis.
- In the subreddits 'pics' we observe a lot of 'gem' term which indicates treasure or something valuable. This is interesting as it is a subreddit for sharing pictures.
- The subreddits 'news' is associated in terms of frequency with the term 'people' which is logical as news is about people.

#### Similarity between Subreddits/Documents

we analyze similarities and dissimilarities between the documents (through words) in the data. We use quanteda extensively. Weâ€™ll use the objects created previously in the exercises.

Then use the functions textstat_simil() and textstat_dist() to compute the Jaccard index matrix, the cosine matrix, and the Euclidean distances matrix.

```{r similarity_subreddits, eval=TRUE, echo=TRUE, cache=TRUE}
#We restrict ourselves to a small subset of the highest tf-idf of the data to avoid memory issues and readability issues in the plots.
df.subreddit.tfidf.small <- df.subreddit.tfidf[1:50,]
df.subreddit.tfidf.small

df.subreddit.jac <- textstat_simil(
  df.subreddit.tfidf.small, method = "jaccard", margin = "documents")

df.subreddit.cos <- textstat_simil(
  df.subreddit.tfidf.small, method = "cosine", margin = "documents")

df.subreddit.eucl <- textstat_dist(
  df.subreddit.tfidf.small, method = "euclidean", margin = "documents")

#heatmap representation of similariteis between subreddits
## jaccard
df.subreddit.jac.mat <- melt(as.matrix(df.subreddit.jac))
ggplot( data = df.subreddit.jac.mat, 
        mapping = aes(x = Var1, y = Var2, fill = value)) +
  scale_fill_gradient2(
    low = "blue", 
    high = "red", 
    mid = 'white', 
    midpoint = 0.5, 
    limit = c(0,1), 
    name = "Jaccard") +
  geom_tile() + xlab("") + ylab("") +
  #incline x-axis labels
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

## cosine
df.subreddit.cos.mat <- melt(as.matrix(df.subreddit.cos))
ggplot( data = df.subreddit.cos.mat, 
        mapping = aes(x = Var1, y = Var2, fill = value)) +
  scale_fill_gradient2(
    low = "blue", 
    high = "red", 
    mid = 'white', 
    midpoint = 0.5, 
    limit = c(0,1), 
    name = "Cosine") +
  geom_tile() + xlab("") + ylab("") +
  #incline x-axis labels
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

## euclidean
df.subreddit.eucl.mat <- melt(as.matrix(df.subreddit.eucl))
ggplot( data = df.subreddit.eucl.mat, 
        mapping = aes(x = Var1, y = Var2, fill = value)) +
  scale_fill_gradient2(
    low = "blue", 
    high = "red", 
    mid = 'white', 
    midpoint = 0.5, 
    limit = c(0,1), 
    name = "Euclidean") +
  geom_tile() + xlab("") + ylab("") +
  #incline x-axis labels
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

#### Clustering of Documents

Hierarchical clustering and K-means. The first one is applied on the dissimilarities (Euclidean, inverted Jaccard, and inverted cosine). The second one is applied on the features, here, TF-IDF. To illustrate the methods, we decide to create five clusters.

```{r clustering_subreddits, eval=TRUE, echo=TRUE}
#hierarchical
df.subreddit.hc <- hclust(as.dist(df.subreddit.eucl))
## df.subreddit.hc <- hclust(as.dist(1 - df.subreddit.jac)) # use this line for Jaccard
## df.subreddit.hc <- hclust(as.dist(1 - df.subreddit.cos)) # use this line for Cosine
plot(df.subreddit.hc)
df.subreddit.clust <- cutree(df.subreddit.hc, k = 3)
df.subreddit.clust
#k-means

df.subreddit.km <- kmeans(df.subreddit.tfidf.small, centers = 3)
df.subreddit.km$cluster
```

### Topic Modelling per subrreddit
#### LSA

```{r lsa_subreddit, eval=TRUE, echo=TRUE}
#select subset of data of the top 10 subreddits for readability purposes
df.top10_subreddit <- df.subreddit %>%
  arrange(desc(nchar(body))) %>%
  filter(row_number() <= 10)

#corpus
df.top10_subreddit.cp <- corpus(df.top10_subreddit$body)
# Assign subreddit names as document identifiers
docnames(df.top10_subreddit.cp) <- df.top10_subreddit$subreddit

#token
df.top10_subreddit.tk <- tokens(
  corpus(df.top10_subreddit$body),
  remove_numbers = TRUE,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_url = TRUE,
  remove_hyphens = TRUE,
  remove_separators = TRUE
)
df.top10_subreddit.tk <- df.top10_subreddit.tk %>%
  tokens_tolower() %>%
  tokens_remove(stop_words$word) %>%
  tokens_remove(c("reddit", "subreddit", "amp", "gt", "deleted", "x+")) %>%
  tokens_replace(
    pattern = hash_lemmas$token,
    replacement = hash_lemmas$lemma)

# Create Document-Feature Matrix (DFM)
df.top10_subreddit.dfm <- dfm(df.top10_subreddit.tk)

# Explicitly set document names in the DFM to the subreddit names
docnames(df.top10_subreddit.dfm) <- df.top10_subreddit$subreddit
df.top10_subreddit.dfm

#use textmodel_lsa() on dfm using n dimensions
df.subreddit.lsa <- textmodel_lsa(
  x = df.top10_subreddit.dfm,
  nd = 5)

head(df.subreddit.lsa$docs)
#interpretation
## we look at the five terms with the largest values and the five ones with the lowest value (i.e., largest negative value)

n.terms <- 5
##for dim 2
w.order <- sort(df.subreddit.lsa$features[, 2], decreasing = TRUE)
w.top2 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))
##for dim 3
w.order <- sort(df.subreddit.lsa$features[, 3], decreasing = TRUE)
w.top3 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))

w.top2
w.top3
```

Topic 2 is associated positively with topic like  (game, team, play, games, fuck, ) and negatively with topics like (questions, question, comments, post, people)

```{r pca, cache=TRUE, eval=TRUE, echo=TRUE}
#PCA bibplots for links between topics and documents and topcis with terms
w.subset <-
  ##restric chart to terms that are mostly related to dim2 and dim3 so w.top2 and w.top3
  df.subreddit.lsa$features[
    c(unique(c(names(w.top2), names(w.top3)))), 2:3]

biplot(
  y = df.subreddit.lsa$docs[, 2:3],
  x = df.subreddit.lsa$features[,2:3],
  col = c("black","red"),
  cex = c(0.5, 0.5),
  xlab = "Dim 2",
  ylab = "Dim 3")
```
We observe that the word `fuck` is associated with dim 3 and is interestingly negatively associated to `happy` which makes sense.

#### LDA

```{r lda_subreddit, cache=TRUE, eval=TRUE, echo=TRUE}
docnames(df.top10_subreddit.dfm) <- df.top10_subreddit$subreddit
set.seed(1234) #To create reproducible results
df.top10_subbredit.lda <- textmodel_lda(
  x = df.top10_subreddit.dfm,
  k = 10)

#extract top 5 terms per topic
seededlda::terms(df.top10_subbredit.lda, 5)

# Extract subreddits per topic and rename output to match subreddit names
top_subreddits <- seededlda::topics(df.top10_subbredit.lda)
names(top_subreddits) <- df.top10_subreddit$subreddit
top_subreddits

#count the number of documents per topic
seededlda::topics(df.top10_subbredit.lda) %>% table()
```

Topic 8 with 3 subreddits ( AskReddit, funny and news) is the most popular topic with words like people, time, fuck, day, feel which may represent human experience and emotions. They touch on aspects of daily life, feelings, and the passage of time, with a bit of raw expression thrown in. It's a mix of the mundane and the profound, capturing the essence of what it means to be human. Indeed, what reddit is about haha

#### Topic-term Analysis

```{r topic_term_analysis, eval=TRUE, echo=TRUE}
#transform into a long df
phi.long <- melt(
  df.top10_subbredit.lda$phi,
  varnames = c("Topic", "Term"),
  value.name = "Phi") 

#plot 10 largest prob terms within each subject
phi.long %>% 
  group_by(Topic) %>% 
  top_n(10, Phi) %>% 
  ggplot(aes(reorder_within(Term, Phi, Topic), Phi)) + 
  geom_col(show.legend = FALSE) +
  coord_flip() +
  facet_wrap(~ Topic, scales = "free_y") +
  scale_x_reordered() + 
  xlab("Term") + 
  theme(
    axis.text.y = element_text(size = 5),
    strip.text = element_text(size = 5))
```
Similar analysis as the to the previous but now we see the probs so more precise

But we observe that lots of topics have the same terms. Therefore, adjusting k topics may be necessary.

We try with 7 topics

```{r topic_term_with_7, cache=TRUE, eval=TRUE, echo=TRUE}
docnames(df.top10_subreddit.dfm) <- df.top10_subreddit$subreddit
set.seed(1234) #To create reproducible results
df.top10_subbredit.lda <- textmodel_lda(
  x = df.top10_subreddit.dfm,
  k = 7)

#extract top 5 terms per topic
seededlda::terms(df.top10_subbredit.lda, 5)

# Extract subreddits per topic and rename output to match subreddit names
top_subreddits <- seededlda::topics(df.top10_subbredit.lda)
names(top_subreddits) <- df.top10_subreddit$subreddit
top_subreddits

#count the number of documents per topic
seededlda::topics(df.top10_subbredit.lda) %>% table()
#transform into a long df
phi.long <- melt(
  df.top10_subbredit.lda$phi,
  varnames = c("Topic", "Term"),
  value.name = "Phi") 

#plot 10 largest prob terms within each subject
phi.long %>% 
  group_by(Topic) %>% 
  top_n(10, Phi) %>% 
  ggplot(aes(reorder_within(Term, Phi, Topic), Phi)) + 
  geom_col(show.legend = FALSE) +
  coord_flip() +
  facet_wrap(~ Topic, scales = "free_y") +
  scale_x_reordered() + 
  xlab("Term") + 
  theme(
    axis.text.y = element_text(size = 5),
    strip.text = element_text(size = 5))
```

the distinction between topics is more clear now. We observe that topic1 may be related to 'hockey' and topic 2 to 'CFB' both sports so they have similar words expect specific ones. Topic 3 might more be related to 'news' as it has words like link (to refer to a source), interview, story, murder. Topic 4 is more related to feelings with words like time, day, feel, shit, pretty,... but harder to distinguish. Topic 5 might be related to government, nations or world maybe ? with words like police, american, russia, war, country. Topic 6 might well be a question topic with words like comment, contact,.... And topic 7 might more be a PC or PC gaming type of topic with words like game, steam, ram, build. we see that they reflect our subreddit which makes sense, now it would be interesting to take more that 10 subreddit as we might have a interesting regrouping of subreddits.

#### Topic document Analysis

```{r topic_doc_analysis, eval=TRUE, echo=TRUE}
set.seed(1234)
theta.long <- melt(
  df.top10_subbredit.lda$theta,
  varnames = c("Doc", "Topic"),
  value.name = "Theta")

# Ensure `theta.long` has the same document order as `df.top10_subreddit`
theta.long$Doc <- rep(df.top10_subreddit$subreddit, each = ncol(df.top10_subreddit.dfm$theta))

theta.long %>% 
  group_by(Topic) %>% 
  top_n(10, Theta) %>% 
  ggplot(aes(reorder_within(Doc, Theta, Topic), Theta)) + 
  geom_col(show.legend = FALSE) +
  coord_flip()+
  facet_wrap(~ Topic, scales = "free_y") +
  scale_x_reordered() + 
  xlab("Document") + 
  theme(
    axis.text.y = element_text(size = 5),
    strip.text = element_text(size = 5))
```
This confirms my previous analysis. we see topic one related to hockey, topic 2 related to CFB but interestingly lol is also here. Topic 3 was in fact reports, topic 4 was indeed feelings as it encompass a large range of subreddits. Topic 5 was indeed related to world news, news in general, topic 6 was indeed questions and topic 7 was indeed PC.

#### LDA diagnostics

```{r topic_diagnostics, eval=TRUE, echo=TRUE}
#topic prevalence
rev(sort(colSums(df.top10_subbredit.lda$theta)/sum(df.top10_subbredit.lda$theta)))
```
Topic 4 most prevalent.

```{r topic_diagnostics_2, cache=TRUE, eval=TRUE, echo=TRUE}
#using topicmodels
df.top10_subbredit.LDA <- LDA(
  convert(df.top10_subreddit.dfm, to = "topicmodels"),
  k = 7
)

topicmodels::terms(df.top10_subbredit.LDA, 5)
topicmodels::topics(df.top10_subbredit.LDA)
topicmodels::topics(df.top10_subbredit.LDA) %>% table()

topic_diagnostics(
  topic_model = df.top10_subbredit.LDA, 
  dtm_data = convert(df.top10_subreddit.dfm, to = "topicmodels"))
```

```{r topic_term_analysis_2, eval=TRUE, echo=TRUE}
#reproduce  term-topic analysis with this package
beta.long <- tidy(
  df.top10_subbredit.LDA,
  matrix = "beta") # equivalent to melt (with this package)

beta.long %>% 
  group_by(topic) %>% 
  top_n(10, beta) %>% 
  ggplot(aes(reorder_within(term, beta, topic), beta)) + 
  geom_col(show.legend = FALSE) +
  coord_flip()+
  facet_wrap(~ topic, scales = "free_y") +
  scale_x_reordered() + 
  xlab("Term") +
  theme(
    axis.text.y = element_text(size = 5),
    axis.text.x = element_text(size = 5),
    strip.text = element_text(size = 5))
```


### Extract description and display name of subreddit

```{r, eval=FALSE}
# Example list of subreddit names (replace this with your dataframe column)
#subreddit_names <- head(df.subreddit$subreddit, 1000)
# Select rows 1001 to 3500
subreddit_names <- df.subreddit$subreddit[3501:nrow(df.subreddit)]
#subreddit_names <- c("100yearsago", "AskReddit", "nonexistent_subreddit", "private_subreddit")

# Function to scrape display-name and description
get_subreddit_metadata <- function(subreddit) {
  # Construct the URL
  url <- paste0("https://www.reddit.com/r/", subreddit, "/")
  
  # Try to scrape the metadata
  tryCatch({
    # Read the HTML content of the subreddit page
    page <- read_html(url)
    
    # Extract the <shreddit-subreddit-header> element
    header_element <- page %>%
      html_node("shreddit-subreddit-header")  # Target the specific element
    
    # Extract the attributes
    display_name <- header_element %>% html_attr("display-name")
    description <- header_element %>% html_attr("description")
    
    # Return the results
    return(data.frame(
      subreddit = subreddit,
      display_name = display_name,
      description = description,
      stringsAsFactors = FALSE
    ))
  }, error = function(e) {
    # If an error occurs (e.g., subreddit doesn't exist), return NA
    return(data.frame(
      subreddit = subreddit,
      display_name = NA,
      description = NA,
      stringsAsFactors = FALSE
    ))
  })
}

# Introduce a delay between requests
subreddit_metadata <- do.call(rbind, lapply(subreddit_names, function(subreddit) {
  # Randomize sleep time between 3 and 5 seconds
  sleep_time <- runif(1, min = 1.5, max = 3)

# Sleep for the random duration
  Sys.sleep(sleep_time) # Wait 2 seconds between requests
  get_subreddit_metadata(subreddit)
}))

# Save the cleaned dataset to a new CSV file
#write_csv(subreddit_metadata, "subreddit_metadata.csv")
# View the results
print(subreddit_metadata)
```

put data in a csv file

```{r}
#write_csv(subreddit_metadata, "subreddit_metadata_first1000.csv")
#write_csv(subreddit_metadata, "subreddit_metadata_last2500.csv")
```

for computational purpose we don't run the code above as it takes a lot of time to scrape the data. We will use the data that we have already scraped.

remove NA
```{r, message=FALSE}
subreddit_first1000 <- read_csv("../../data/subreddit_metadata_first1000.csv")
subreddit_first1000 <- na.omit(subreddit_first1000)
subreddit_next2500 <- read_csv("../../data/subreddit_metadata_next2500.csv")
subreddit_next2500 <- na.omit(subreddit_next2500)
# subreddit_last <-read_csv("../../data/subreddit_metadata_last.csv")
# subreddit_last <- na.omit(subreddit_last)
# Combine the two dataframes
combined_subreddit_theme <- rbind(subreddit_first1000, subreddit_next2500)
```

clustering test similarity

```{r, warning=FALSE}
corpus <- Corpus(VectorSource(combined_subreddit_theme$description))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("en"))
dtm <- DocumentTermMatrix(corpus)
```

### clusters

```{r, cache=TRUE}
set.seed(123)
k <- 30  # Choose the number of clusters
clusters <- kmeans(as.matrix(dtm), centers = k)
combined_subreddit_theme$theme <- as.factor(clusters$cluster)
# Rename the column
combined_subreddit_theme <- combined_subreddit_theme %>%
  rename(`theme (cluster number)` = theme)
```

merge df with combined_subreddit_theme

```{r}
# Perform a left join to add the columns to 'df'
df <- df %>%
  left_join(combined_subreddit_theme, by = "subreddit")
```


using reddit api

```{r echo=FALSE}
# # Load necessary libraries
# library(httr)
# library(jsonlite)
# 
# # Replace these with your actual Reddit app's client ID, client secret, and redirect URI
# client_id <- "Qw1A2hzjh5bsUpKvvTSM-A"
# client_secret <- "_tk5YYxoVjiZqMmAFx3SfE9tF8rlZg"
# redirect_uri <- "http://localhost:8000"  # Make sure this matches the redirect URI you set in Reddit
# 
# # Step 1: Get the authorization URL
# authorize_url <- paste0("https://www.reddit.com/api/v1/authorize?client_id=", client_id,
#                        "&response_type=code&state=some_random_string&redirect_uri=", redirect_uri,
#                        "&scope=read")
# 
# # Print the URL and manually visit it to authorize the app
# cat("Visit this URL to authorize the app:\n", authorize_url, "\n")
# 
# # Once authorized, you will be redirected to the redirect_uri with a 'code' parameter in the URL
# # You need to capture the 'code' parameter, which you will use to obtain an access token
# 
# # Step 2: User visits the URL and grants permission, then gets the 'code' from the URL
# 
# # Prompt user to enter the code received after authorization
# auth_code <- readline(prompt = "Enter the code you received from Reddit: ")
# 
# # Step 3: Use the code to get an access token
# token_response <- POST("https://www.reddit.com/api/v1/access_token",
#                        authenticate(client_id, client_secret),
#                        body = list(
#                          grant_type = "authorization_code",
#                          code = auth_code,
#                          redirect_uri = redirect_uri
#                        ),
#                        encode = "form")
# 
# # Parse the response to get the access token
# token_data <- fromJSON(content(token_response, "text"))
# access_token <- token_data$access_token
# 
# # Step 4: Use the access token to make authorized API requests
# # Now you can make authenticated requests to Reddit's API
# 
# # Example: Get subreddit metadata (e.g., display name and description)
# get_subreddit_metadata <- function(subreddit) {
#   url <- paste0("https://www.reddit.com/r/", subreddit, "/about.json")
#   
#   # Send a GET request to Reddit API with the authorization header
#   response <- GET(url, add_headers(Authorization = paste("Bearer", access_token)))
#   
#   if (status_code(response) != 200) {
#     return(data.frame(
#       subreddit = subreddit,
#       display_name = NA,
#       description = NA,
#       stringsAsFactors = FALSE
#     ))
#   }
#   
#   # Parse the JSON response
#   data <- fromJSON(content(response, "text"))
#   
#   # Extract metadata
#   display_name <- data$data$display_name
#   description <- data$data$public_description
#   
#   # Return the result
#   return(data.frame(
#     subreddit = subreddit,
#     display_name = display_name,
#     description = description,
#     stringsAsFactors = FALSE
#   ))
# }
# 
# # Example list of subreddits to get metadata for
# subreddit_names <- c("r/programming", "r/learnprogramming")
# 
# # Get metadata for the subreddits
# subreddit_metadata <- do.call(rbind, lapply(subreddit_names, function(subreddit) {
#   Sys.sleep(2)  # Optional: add a delay between requests to avoid rate limiting
#   get_subreddit_metadata(subreddit)
# }))
# 
# # View the results
# print(subreddit_metadata)


```


## Sentiment Analysis

### Sentiment by Subreddit

Analyzing the top *30* subreddits (30 for vizualisation purposes) as a *whole*, focusing on average sentiment scores for each subreddit to get a general idea of the sentiment of the comments in each subreddit. We will use the AFINN, NRC, and BING lexicons to assign sentiment scores to words in the comments. We will then calculate the average sentiment score for each subreddit based on the sentiment scores of the words in the comments.

#### AFINN

The AFINN lexicon is a list of English words rated for valence with an integer between -5 (negative) and +5 (positive).

- Weaknessess : It may not capture the sentiment of slang, sarcasm, or other informal language. Also limited to the words in the lexicon.
- Strengths : It is simple and easy to use.

```{r sentiment_afinn, eval=TRUE, echo=TRUE}
#get top 50 subreddits
df.top30_subreddit <- df.subreddit %>%
  arrange(desc(nchar(body))) %>%
  filter(row_number() <= 30)

df.top30_subreddit.tokens <- df.top30_subreddit %>% unnest_tokens(word, body)

df.top30_subreddit.affin <- inner_join(df.top30_subreddit.tokens, get_sentiments("afinn"),
 by = c("word" = "word")) %>% group_by(subreddit) %>%
 summarize(Sentiment = mean(value)) %>% ungroup()

ggplot(df.top30_subreddit.affin, aes(x = Sentiment, y = reorder(subreddit, Sentiment), fill = Sentiment)) +
  geom_col() +
  scale_fill_viridis_c() +
  ylab("") + 
  #increase text size
  theme(axis.text.y = element_text(size = 10),
        axis.text.x = element_text(size = 10))
```

There seems to be more positive subreddits than negative ones.

We interestingly observe that based on the affin dictionnary the `news` and `politics` subreddit are the most negative, which could reflect a state of the world.

And `pc` Subreddits like  `pcmasterrace` or `builapc` are the most positive.

#### BING

The Bing sentiment lexicon categorizes words as either "positive" or "negative."

- Weaknesses: It lacks the granularity of numerical scores and may not capture the intensity of sentiment. It also does not account for neutral words.
- Strengths: Easy to use and understand. It provides a straightforward classification of words.

```{r bing_sentiment, eval=TRUE, echo=TRUE}
df.top30_subreddit.bing <- inner_join(df.top30_subreddit.tokens, get_sentiments("bing"),
 by = c("word" = "word")) %>% group_by(subreddit) %>%
 summarize(Sentiment = mean(ifelse(sentiment == "positive", 1, ifelse(sentiment == "negative", -1, 0)))) %>%
 select(subreddit, Sentiment)
 
ggplot(df.top30_subreddit.bing, aes(x = Sentiment, y = reorder(subreddit, Sentiment), fill = Sentiment)) +
  geom_col() +
  scale_fill_viridis_c() +
  ylab("") + 
  #increase text size
  theme(axis.text.y = element_text(size = 10),
        axis.text.x = element_text(size = 10))
```

BING seems to have a bit the same proporiton of positive and negative as the AFINN dictionnary. Indeed, we observe the same pattern with `pcmasterrace` at the top and `news` at the bottom, but there is slight variation when compared to the AFINN dictionnary.

#### NRC

The NRC Emotion Lexicon (EmoLex) associates words with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (positive and negative).

- Weaknesses: More complex to implement and interpret. It may require more computational resources.
- Strengths: Provides a more detailed analysis by categorizing words into multiple emotions. It can capture a wider range of sentiments and emotions.

```{r nrc_sentiment, eval=TRUE, echo=TRUE}
df.top30_subreddit.nrc <- inner_join(df.top30_subreddit.tokens, get_sentiments("nrc"),
 by = c("word" = "word")) %>% group_by(subreddit) %>%
 summarize(Sentiment = mean(ifelse(sentiment == "positive", 1, ifelse(sentiment == "negative", -1, 0)))) %>%
 select(subreddit, Sentiment)

ggplot(df.top30_subreddit.nrc, aes(x = Sentiment, y = reorder(subreddit, Sentiment), fill = Sentiment)) +
  geom_col() +
  scale_fill_viridis_c() +
  ylab("") + 
  #increase text size
  theme(axis.text.y = element_text(size = 10),
        axis.text.x = element_text(size = 10))
```

Through the NRC dictionnary we observe far more positive subreddits than negative ones. This inbalance could be explain by the fact that the NRC dictionnary is more biased towards positive words.

We still observe quite the same patterns though with `pcmasterrace`  at the top and `news` at the bottom.

#### Summary

To summarize it is interesting to observe that the NRC classification is totally different from the AFINN and BING classification. Indeed, the NRC classification seems to be more biased towards positive words but on the other hand it shall be more precise as it has more categories. 

Why one to choose ? It depends on the goal of the analysis. If we want a simple and easy to use classification we can use AFINN or BING. If we want a more precise classification we can use NRC.

### Sentiment by comment

Now to have a more precise understanding of the sentiment of the comments, we will analyze the sentiment per comment for each dictionary (AFINN, BING, NRC). So that we can choose the best dictionary for our analysis.

We will analyze a sample of small comments to have a better understanding.

#### Afinn

```{r sample_afinn_comment, eval=TRUE, echo=TRUE}
#define a random sample of 100 comments
set.seed(1234)
df.sample <- df %>% sample_n(100) %>%
  #select only columns 'id' and 'body'
  select(id, body, subreddit)

# use afinn to analyse sentiment all the rows based on column body
df.sentiment <- inner_join(df.sample %>% unnest_tokens(word, body),
 get_sentiments("afinn"), by = c("word" = "word")) %>% group_by(id) %>%
 summarize(Sentiment = mean(value)) %>% inner_join(df.sample, by = "id")

# Normalize the sentiment scores
df.sentiment.a <- df.sentiment %>%
 mutate(Normalized_Sentiment = (Sentiment - min(Sentiment)) / (max(Sentiment) - min(Sentiment)))

# show the sentiment of the comments
reactable(df.sentiment.a,
          resizable = TRUE,
          defaultPageSize = 2,
          sortable = TRUE,
          searchable = TRUE,
          filterable = TRUE,
          pagination = TRUE,
          highlight = TRUE,
          #add possibility to increase rows per page
          pageSizeOptions = c(5, 10, 15, 20)
          )
#save data as 'AFINN_sentiment_sample.csv'
#write_csv(df.sentiment, "../../data/AFINN_sentiment_sample.csv")
```

#### Bing

```{r sample_bing_comment, eval=TRUE, echo=TRUE}
df.sentiment <- inner_join(df.sample %>% unnest_tokens(word, body),
 get_sentiments("bing"), by = c("word" = "word")) %>% group_by(id) %>%
 summarize(Sentiment = mean(ifelse(sentiment == "positive", 1, ifelse(sentiment == "negative", -1, 0)))) %>%
 inner_join(df.sample, by = "id")

# Normalize the sentiment scores
df.sentiment.b <- df.sentiment %>%
 mutate(Normalized_Sentiment = (Sentiment - min(Sentiment)) / (max(Sentiment) - min(Sentiment)))

#show 
reactable(df.sentiment.b,
          resizable = TRUE,
          defaultPageSize = 2,
          sortable = TRUE,
          searchable = TRUE,
          filterable = TRUE,
          pagination = TRUE,
          highlight = TRUE,
          #add possibility to increase rows per page
          pageSizeOptions = c(5, 10, 15, 20)
          )

#save data as 'AFINN_sentiment_sample.csv'
#write_csv(df.sentiment, "../../data/BING_sentiment_sample.csv")
```

After reading the comments, we observe that they are mostly right. For example this comment was classified as neutral with a score of 0 : [Dude Congrats Im hoping to lose a large amount of weight in the same amount of time Huge inspiration here Keep kicking ass]

And this comment was classified as quite negative with a score of -2 : [Why did the chicken cross the road It didnt HHAHHAHAHAHHAHAHHA sorry that wasnt funny 472]. It seems, it did not understood the context

#### NRC

```{r sample_NRC_comment}
#use NRC to analyse sentiment all the rows based on column body
df.sentiment <- inner_join(df.sample %>% unnest_tokens(word, body),
 get_sentiments("nrc"), by = c("word" = "word")) %>% group_by(id) %>%
 summarize(Sentiment = mean(ifelse(sentiment == "positive", 1, ifelse(sentiment == "negative", -1, 0)))) %>%
 inner_join(df.sample, by = "id")

# Normalize the sentiment scores
df.sentiment.n <- df.sentiment %>%
 mutate(Normalized_Sentiment = (Sentiment - min(Sentiment)) / (max(Sentiment) - min(Sentiment)))

#show 
reactable(df.sentiment.n,
          resizable = TRUE,
          defaultPageSize = 2,
          sortable = TRUE,
          searchable = TRUE,
          filterable = TRUE,
          pagination = TRUE,
          highlight = TRUE,
          #add possibility to increase rows per page
          pageSizeOptions = c(5, 10, 15, 20)
          )

#save data as 'AFINN_sentiment_sample.csv'
#write_csv(df.sentiment, "../../data/NRC_sentiment_sample.csv")
```

We observe here that comments like "Night time moon light and a light snow The glow of the moon and the silence is amazing", which evokes a serene and appreciative tone for natural beauty suggest a strong positive emotion. The three dictionaries seem to have classified this comment as positive with the BING and AFINN dicitonnary attributing a higher score than the NRC.

#### Vizualising the Sentiment distribution based on a dictionnary

```{r sent_dist, cache=TRUE}
# use afinn to analyse sentiment all the rows based on column body
df.sentiment.a <- inner_join(df %>% unnest_tokens(word, body),
 get_sentiments("afinn"), by = c("word" = "word")) %>% group_by(id) %>%
 summarize(Sentiment = mean(value)) %>% inner_join(df.sample, by = "id")

# use bing to analyse sentiment all the rows based on column body
df.sentiment.b <- inner_join(df %>% unnest_tokens(word, body),
 get_sentiments("bing"), by = c("word" = "word")) %>% group_by(id) %>%
 summarize(Sentiment = mean(ifelse(sentiment == "positive", 1, ifelse(sentiment == "negative", -1, 0)))) %>%
 inner_join(df, by = "id")

#use NRC to analyse sentiment all the rows based on column body
df.sentiment.n <- inner_join(df %>% unnest_tokens(word, body),
 get_sentiments("nrc"), by = c("word" = "word")) %>% group_by(id) %>%
 summarize(Sentiment = mean(ifelse(sentiment == "positive", 1, ifelse(sentiment == "negative", -1, 0)))) %>%
 inner_join(df, by = "id")


#plot the three sentiment distribution on one using viridis color
ggplot() +
  geom_density(data = df.sentiment.a, aes(x = Sentiment, fill = "AFINN"), alpha = 0.5) +
  geom_density(data = df.sentiment.b, aes(x = Sentiment, fill = "BING"), alpha = 0.5) +
  geom_density(data = df.sentiment.n, aes(x = Sentiment, fill = "NRC"), alpha = 0.5) +
  scale_fill_viridis_d() +
  theme_minimal()
```

- The AFINN method shows a relatively flat distribution with a slight peak around 0. This suggests that the sentiment scores are spread out, with a slight tendency towards neutrality.

- The Bing method displays a bimodal distribution with two distinct peaks, one around -1 and another around 1. This indicates that the sentiment scores are polarized, with many comments being classified as either quite negative or quite positive. Which is logical based on the previous analysis

- The NRC method has a sharp peak at 0, suggesting that most of the sentiment scores are neutral. This implies that the NRC method tends to classify a large number of comments as neutral, with fewer comments being classified as strongly positive or negative. But we note that the NRC is also quite condensed

#### Normalization to correctly compare

```{r norm_sent_dist, eval=TRUE, echo=TRUE}
# Normalize the sentiment scores
df.sentiment.a <- df.sentiment.a %>%
  mutate(Normalized_Sentiment = (Sentiment - min(Sentiment)) / (max(Sentiment) - min(Sentiment)))

df.sentiment.b <- df.sentiment.b %>%
  mutate(Normalized_Sentiment = (Sentiment - min(Sentiment)) / (max(Sentiment) - min(Sentiment)))

df.sentiment.n <- df.sentiment.n %>%
  mutate(Normalized_Sentiment = (Sentiment - min(Sentiment)) / (max(Sentiment) - min(Sentiment)))

# Create the ggplot object
p <- ggplot() +
  geom_density(data = df.sentiment.a, aes(x = Normalized_Sentiment, fill = "AFINN"), alpha = 0.3) +
  geom_density(data = df.sentiment.b, aes(x = Normalized_Sentiment, fill = "BING"), alpha = 0.5) +
  geom_density(data = df.sentiment.n, aes(x = Normalized_Sentiment, fill = "NRC"), alpha = 0.5) +
  scale_fill_viridis_d() +
  theme_minimal()

# Convert the ggplot object to an interactive plotly object
interactive_plot <- ggplotly(p)

# Adjust the height and width 
interactive_plot <- layout(interactive_plot, width = 600, height = 400)
# Display the interactive plot
interactive_plot
```

#### Summary

We might choose one method over the others. For example, if we need a method that captures strong sentiments, Bing might be more suitable. For a more balanced view, AFINN could be the way to go. With more neutral comments, NRC might be the best choice.

#### LLM interpretation

We are going to use some LLM on a random sample to judge the accuracy of each dictionnary and judge which one to take.

prompt : 'Assign a sentiment score to this reddit comment from 0 (extremely negative) to 1 (extremely positive)'

Loop through the comments and assign a sentiment score 

## Analyzing the relationship between sentiment and score

There seem to be two outliers in this plot, let's examine them


There seems to be no relation between sentiment and score

### Apply The findings on the whole dataset

- investigate why it reduces the number of comment

```{r}
# use afinn to analyse sentiment all the rows based on column body
df.sentiment <- inner_join(df %>% unnest_tokens(word, body),
 get_sentiments("afinn"), by = c("word" = "word")) %>% group_by(id) %>%
 summarize(Sentiment = mean(value)) %>% inner_join(df, by = "id")

# Normalize the sentiment scores
df.sentiment <- df.sentiment %>%
 mutate(Normalized_Sentiment = (Sentiment - min(Sentiment)) / (max(Sentiment) - min(Sentiment)))

#new column for sentiment category
df.sentiment$sentiment_category <- ifelse(df.sentiment$Sentiment >= 1, "positive",
 ifelse(df.sentiment$Sentiment <= -1, "negative", "neutral"))

df.sentiment$sentiment_category

# save the data as a .csv file
#write_csv(df.sentiment, "../../data/AFINN_sentiment.csv")
```

## Score

```{r}
df_hugo <- df %>%
  select(-c("X","author_flair_text","id"))

score_2 <- sum(df_hugo$score==2)
score_1 <- sum(df_hugo$score==1)
score_0 <- sum(df_hugo$score==0)

print(paste0("Total number of comments without Up or Downvotes is ", score_1))
print(paste0("Total number of comments with only one Up or Downvote is ", score_0 + score_2))
print(paste0(round((100*(score_0 + score_1 + score_2)/138070),digits = 2), "% of the comments have 0 or 1 upvote/downvote"))
```

```{r}
# Filtering scores to focus on the range from -10 to 100
range_filtered <- df_hugo %>%
  filter(score >= -10 & score <= 20)

# Plotting the distribution of the 'score' column from -10 to 100 using ggplot2
ggplot(range_filtered, aes(x = score)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Score Column from -10 to 20",
       x = "Score",
       y = "Frequency") +
  theme_minimal() +
  theme(panel.grid.major = element_line(color = "grey", linetype = "dashed", size = 0.5))
```
```{r}
print(summary(df_hugo$score))
```

```{r}
df_high_score <- df_hugo %>%
  filter(score %in% (10:3967))

ggplot(df_high_score, aes(x = score)) +
  geom_histogram(bins = 30, fill = "lightblue", color = "black", alpha = 0.7) +
  scale_y_log10() +
  scale_x_log10()+
  labs(title = "Log-Distribution of High Scoring (10+ upvotes) Comments",
       x = "Score",
       y = "Log(Frequency)") +
  theme_minimal() +
  scale_y_continuous(labels = scales::label_comma())
```


```{r}
df_low_score <- df_hugo %>%
  filter(score %in% (-302:-10))

# Plotting the distribution of low scoring comments using ggplot2 with log transformation on both axes
ggplot(df_low_score, aes(x = score)) +
  geom_histogram(bins = 30, fill = "lightblue", color = "black", alpha = 0.7) +
  scale_y_log10() +
  scale_x_reverse() +
  labs(title = "Log-Distribution of Low Scoring (-10 and below) Comments",
       x = "Score",
       y = "Log(Frequency)") +
  theme_minimal() +
  scale_y_continuous(labels = scales::label_comma())

```
```{r}
# Counting unique subreddits in the dataset
unique_subreddits <- n_distinct(df_high_score$subreddit)
print(paste0("Total number of unique subreddits in the High-score dataset: ", unique_subreddits))
```
Only keep the subreddits with 5 or more comments
```{r}
# Analyzing subreddit-wise tendency of high scores
df_subreddit_high_score <- df_high_score %>%
  group_by(subreddit) %>%
  summarise(avg_score = mean(score), median_score = median(score), comment_count = n()) %>%
  filter(comment_count >= 5) %>%
  arrange(desc(avg_score))

# Printing subreddit analysis
df_subreddit_high_score
```


```{r}
# Plotting histogram of the number of comments per subreddit for high scoring comments without subreddit names
count_distribution <- df_high_score %>%
  group_by(subreddit) %>%
  summarise(comment_count = n()) %>%
  filter(comment_count >= 5) %>%
  arrange(desc(comment_count))

# Plotting the distribution of number of comments per subreddit
ggplot(count_distribution, aes(x = comment_count)) +
  geom_histogram(bins = 30, fill = "darkorange", alpha = 0.8) +
  labs(title = "Distribution of Number of Comments per Subreddit for High Scoring Comments",
       x = "Number of Comments",
       y = "Frequency") +
  theme_minimal()
```
```{r}
# Plotting average score by subreddit for high scoring comments with 5 or more comments
ggplot(df_subreddit_high_score, aes(x = reorder(subreddit, avg_score), y = avg_score)) +
  geom_bar(stat = "identity", fill = "steelblue", alpha = 0.8) +
  coord_flip() +
  labs(title = "Average Score by Subreddit for High Scoring Comments (5 or more comments)",
       x = "Subreddit",
       y = "Average Score") +
  theme_minimal()
```
```{r}
# Analyzing subreddit-wise tendency of low scores
df_subreddit_low_score <- df_low_score %>%
  group_by(subreddit) %>%
  summarise(avg_score = mean(score), median_score = median(score), comment_count = n()) %>%
  filter(comment_count >= 2) %>%
  arrange(avg_score)

# Printing subreddit analysis for low scores
df_subreddit_low_score
```

```{r}
ggplot(df_subreddit_low_score, aes(x = avg_score, y = reorder(subreddit, avg_score))) +
  geom_bar(stat = "identity", fill = "red", alpha = 0.8) +
  scale_x_reverse(limits = c(0, min(df_subreddit_low_score$avg_score))) +
  labs(title = "Average Score by Subreddit for Low Scoring Comments (5 or more comments)",
       x = "Subreddit",
       y = "Average Score") +
  theme_minimal()
```

```{r}
# Plotting histogram of the number of comments per subreddit for low scoring comments without subreddit names
count_distribution_low <- df_low_score %>%
  group_by(subreddit) %>%
  summarise(comment_count = n()) %>%
  filter(comment_count >= 2) %>%
  arrange(desc(comment_count))

# Plotting the distribution of number of comments per subreddit for low scores
ggplot(count_distribution_low, aes(x = comment_count)) +
  geom_histogram(bins = 20, fill = "darkred", alpha = 0.8) +
  labs(title = "Distribution of Number of Comments per Subreddit for Low Scoring Comments",
       x = "Number of Comments",
       y = "Frequency") +
  theme_minimal()

```



```{r}
# Clustering analysis based on number of comments and average score for high scoring subreddits
df_subreddit_cluster <- df_high_score %>%
  group_by(subreddit) %>%
  summarise(avg_score = mean(score), comment_count = n())

# Scaling the data for clustering
cluster_data <- df_subreddit_cluster %>%
  select(comment_count, avg_score) %>%
  scale()

# Determining the optimal number of clusters using the elbow method
set.seed(123)
fviz_nbclust(cluster_data, kmeans, method = "wss") +
  labs(title = "Elbow Method for Determining Optimal Number of Clusters",
       x = "Number of Clusters",
       y = "Within-Cluster Sum of Squares") +
  theme_minimal()
```
We'll settle with 5 clusters

```{r}
# Running k-means clustering with 5 clusters
set.seed(123) # For reproducibility
kmeans_result <- kmeans(cluster_data, centers = 5)
df_subreddit_cluster$cluster <- as.factor(kmeans_result$cluster)

# Plotting the clusters
ggplot(df_subreddit_cluster, aes(x = comment_count, y = avg_score, color = cluster)) +
  geom_point(size = 3, alpha = 0.7) +
  labs(title = "Clustering of Subreddits Based on Number of Comments and Average Score",
       x = "Number of Comments",
       y = "Average Score") +
  theme_minimal() 
```

## Additional Variable
### Adding Length of comment as a feature

```{r, eval=TRUE, echo=TRUE}
#show length of one random comment
df$body[1] %>% nchar() #char shows the number of characters in a string
#to show the number of words in a comment we can use str_count
df$body[1] %>% str_count("\\w+")
df$body[1]
#add length of comment as a feature
df$length <- str_count(df$body, "\\w+")
head(df)

#get summary statistics on column length
summary(df$length)
```