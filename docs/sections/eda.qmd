```{r viridis, eval=TRUE, echo=FALSE}
#save some viridis colors
viridis_1 <- viridis(10)[1]
viridis_2 <- viridis(10)[2]
viridis_3 <- viridis(10)[3]
viridis_4 <- viridis(10)[4]
viridis_5 <- viridis(10)[5]
viridis_6 <- viridis(10)[6]
viridis_7 <- viridis(10)[7]
viridis_8 <- viridis(10)[8]
viridis_9 <- viridis(10)[9]
viridis_10 <- viridis(10)[10]
```

# EDA

## Intro: General exploration

### Subreddits

For further discussions: a subreddit = A document

Here we take the idea that a 'document' is a subreddit.
It is like all comments that are part of one subreddit will be a document, this will help for text mining analysis down the line.
Here is a visualization of comments per subreddit

```{r top_10_sub, eval=TRUE, echo=TRUE}
#take the df data and group all comments by subreddit
df.subreddit <- df %>%
  group_by(subreddit) %>%
  summarise(body = paste(body, collapse = " "))

# show number of subreddits
cat("Number of subreddits:", nrow(df.subreddit), "\n")

# show top 10 subreddits by length in a bar plot
df.subreddit %>%
  mutate(subreddit = reorder(subreddit, nchar(body), fill = nchar(body))) %>%
  top_n(10, nchar(body)) %>%
  ggplot(aes(x = subreddit, y = nchar(body))) +
  geom_col(fill = viridis_1) +
  coord_flip() +
  labs(title = "Top 10 Subreddits by Length", x = "Subreddit", y = "Length") +
  #increase text size
  theme(axis.text.y = element_text(size = 10),
        axis.text.x = element_text(size = 10))
```

We observe that AskReddit is by far the largest Subreddit.
Half of the top 10 subreddits are for large audiences (i.e. news/worldnews, funny, etc.), while the other half is for more niche subjects.

### Words

Here, we have a look at the most used words within comments while controlling for "meaningless" words, such as common stopwords (the, it, etc.) and reddit-related redundant words (reddit, subreddit, etc.)

```{r word_freq, eval=TRUE, echo=TRUE, cache=TRUE}
# preprocess the text data
df.subreddit.cp <- corpus(df.subreddit$body)
docnames(df.subreddit.cp) <- df.subreddit$subreddit  # Assign subreddit names as document identifiers

df.subreddit.tk <- tokens(
  df.subreddit.cp,
  remove_numbers = TRUE,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_url = TRUE,
  remove_hyphens = TRUE,
  remove_separators = TRUE
)
df.subreddit.tk <- df.subreddit.tk %>%
  tokens_tolower() %>%
  tokens_remove(stop_words$word) %>%
  tokens_remove(c("reddit", "subreddit", "amp", "gt", "deleted", "x+"))

# tf and tf-idf analysis
#tf
df.subreddit.dfm <- dfm(df.subreddit.tk)
tf <- rowSums(t(df.subreddit.dfm))
tf <- data.frame(term = names(tf), count = tf) %>%
  tibble() %>%
  arrange(desc(count))

#freq per documents
tidy(df.subreddit.dfm) %>%
  arrange(desc(count))

#freq per term
df.subreddit.freq <- textstat_frequency(df.subreddit.dfm)
head(df.subreddit.freq,20)

#plot 20 most frequent words
df.subreddit.freq %>%
  top_n(20, frequency) %>%
  ggplot(aes(
    x = reorder(feature, frequency),
    y = frequency)) +
  geom_bar(stat = "identity", fill = viridis_2) +
  coord_flip() +
  xlab("Frequency") +
  ylab("term") +
  #increase size of text
  theme(axis.text.y = element_text(size = 10),
        axis.text.x = element_text(size = 10))

```

We observe here a lot of abbreviations that are not taken into account by the stop words.
Indeed, a lot of word are still meaningless without context (I'm, You're, etc.)

We then produce a word cloud for better visualization of the most common words

```{r textplot_wordcloud, eval=TRUE, echo=TRUE}
#textplot_wordcloud(df.subreddit.dfm)

df.subreddit.freq %>%
  top_n(30, frequency) %>%
  ggplot(aes(label = feature, size = frequency, color = frequency)) +
  geom_text_wordcloud() +
  scale_size_area(max_size = 14) +
  scale_color_viridis_c() +
  theme_minimal()
```

We can observe that the most frequent words are 'people', 'time', 'game'.

#### Term-frequency - Inverse document frequency (TF-IDF)

This methods allows for an exploration of unique word usage per subreddits.

$$TF=\frac{Number\ of\ times\ the\ term\ appears\ in\ the\ document}{Total\ number\ of\ terms\ in\ the\ document}$$

$$IDF=\log{\frac{Number\ of\ times\ the\ term\ appears\ in\ the\ document}{Total\ number\ of\ terms\ in\ the\ document}}$$

These two equations combined show when a word is important within a single subreddit, they balance the commonality within a document (a subreddit) and the rarity within the entire corpus (reddit as a whole)

```{r tf_idf, eval=TRUE, echo=TRUE}
#tf-idf
df.subreddit.tfidf <- dfm_tfidf(df.subreddit.dfm)
df.subreddit.tfidf.tidy <- tidy(df.subreddit.tfidf) %>%
  bind_tf_idf(term = term, document = document, n = count) %>%
  arrange(desc(tf_idf))
head(df.subreddit.tfidf.tidy, n = 20) %>% flextable() %>% autofit()
```

So we have here lots of 'small' and 'rare' subreddit that have specific words that are not used in other subreddit.
This is interesting as it shows us the specificity of each subreddit.

```{r tf_idf_plot, eval=TRUE, echo=TRUE}
#plot per document
df.subreddit.dfm %>%
  tidy() %>%
  top_n(12, count) %>%
  ggplot(aes(x = term, y = count)) +
  geom_bar(stat = "identity", fill = viridis_3) +
  coord_flip() +
  theme(axis.text.y = element_text(size = 10),
        axis.ticks.y = element_blank()) +
  facet_wrap(~document, ncol = 3)
```

So in summary: 

- We observe that AskReddit has lots of words frequency which makes sense as it is the largest subreddit.

-   We also observe that the term 'game' is associated in term of frequency with sports subreddits like CFB (College Football) and Hockey.

-   Interestingly the subreddit for the game League of Legends is associated with the term 'happy', which show us that the community may be the happiest, careful bias here, it is just a word frequency analysis.

-   In the subreddits 'pics' we observe a lot of 'gem' term which indicates treasure or something valuable.
    This is interesting as it is a subreddit for sharing pictures.

-   The subreddits 'news' is associated in terms of frequency with the term 'people' which is logical as news is about people.

#### Similarity between Subreddits/Documents

we analyze similarities and dissimilarities between the documents (through words) in the data.
For this, we will extensively use the quanteda package and the objects created above.

Then use the functions textstat_simil() and textstat_dist() to compute the Jaccard index matrix.

We choose the Jaccard index because it is a good measure of similarity between two documents.

```{r similarity_subreddits, eval=TRUE, echo=TRUE, cache=TRUE}
#We restrict ourselves to a small subset of the highest tf-idf of the data to avoid memory issues and readability issues in the plots.
# df.subreddit.tfidf.small <- df.subreddit.tfidf[1:1000,]
# df.subreddit.tfidf.small
# 
# #compute the jaccard index
# #comment out the next line to avoid recomputing the jaccard index
# df.subreddit.jac <- textstat_simil(
#   df.subreddit.tfidf.small, method = "jaccard", margin = "documents")
# 
# # Heatmap representation of similarities between subreddits
# df.subreddit.jac.mat <- melt(as.matrix(df.subreddit.jac))

#save
#write.csv(df.subreddit.jac.mat, "../../data/df.subreddit.jac.mat_1000.csv")

#load
df.subreddit.jac.mat <- read.csv("../../data/df.subreddit.jac.mat_1000.csv")

#heatmap representation of similariteis between subreddits
## jaccard
p <- ggplot(data = df.subreddit.jac.mat, 
            mapping = aes(x = Var1, y = Var2, fill = value, text = paste("Subreddit 1:", Var1, "<br>Subreddit 2:", Var2, "<br>Jaccard:", round(value, 2)))) +
  scale_fill_gradient2(
    low = viridis_1, 
    high = viridis_5, 
    mid = viridis_10, 
    midpoint = 0.5, 
    limit = c(0, 1), 
    name = "Jaccard") +
  geom_tile() + 
  xlab("") + 
  ylab("") +
  theme(axis.text.x = element_blank(),  # Remove x-axis text
        axis.text.y = element_blank(),  # Remove y-axis text
        axis.ticks = element_blank())   # Remove axis ticks
print(p)
```

We observe that similarity using Jaccard index on the top 1000 tf-idf words is inconclusive.
Indeed, we do not observe any clear pattern.
This is due to the fact that we have a lot of words that are specific to each subreddit.
Therefore, we will try to cluster the subreddits to see if we can regroup them in a more meaningful way.

#### Clustering of Documents

Hierarchical clustering and K-means.
The first one is applied on the dissimilarities (Euclidean).
The second one is applied on the features, here, TF-IDF.
To illustrate the methods, we decide to create 3 clusters.

```{r clustering_subreddits, eval=TRUE, echo=TRUE}
df.subreddit.tfidf.small <- df.subreddit.tfidf[1:50,]
df.subreddit.eucl <- textstat_dist(
  df.subreddit.tfidf.small, method = "euclidean", margin = "documents")
#hierarchical
df.subreddit.hc <- hclust(as.dist(df.subreddit.eucl))
## df.subreddit.hc <- hclust(as.dist(1 - df.subreddit.jac)) # use this line for Jaccard
## df.subreddit.hc <- hclust(as.dist(1 - df.subreddit.cos)) # use this line for Cosine
plot(df.subreddit.hc)
df.subreddit.clust <- cutree(df.subreddit.hc, k = 3)
df.subreddit.clust
#k-means

df.subreddit.km <- kmeans(df.subreddit.tfidf.small, centers = 3)
df.subreddit.km$cluster
```

Only 50 where selected to show, but same applies to all subreddits.
Again, we observe that the clustering is not very clear, even worse.
We have a lot of subreddits that are not well regrouped.
his is due to the fact that we have a lot of specific words in each subreddit.
Therefore, we will try to use topic modeling to see if we can regroup the subreddits in a more meaningful way.

## Themes and topics

### Latent Semantic Analysis

LSA is a core technique in topic modelling, it uses matrices to separate topics and terms.

(see code for more detail on the results)

```{r lsa_subreddit, eval=TRUE, echo=TRUE}
#select subset of data of the top 10 subreddits for readability purposes
df.top10_subreddit <- df.subreddit %>%
  arrange(desc(nchar(body))) %>%
  filter(row_number() <= 10)

#corpus
df.top10_subreddit.cp <- corpus(df.top10_subreddit$body)
# Assign subreddit names as document identifiers
docnames(df.top10_subreddit.cp) <- df.top10_subreddit$subreddit

#token
df.top10_subreddit.tk <- tokens(
  corpus(df.top10_subreddit$body),
  remove_numbers = TRUE,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_url = TRUE,
  remove_separators = TRUE
)
df.top10_subreddit.tk <- df.top10_subreddit.tk %>%
  tokens_tolower() %>%
  tokens_remove(stop_words$word) %>%
  tokens_remove(c("reddit", "subreddit", "amp", "gt", "deleted", "x+")) %>%
  tokens_replace(
    pattern = hash_lemmas$token,
    replacement = hash_lemmas$lemma)

# Create Document-Feature Matrix (DFM)
df.top10_subreddit.dfm <- dfm(df.top10_subreddit.tk)

# Explicitly set document names in the DFM to the subreddit names
docnames(df.top10_subreddit.dfm) <- df.top10_subreddit$subreddit
df.top10_subreddit.dfm

#use textmodel_lsa() on dfm using n dimensions
df.subreddit.lsa <- textmodel_lsa(
  x = df.top10_subreddit.dfm,
  nd = 5)

head(df.subreddit.lsa$docs)
#interpretation
## we look at the five terms with the largest values and the five ones with the lowest value (i.e., largest negative value)

n.terms <- 5
##for dim 2
w.order <- sort(df.subreddit.lsa$features[, 2], decreasing = TRUE)
w.top2 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))
##for dim 3
w.order <- sort(df.subreddit.lsa$features[, 3], decreasing = TRUE)
w.top3 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))

w.top2
w.top3
```

For readability, we apply it first on the top 10 subreddits to assess its application in our context.

-   AskReddit is strongly associated with terms like "people" and "job," indicating discussions about personal experiences and employment.
-   funny has a moderate association with "people" and "post," suggesting content related to social interactions and posts.
-   CFB and league of legends are associated with terms like "team" and "game," indicating discussions about sports and gaming.

The terms are associated with different dimensions, indicating their importance in the context of the documents.
Here are some key terms and their associations:

-   Dimension 1: "people" (0.1511), "question" (0.1346), "post" (0.1174)
-   Dimension 2: "team" (-0.2862), "play" (-0.2991), "game" (-0.5395)
-   Dimension 3: "fuck" (0.1837), "team" (0.1602), "call" (0.1255)
-   Dimension 4: "bowl" (0.1155), "boise" (0.0894), "steam" (-0.1784)
-   Dimension 5: "awesome" (-0.2124), "card" (-0.2553), "pc" (-0.2608)

However, we can see that it is not precise in our use-case, dim 4 and 2 and 3 seems quite similar..

We therefore inspect further with Singular Value Decomposition (SVD). The SVD coordinates help visualize the positioning of documents in the reduced-dimensional space, showing how closely related they are based on the terms they contain.

We restrict here the PCA for links between subreddits and topics to only dimension 2 and 3 to obverse its efficacy.

```{r pca, cache=FALSE, eval=TRUE, echo=TRUE}
set.seed(123)
# Restrict chart to terms that are mostly related to dim2 and dim3 
w.subset <- df.subreddit.lsa$features[ c(unique(c(names(w.top2), names(w.top3)))), 2:3]
# Create data frames for documents and features
docs_df <- as.data.frame(df.subreddit.lsa$docs[, 2:3])
features_df <- as.data.frame(w.subset) # to show all features (words) use instead : #features_df <- as.data.frame(df.subreddit.lsa$features[, 2:3])

# Rename columns for clarity
colnames(docs_df) <- c("Dim2", "Dim3")
colnames(features_df) <- c("Dim2", "Dim3")

# Create the ggplot biplot
p <- ggplot() +
  geom_segment(data = docs_df, 
               aes(x = 0, y = 0, xend = Dim2, yend = Dim3, text = rownames(docs_df)), 
               arrow = arrow(length = unit(0.2, "cm")), 
               color = "red") +
  geom_point(data = features_df, 
             aes(x = Dim2, y = Dim3, text = rownames(features_df)), 
             color = "black", size = 1) +
  xlab("Dim 2") +
  ylab("Dim 3") +
  theme_minimal()

# Convert ggplot to plotly for interactivity
p_interactive <- ggplotly(p, tooltip = "text")

# Update width and heigth of the plot
p_interactive <- p_interactive %>% layout(width = 800, height = 600)

# Display the interactive plot
p_interactive
```

We only show the features (words) that are mostly related to dim2 and dim3 so that the chart is not overcrowded.

We observe for example that the word `fuck` is associated with dim 3 and is interestingly inversly correlated to the word `happy` which makes sense.
This is more convincing than the similarity and clustering analysis we did before.
The subreddits are indeed better regrouped, in a more meaningful way.

### Latent Dirichlet Allocation

LDA is a Bayesian version of the probabilistic Latent Semantic Analysis.
It answers the question "*Given this type of distribution, what are some actual probability distributions I am likely to see?*"

(see code for more details on the results)

```{r lda_subreddit, cache=TRUE, eval=TRUE, echo=TRUE}
docnames(df.top10_subreddit.dfm) <- df.top10_subreddit$subreddit
set.seed(1234) #To create reproducible results
df.top10_subbredit.lda <- textmodel_lda(
  x = df.top10_subreddit.dfm,
  k = 10)

#extract top 5 terms per topic
top_terms <- seededlda::terms(df.top10_subbredit.lda, 5)
top_terms
# Extract subreddits per topic and rename output to match subreddit names
top_subreddits <- seededlda::topics(df.top10_subbredit.lda)
names(top_subreddits) <- df.top10_subreddit$subreddit
top_subreddits

#count the number of documents per topic
topic_counts <- seededlda::topics(df.top10_subbredit.lda) %>% table
topic_counts
```

Topic 7 with  subreddits such as AskReddit, funny, pics and news is the most popular topic with words like "people", "time", "im", "youre" which may represent human experience and emotions. They touch on aspects of daily life, feelings, and the passage of time, with a bit of raw expression thrown in. It's a mix of the mundane and the profound, capturing the essence of what it means to be human.

However, the distinction between topics is still not very clear. We observe a lot of overlapping terms within the differnt topics.

#### LDA diagnostics

```{r topic_diagnostics, eval=TRUE, echo=TRUE}
#topic prevalence
rev(sort(colSums(df.top10_subbredit.lda$theta)/sum(df.top10_subbredit.lda$theta)))
```

Topic 4 most prevalent by far which makes sense as it is related to feelings and human experience that people often use in their comments.

### Topic-term Analysis

```{r topic_term_analysis, eval=TRUE, echo=TRUE}
#transform into a long df
phi.long <- melt(
  df.top10_subbredit.lda$phi,
  varnames = c("Topic", "Term"),
  value.name = "Phi") 

#plot 10 largest prob terms within each subject
phi.long %>% 
  group_by(Topic) %>% 
  top_n(10, Phi) %>% 
  ggplot(aes(reorder_within(Term, Phi, Topic), Phi, fill = as.factor(Topic))) + 
  geom_col(show.legend = FALSE) +
  coord_flip() +
  facet_wrap(~ Topic, scales = "free_y") +
  scale_x_reordered() + 
  scale_fill_viridis_d() +
  xlab("Term") + 
  theme(
    axis.text.y = element_text(size = 5),
    strip.text = element_text(size = 5))
```

The Topic-Term Analysis produces similar results as the LDA. However this plot provides the probabilities, therefore refining our understanding.

We still observe that lots of topics have the same terms.Therefore, adjusting k topics may be helpful. We first try with 7 topics

```{r topic_term_with_7, cache=TRUE, eval=TRUE, echo=TRUE}
docnames(df.top10_subreddit.dfm) <- df.top10_subreddit$subreddit
set.seed(1234) #To create reproducible results
df.top10_subbredit.lda <- textmodel_lda(
  x = df.top10_subreddit.dfm,
  k = 7)

#extract top 5 terms per topic
seededlda::terms(df.top10_subbredit.lda, 5)

# Extract subreddits per topic and rename output to match subreddit names
top_subreddits <- seededlda::topics(df.top10_subbredit.lda)
names(top_subreddits) <- df.top10_subreddit$subreddit
top_subreddits

#count the number of documents per topic
seededlda::topics(df.top10_subbredit.lda) %>% table()
#transform into a long df
phi.long <- melt(
  df.top10_subbredit.lda$phi,
  varnames = c("Topic", "Term"),
  value.name = "Phi") 

#plot 10 largest prob terms within each subject
phi.long %>% 
  group_by(Topic) %>% 
  top_n(10, Phi) %>% 
  ggplot(aes(reorder_within(Term, Phi, Topic), Phi, fill = as.factor(Topic))) + 
  geom_col(show.legend = FALSE) +
  coord_flip() +
  facet_wrap(~ Topic, scales = "free_y") +
  scale_x_reordered() + 
  scale_fill_viridis_d() +
  xlab("Term") + 
  theme(
    axis.text.y = element_text(size = 5),
    strip.text = element_text(size = 5))
```

The distinction between topics is clearer now, we observe that:

- Topic 1 may be related to 'pc games' ("game", "pc", "steam", "play", etc.)

- Topic 2 seems to also be related to pc games, but more precisely to the game League of Legends, where terms such as "champions", "ping", "riot" and "team" are prevalent.

- Topic 3 is also related to some sorts of games, we can maybe infer that it related to sports more specifically, with the presence of words such as "fan".

- Topic 4 is harder to distinguish with the presence of conflicting words such as "play", "kill", "run", and "raid".

- Topic 5 seems to be related to feelings with words like "im", time", "day", "feel", etc.

- Topic 6 seems to be related to Crime and Justice with the presence of words such as "murder", "evidence", and "interview".

- Topic 7 seems to be related to news and world event with the presence of words such as "country", "american", "russia," "war".
  
We see that they reflect our subreddit which makes sense. It would be interesting to take more that 10 subreddit as we might uncover interesting grouping of subreddits but it is so computationally intensive that we refrain from doing that for now.

### Topic document Analysis

We will now analyze the distribution of topics across the documents (subreddits). This differs from the previous analysis, where we looked at the distribution of terms (words) across topics. Here, we look at the distribution of topics across documents (subreddits).

```{r topic_doc_analysis, eval=TRUE, echo=TRUE}
set.seed(1234)
theta.long <- melt(
  df.top10_subbredit.lda$theta,
  varnames = c("Doc", "Topic"),
  value.name = "Theta")

# Ensure `theta.long` has the same document order as `df.top10_subreddit`
theta.long$Doc <- rep(df.top10_subreddit$subreddit, each = ncol(df.top10_subreddit.dfm$theta))

theta.long %>% 
  group_by(Topic) %>% 
  top_n(10, Theta) %>% 
  ggplot(aes(reorder_within(Doc, Theta, Topic), Theta, fill = Theta)) + 
  geom_col(show.legend = FALSE) +
  coord_flip() +
  facet_wrap(~ Topic, scales = "free_y") +
  scale_x_reordered() + 
  scale_fill_viridis_c() + 
  xlab("Document") + 
  theme(
    axis.text.y = element_text(size = 5),
    strip.text = element_text(size = 5))
```

This confirms our previous analysis. We see topic 1 related to pc/games, topic 2 related to league of legends, etc.

### Topic modelling summary

For our first analysis, we regrouped ALL comments per subreddit. From the results we obtained, we can argue that the LDA produced the best results:

  - It was more efficient that the clustering and similarity analysis.
  - The subreddits are well regrouped, in a meaningful way.
  - The topics are well defined and they reflect our subreddits.
  - The topics are well dsitributed across the documents (subreddits), which is a good sign.

However, regrouping ALL the comments per subreddits is not the best way to analyze the data. Indeed, we have a lot of specific words that are not taken into account by the stop words. Therefore, we will try to group in a more meaningful way, using the 'description' and 'display name' of each subreddit, obtained  via web scraping.

### Web Scraping (Description and name)

First attempt to scrape data

```{r, eval=FALSE}
# Example list of subreddit names (replace this with your dataframe column)
#subreddit_names <- head(df.subreddit$subreddit, 1000)
# Select rows 1001 to 3500
subreddit_names <- df.subreddit$subreddit[5715:nrow(df.subreddit)]
#subreddit_names <- c("100yearsago", "AskReddit", "nonexistent_subreddit", "private_subreddit")

# Function to scrape display-name and description
get_subreddit_metadata <- function(subreddit) {
  # Construct the URL
  url <- paste0("https://www.reddit.com/r/", subreddit, "/")
  
  # Try to scrape the metadata
  tryCatch({
    # Read the HTML content of the subreddit page
    page <- read_html(url)
    
    # Extract the <shreddit-subreddit-header> element
    header_element <- page %>%
      html_node("shreddit-subreddit-header")  # Target the specific element
    
    # Extract the attributes
    display_name <- header_element %>% html_attr("display-name")
    description <- header_element %>% html_attr("description")
    
    # Return the results
    return(data.frame(
      subreddit = subreddit,
      print("sub"),
      display_name = display_name,
      description = description,
      stringsAsFactors = FALSE
    ))
  }, error = function(e) {
    # If an error occurs (e.g., subreddit doesn't exist), return NA
    return(data.frame(
      subreddit = subreddit,
      display_name = NA,
      description = NA,
      stringsAsFactors = FALSE
    ))
  })
}

# Introduce a delay between requests
subreddit_metadata <- do.call(rbind, lapply(subreddit_names, function(subreddit) {
  # Randomize sleep time between 3 and 5 seconds
  sleep_time <- runif(1, min = 2, max = 4)

# Sleep for the random duration
  Sys.sleep(sleep_time) # Wait 2 seconds between requests
  get_subreddit_metadata(subreddit)
}))

# Save the cleaned dataset to a new CSV file
#write_csv(subreddit_metadata, "subreddit_metadata.csv")
# View the results
print(subreddit_metadata)
```

Second attempt, better way to scrape data.

```{r, eval=FALSE}
# Function to scrape display-name and description
get_subreddit_metadata <- function(subreddit) {
  # Construct the URL
  url <- paste0("https://www.reddit.com/r/", subreddit, "/")
  
  tryCatch({
    # Make the GET request with a User-Agent
    response <- GET(url, add_headers("User-Agent" = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"))
    
    # Check HTTP status code
    if (status_code(response) != 200) {
      cat(paste("Failed to fetch:", subreddit, "Status Code:", status_code(response), "\n"))
      return(data.frame(
        subreddit = subreddit,
        display_name = NA,
        description = NA,
        stringsAsFactors = FALSE
      ))
    }
    
    # Parse the HTML content
    page <- read_html(content(response, "text"))
    
    # Extract the <shreddit-subreddit-header> element
    header_element <- page %>%
      html_node("shreddit-subreddit-header")
    
    # Check if the element exists
    if (is.null(header_element)) {
      cat(paste("No header element found for subreddit:", subreddit, "\n"))
      return(data.frame(
        subreddit = subreddit,
        display_name = NA,
        description = NA,
        stringsAsFactors = FALSE
      ))
    }
    
    # Extract the attributes
    display_name <- header_element %>% html_attr("display-name")
    description <- header_element %>% html_attr("description")
    
    # Return the metadata
    return(data.frame(
      subreddit = subreddit,
      display_name = display_name,
      description = description,
      stringsAsFactors = FALSE
    ))
  }, error = function(e) {
    cat(paste("Error for subreddit:", subreddit, "with error:", e$message, "\n"))
    return(data.frame(
      subreddit = subreddit,
      display_name = NA,
      description = NA,
      stringsAsFactors = FALSE
    ))
  })
}

# Example usage with delays
subreddit_names <- tail(df.subreddit$subreddit, 2190)
#print(subreddit_names)

request_counter <- 0

# Introduce random delays between requests
results <- do.call(rbind, lapply(subreddit_names, function(subreddit) {
  # Increment the request counter
  request_counter <<- request_counter + 1
  
  # Print the current request number
  cat(paste("Request number:", request_counter, "\n"))
  
  # Randomize sleep time between 2 and 4 seconds
  sleep_time <- runif(1, min = 2, max = 3.8)
  Sys.sleep(sleep_time) # Delay between requests
  
  # Fetch metadata
  get_subreddit_metadata(subreddit)
}))

# View results
print(results)


```

Store in csv file

```{r}
#write_csv(subreddit_metadata, "subreddit_metadata_first1000.csv")
#write_csv(results, "subreddit_metadata_last2500.csv")
```

For computational purpose we don't run the code above as it takes a lot of time to scrape the data. We will use the data that we have already scraped.

remove NA

```{r, message=FALSE}
subreddit_first1000 <- read_csv("../../data/subreddit_metadata_first1000.csv")
subreddit_first1000 <- na.omit(subreddit_first1000)
subreddit_next2500 <- read_csv("../../data/subreddit_metadata_next2500.csv")
subreddit_next2500 <- na.omit(subreddit_next2500)
subreddit_last <-read_csv("../../data/subreddit_metadata_last2500.csv")
subreddit_last <- na.omit(subreddit_last)
# Combine the two dataframes
combined_subreddit_theme <- rbind(subreddit_first1000, subreddit_next2500,subreddit_last)
# Replace "/r/" with "subreddit" in the 'description' column of the 'combined_subreddit_theme' dataframe
combined_subreddit_theme$description <- gsub("/r/", "subreddit ", combined_subreddit_theme$description)

# Remove emojis and special characters from the 'description' column
combined_subreddit_theme$description <- gsub("[^[:alnum:][:space:]]", "", 
                                             iconv(combined_subreddit_theme$description, 
                                                   from = "UTF-8", to = "ASCII", sub = ""))
# Remove observations with empty or whitespace-only descriptions
combined_subreddit_theme <- combined_subreddit_theme %>%
  filter(str_trim(description) != "")

# Remove observations with "weird" descriptions
combined_subreddit_theme <- combined_subreddit_theme %>%
  filter(
    str_trim(description) != "" &            # Remove empty or whitespace-only descriptions
    nchar(str_trim(description)) > 3 &       # Keep descriptions longer than 3 characters
    str_detect(description, "[A-Za-z0-9]")   # Ensure descriptions contain alphanumeric characters
  )

# View the cleaned dataframe
head(combined_subreddit_theme)


```

clustering test similarity

```{r, warning=FALSE}
corpus <- Corpus(VectorSource(combined_subreddit_theme$description))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("en"))
dtm <- DocumentTermMatrix(corpus)
```

### Clusters

```{r, cache=TRUE}
set.seed(123)
k <- 30  # Choose the number of clusters
clusters <- kmeans(as.matrix(dtm), centers = k)
combined_subreddit_theme$theme <- as.factor(clusters$cluster)
# Rename the column
combined_subreddit_theme <- combined_subreddit_theme %>%
  rename(`theme (cluster number)` = theme)
# Remove duplicates based on the "subreddit" column
combined_subreddit_theme <- combined_subreddit_theme[!duplicated(combined_subreddit_theme$subreddit), ]

#plot
combined_subreddit_theme %>%
  ggplot(aes(x = `theme (cluster number)`)) +
  geom_bar(fill = viridis_1, color = "black") +
  labs(title = "Number of Subreddits per Cluster",
       x = "Cluster Number",
       y = "Number of Subreddits") +
  theme_minimal()
```

The k-means clustering method does not present interesting results.


#### Further tests with the LDA method

Based on the data we scrapped, we test our most promising method (LDA) on small scale of our new, more precise, dataset.

```{r cache=TRUE}
# Define irrelevant words
irrelevant_words <- c("reddit","reddits", "reddit's","www.reddit.com","https","http", "subreddit", "redditors","related", "community", "dedicated", "discussion", "discuss", "discussed","post", "content", "questions", "discussions", "share", "stories", "httpswwwredditcomsubreddit")

# Remove irrelevant words during the cleaning process
data_clean <- combined_subreddit_theme %>%
  unnest_tokens(word, description) %>%    # Tokenize the text into words
  anti_join(stop_words, by = "word") %>%  # Remove stopwords
  filter(!word %in% irrelevant_words) %>% # Exclude specific words
  filter(!str_detect(word, "^[0-9]+$"))   # Remove numbers

# Create a document-term matrix
dtm <- data_clean %>%
  count(subreddit, word) %>%                     # Count words per subreddit
  cast_dtm(subreddit, word, n)                  # Create the DTM
# Fit the LDA model
num_topics <- 5  
lda_model <- LDA(dtm, k = num_topics, control = list(seed = 1234))

# Get the terms associated with each topic
topics <- tidy(lda_model, matrix = "beta")

# Top terms for each topic
top_terms <- topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  arrange(topic, -beta)

print(top_terms)

# Get the topic distribution for each subreddit
subreddit_topics <- tidy(lda_model, matrix = "gamma")

print(subreddit_topics)

#list unique in topic
unique(subreddit_topics$topic)

# Visualize the top terms in each topic
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  scale_fill_viridis_d()
```

We see that the results are promising, we keep analyzing further.We now test with 30 topics. We use this incremental method because of the heavy computational resources needed for this method.

```{r, cache=TRUE}
num_topics <- 20  # Choose the number of topics you want to extract
lda_model <- LDA(dtm, k = num_topics, control = list(seed = 1234))


# Extract term-topic probabilities
topics <- tidy(lda_model, matrix = "beta")

# Top terms for each topic
top_terms <- topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 15) %>%
  ungroup() %>%
  arrange(topic, -beta)

print(top_terms)

# Extract subreddit-topic distributions
subreddit_topics <- tidy(lda_model, matrix = "gamma")

print(subreddit_topics)
```

Visualize:

```{r, warning=FALSE}
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "Top Terms in Each Topic",
       x = "Terms",
       y = "Beta") +
  scale_fill_viridis_d()
```

chosen method because best so far

below is just the graph in text

```{r}
library(dplyr)

# Assuming your data is stored in a dataframe called `top_terms`
top_terms_text <- top_terms %>%
  group_by(topic) %>%                # Group by topic
  slice_max(order_by = beta, n = 15) %>% # Select top 15 terms by beta for each topic
  arrange(topic, desc(beta)) %>%     # Arrange terms by topic and beta
  summarise(top_words = paste(term, collapse = ", ")) %>% # Combine the top 10 terms into a single string
  ungroup()

# View the result
#print(top_terms_text)

```


### extract dominant topic

to see more granularity

```{r}
# Extract subreddit-topic probabilities
subreddit_topics <- tidy(lda_model, matrix = "gamma")

# Find the topic with the highest probability for each subreddit
dominant_topics <- subreddit_topics %>%
  group_by(document) %>%
  slice_max(gamma, n = 1) %>%
  ungroup() %>%
  rename(subreddit = document, dominant_topic = topic)

# View dominant topics
print(dominant_topics)
```

### add name

```{r}
# Add dominant topic to the original dataframe
combined_subreddit_theme <- combined_subreddit_theme %>%
  left_join(dominant_topics, by = c("subreddit" = "subreddit"))

# Define meaningful names for each topic
topic_names <- c(
  "Rules, Information, and Wiki Communities", 
  "Language, Sports, and Club Communities", 
  "Discord, Servers, and Safe Spaces", 
  "Gaming, TV Shows, and Strategy", 
  "University, Work, and Support Groups", 
  "NSFW, Visual Media, and Photography", 
  "City, Policy, and Political Communities", 
  "Food, Environment, and Social Selling", 
  "Sports, Teams, and Player Discussions", 
  "RPG, Fire, and Online Roleplaying", 
  "Advice, Forums, and Technical Help", 
  "Music, Tech, and Enthusiast Groups", 
  "Fans, Comics, and Book Discussions", 
  "Advice, Pokémon, and Basic Tips", 
  "Art, Social Media, and Tactics", 
  "Learning, Projects, and Flair Sharing", 
  "Politics, Science, and Star Wars", 
  "Sharing Tips, Events, and Hardcore Music", 
  "Visual Media, Fans, and Rock Music", 
  "Gaming Systems, PC, and Friends"
)



# Replace the dominant_topic column with topic names

combined_subreddit_theme <- combined_subreddit_theme %>%
  mutate(dominant_topic = topic_names[dominant_topic])

# View updated dataframe
head(combined_subreddit_theme)
```
In the table combined_subreddit_theme, the subreddit are regrouped in a more meaningful way only when beta is closer from 1. But it's far from perfect, in the end the best method is probably to find a theme manually for each subreddit.

merge df with combined_subreddit_theme

```{r, warning=FALSE}
#save data
# write.csv(combined_subreddit_theme, "combined_subreddit_theme.csv", row.names = TRUE)

#read data
combined_subreddit_theme <- read.csv("../../data/combined_subreddit_theme.csv")

# Perform a left join to add the columns to 'df'
df <- df %>%
  left_join(combined_subreddit_theme, by = "subreddit")
```


## Sentiment Analysis

### Sentiment by Subreddit

Analyzing the top *30* subreddits (30 for vizualisation purposes) as a *whole*, focusing on average sentiment scores for each subreddit to get a general idea of the sentiment of the comments in each subreddit.
We will use the AFINN, NRC, and BING lexicons to assign sentiment scores to words in the comments.
We will then calculate the average sentiment score for each subreddit based on the sentiment scores of the words in the comments.

#### AFINN

The AFINN lexicon is a list of English words rated for valence with an integer between -5 (negative) and +5 (positive).

-   Weaknessess : It may not capture the sentiment of slang, sarcasm, or other informal language. Also limited to the words in the lexicon.
-   Strengths : It is simple and easy to use.

```{r sentiment_afinn, eval=TRUE, echo=TRUE}
#get top 50 subreddits
df.top30_subreddit <- df.subreddit %>%
  arrange(desc(nchar(body))) %>%
  filter(row_number() <= 30)

df.top30_subreddit.tokens <- df.top30_subreddit %>% unnest_tokens(word, body)

df.top30_subreddit.affin <- inner_join(df.top30_subreddit.tokens, get_sentiments("afinn"),
 by = c("word" = "word")) %>% group_by(subreddit) %>%
 summarize(Sentiment = mean(value)) %>% ungroup()

ggplot(df.top30_subreddit.affin, aes(x = Sentiment, y = reorder(subreddit, Sentiment), fill = Sentiment)) +
  geom_col() +
  scale_fill_viridis_c() +
  ylab("") + 
  #increase text size
  theme(axis.text.y = element_text(size = 10),
        axis.text.x = element_text(size = 10))
```

There seems to be more positive subreddits than negative ones.

We interestingly observe that based on the affin dictionnary the `news` and `politics` subreddit are the most negative, which could reflect a state of the world.

And `pc` Subreddits like `pcmasterrace` or `builapc` are the most positive.

#### BING

The Bing sentiment lexicon categorizes words as either "positive" or "negative."

-   Weaknesses: It lacks the granularity of numerical scores and may not capture the intensity of sentiment. It also does not account for neutral words.
-   Strengths: Easy to use and understand. It provides a straightforward classification of words.

```{r bing_sentiment, eval=TRUE, echo=TRUE}
df.top30_subreddit.bing <- inner_join(df.top30_subreddit.tokens, get_sentiments("bing"),
 by = c("word" = "word")) %>% group_by(subreddit) %>%
 summarize(Sentiment = mean(ifelse(sentiment == "positive", 1, ifelse(sentiment == "negative", -1, 0)))) %>%
 select(subreddit, Sentiment)
 
ggplot(df.top30_subreddit.bing, aes(x = Sentiment, y = reorder(subreddit, Sentiment), fill = Sentiment)) +
  geom_col() +
  scale_fill_viridis_c() +
  ylab("") + 
  #increase text size
  theme(axis.text.y = element_text(size = 10),
        axis.text.x = element_text(size = 10))
```

BING seems to reflect the same proportion of positive and negative as the AFINN dictionary.
Indeed, we observe the same pattern with `pcmasterrace` at the top and `news` at the bottom, but there is slight variation when compared to the AFINN dictionary.

#### NRC

The NRC Emotion Lexicon (EmoLex) associates words with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (positive and negative).

-   Weaknesses: More complex to implement and interpret. It may require more computational resources.
-   Strengths: Provides a more detailed analysis by categorizing words into multiple emotions. It can capture a wider range of sentiments and emotions.

```{r nrc_sentiment, eval=TRUE, echo=TRUE}
df.top30_subreddit.nrc <- inner_join(df.top30_subreddit.tokens, get_sentiments("nrc"),
 by = c("word" = "word")) %>% group_by(subreddit) %>%
 summarize(Sentiment = mean(ifelse(sentiment == "positive", 1, ifelse(sentiment == "negative", -1, 0)))) %>%
 select(subreddit, Sentiment)

ggplot(df.top30_subreddit.nrc, aes(x = Sentiment, y = reorder(subreddit, Sentiment), fill = Sentiment)) +
  geom_col() +
  scale_fill_viridis_c() +
  ylab("") + 
  #increase text size
  theme(axis.text.y = element_text(size = 10),
        axis.text.x = element_text(size = 10))
```

Through the NRC dictionary we observe far more positive subreddits than negative ones.
This imbalance could be explain by the fact that the NRC dictionnary is more biased towards positive words.

We still observe quite the same patterns though with `pcmasterrace` at the top and `news` near the bottom.

#### Summary

To summarize it is interesting to observe that the NRC classification is totally different from the AFINN and BING classification.
Indeed, the NRC classification seems to be more biased towards positive words but on the other hand it shall be more precise as it has more categories.

Why one to choose ?
It depends on the goal of the analysis.
If we want a simple and easy to use classification we can use AFINN or BING.
If we want a more precise classification we can use NRC.

### Sentiment by comment

Now to have a more precise understanding of the sentiment of the comments, we will analyze the sentiment per comment for each dictionary (AFINN, BING, NRC).
So that we can choose the best dictionary for our analysis.

We will analyze a sample of small comments to have a better understanding.

#### Afinn

```{r sample_afinn_comment, eval=TRUE, echo=TRUE}
#define a random sample of 100 comments
set.seed(1234)
df.sample <- df %>% sample_n(100) %>%
  #select only columns 'id' and 'body'
  select(id, body, subreddit)

# use afinn to analyse sentiment all the rows based on column body
df.sentiment <- inner_join(df.sample %>% unnest_tokens(word, body),
 get_sentiments("afinn"), by = c("word" = "word")) %>% group_by(id) %>%
 summarize(Sentiment = mean(value)) %>% inner_join(df.sample, by = "id")

# Normalize the sentiment scores
df.sentiment.a <- df.sentiment %>%
 mutate(Normalized_Sentiment = (Sentiment - min(Sentiment)) / (max(Sentiment) - min(Sentiment)))

# show the sentiment of the comments
reactable(df.sentiment.a,
          resizable = TRUE,
          defaultPageSize = 2,
          sortable = TRUE,
          searchable = TRUE,
          filterable = TRUE,
          pagination = TRUE,
          highlight = TRUE,
          #add possibility to increase rows per page
          pageSizeOptions = c(5, 10, 15, 20)
          )
#save data as 'AFINN_sentiment_sample.csv'
#write_csv(df.sentiment, "../../data/AFINN_sentiment_sample.csv")
```

#### Bing

```{r sample_bing_comment, eval=TRUE, echo=TRUE}
df.sentiment <- inner_join(df.sample %>% unnest_tokens(word, body),
 get_sentiments("bing"), by = c("word" = "word")) %>% group_by(id) %>%
 summarize(Sentiment = mean(ifelse(sentiment == "positive", 1, ifelse(sentiment == "negative", -1, 0)))) %>%
 inner_join(df.sample, by = "id")

# Normalize the sentiment scores
df.sentiment.b <- df.sentiment %>%
 mutate(Normalized_Sentiment = (Sentiment - min(Sentiment)) / (max(Sentiment) - min(Sentiment)))

#show 
reactable(df.sentiment.b,
          resizable = TRUE,
          defaultPageSize = 2,
          sortable = TRUE,
          searchable = TRUE,
          filterable = TRUE,
          pagination = TRUE,
          highlight = TRUE,
          #add possibility to increase rows per page
          pageSizeOptions = c(5, 10, 15, 20)
          )

#save data as 'AFINN_sentiment_sample.csv'
#write_csv(df.sentiment, "../../data/BING_sentiment_sample.csv")
```

After reading the comments, we observe mixed results.

  -  \[Dude Congrats Im hoping to lose a large amount of weight in the same amount of time Huge inspiration here Keep kicking ass\] was classified as neutral with a score of 0.

  - \[Why did the chicken cross the road It didnt HHAHHAHAHAHHAHAHHA sorry that wasnt funny 472\] was classified as quite negative with a score of -2; it seems, it did not understand the full context

#### NRC

```{r sample_NRC_comment}
#use NRC to analyse sentiment all the rows based on column body
df.sentiment <- inner_join(df.sample %>% unnest_tokens(word, body),
 get_sentiments("nrc"), by = c("word" = "word")) %>% group_by(id) %>%
 summarize(Sentiment = mean(ifelse(sentiment == "positive", 1, ifelse(sentiment == "negative", -1, 0)))) %>%
 inner_join(df.sample, by = "id")

# Normalize the sentiment scores
df.sentiment.n <- df.sentiment %>%
 mutate(Normalized_Sentiment = (Sentiment - min(Sentiment)) / (max(Sentiment) - min(Sentiment)))

#show 
reactable(df.sentiment.n,
          resizable = TRUE,
          defaultPageSize = 2,
          sortable = TRUE,
          searchable = TRUE,
          filterable = TRUE,
          pagination = TRUE,
          highlight = TRUE,
          #add possibility to increase rows per page
          pageSizeOptions = c(5, 10, 15, 20)
          )

#save data as 'AFINN_sentiment_sample.csv'
#write_csv(df.sentiment, "../../data/NRC_sentiment_sample.csv")
```

We observe here that comments like "Night time moon light and a light snow The glow of the moon and the silence is amazing", which evokes a serene and appreciative tone for natural beauty suggest a strong positive emotion.
The three dictionaries seem to have classified this comment as positive with the BING and AFINN dictionary attributing a higher score than the NRC.

#### Vizualising the Sentiment distribution based on a dictionnary

```{r sent_dist, cache=TRUE}
# use afinn to analyse sentiment all the rows based on column body
df.sentiment.a <- inner_join(df %>% unnest_tokens(word, body),
 get_sentiments("afinn"), by = c("word" = "word")) %>% group_by(id) %>%
 summarize(Sentiment = mean(value)) %>% inner_join(df.sample, by = "id")

# use bing to analyse sentiment all the rows based on column body
df.sentiment.b <- inner_join(df %>% unnest_tokens(word, body),
 get_sentiments("bing"), by = c("word" = "word")) %>% group_by(id) %>%
 summarize(Sentiment = mean(ifelse(sentiment == "positive", 1, ifelse(sentiment == "negative", -1, 0)))) %>%
 inner_join(df, by = "id")

#use NRC to analyse sentiment all the rows based on column body
df.sentiment.n <- inner_join(df %>% unnest_tokens(word, body),
 get_sentiments("nrc"), by = c("word" = "word")) %>% group_by(id) %>%
 summarize(Sentiment = mean(ifelse(sentiment == "positive", 1, ifelse(sentiment == "negative", -1, 0)))) %>%
 inner_join(df, by = "id")


#plot the three sentiment distribution on one using viridis color
ggplot() +
  geom_density(data = df.sentiment.a, aes(x = Sentiment, fill = "AFINN"), alpha = 0.5) +
  geom_density(data = df.sentiment.b, aes(x = Sentiment, fill = "BING"), alpha = 0.5) +
  geom_density(data = df.sentiment.n, aes(x = Sentiment, fill = "NRC"), alpha = 0.5) +
  scale_fill_viridis_d() +
  theme_minimal()
```

-   The AFINN method shows a relatively flat distribution with a slight peak around 0.
    This suggests that the sentiment scores are spread out, with a slight tendency towards neutrality.

-   The Bing method displays a bimodal distribution with two distinct peaks, one around -1 and another around 1.
    This indicates that the sentiment scores are polarized, with many comments being classified as either quite negative or quite positive.
    Which is logical based on the previous analysis

-   The NRC method has a sharp peak at 0, suggesting that most of the sentiment scores are neutral.
    This implies that the NRC method tends to classify a large number of comments as neutral, with fewer comments being classified as strongly positive or negative.
    But we note that the NRC is also quite condensed

#### Normalization to correctly compare

```{r norm_sent_dist, eval=TRUE, echo=TRUE}
# Normalize the sentiment scores
df.sentiment.a <- df.sentiment.a %>%
  mutate(Normalized_Sentiment = (Sentiment - min(Sentiment)) / (max(Sentiment) - min(Sentiment)))

df.sentiment.b <- df.sentiment.b %>%
  mutate(Normalized_Sentiment = (Sentiment - min(Sentiment)) / (max(Sentiment) - min(Sentiment)))

df.sentiment.n <- df.sentiment.n %>%
  mutate(Normalized_Sentiment = (Sentiment - min(Sentiment)) / (max(Sentiment) - min(Sentiment)))

# Create the ggplot object
p <- ggplot() +
  geom_density(data = df.sentiment.a, aes(x = Normalized_Sentiment, fill = "AFINN"), alpha = 0.3) +
  geom_density(data = df.sentiment.b, aes(x = Normalized_Sentiment, fill = "BING"), alpha = 0.5) +
  geom_density(data = df.sentiment.n, aes(x = Normalized_Sentiment, fill = "NRC"), alpha = 0.5) +
  scale_fill_viridis_d() +
  theme_minimal()

# Convert the ggplot object to an interactive plotly object
interactive_plot <- ggplotly(p)

# Adjust the height and width 
interactive_plot <- layout(interactive_plot, width = 600, height = 400)
# Display the interactive plot
interactive_plot
```

#### Summary

We might choose one method over the others.
For example, if we need a method that captures strong sentiments, Bing might be more suitable.
For a more balanced view, AFINN could be the way to go.
With more neutral comments, NRC might be the best choice.


### Apply The findings on the whole dataset

-   investigate why it reduces the number of comment

```{r}
df.sentiment <- inner_join(df %>% unnest_tokens(word, body),
 get_sentiments("bing"), by = c("word" = "word")) %>% group_by(id) %>%
 summarize(Sentiment = mean(ifelse(sentiment == "positive", 1, ifelse(sentiment == "negative", -1, 0)))) %>%
 inner_join(df, by = "id")

# Normalize the sentiment scores
df.sentiment <- df.sentiment %>%
 mutate(Normalized_Sentiment = (Sentiment - min(Sentiment)) / (max(Sentiment) - min(Sentiment)))


#new column for sentiment category
df.sentiment$sentiment_category <- ifelse(df.sentiment$Sentiment >= 1, "positive",
 ifelse(df.sentiment$Sentiment <= -1, "negative", "neutral"))

df <- df.sentiment 

# save the data as a .csv file
#write.csv(df.sentiment, "../../data/BING_sentiment.csv", row.names = TRUE)
```

## Score

```{r}
df_hugo <- df %>%
  select(-c("X","author_flair_text","id"))

score_2 <- sum(df_hugo$score==2)
score_1 <- sum(df_hugo$score==1)
score_0 <- sum(df_hugo$score==0)

print(paste0("Total number of comments without Up or Downvotes is ", score_1))
print(paste0("Total number of comments with only one Up or Downvote is ", score_0 + score_2))
print(paste0(round((100*(score_0 + score_1 + score_2)/138070),digits = 2), "% of the comments have 0 or 1 upvote/downvote"))
```

```{r}
# Filtering scores to focus on the range from -10 to 100
range_filtered <- df_hugo %>%
  filter(score >= -10 & score <= 20)

# Plotting the distribution of the 'score' column from -10 to 100 using ggplot2
ggplot(range_filtered, aes(x = score)) +
  geom_histogram(bins = 30, fill = viridis_3, color = "black", alpha = 0.7) +
  labs(title = "Distribution of Score Column from -10 to 20",
       x = "Score",
       y = "Frequency") +
  theme_minimal() +
  theme(panel.grid.major = element_line(color = "grey", linetype = "dashed", size = 0.5))
```

```{r}
print(summary(df_hugo$score))
```

```{r}
df_high_score <- df_hugo %>%
  filter(score %in% (10:3967))

ggplot(df_high_score, aes(x = score)) +
  geom_histogram(bins = 30, fill = viridis_4, color = "black", alpha = 0.7) +
  scale_y_log10() +
  scale_x_log10()+
  labs(title = "Log-Distribution of High Scoring (10+ upvotes) Comments",
       x = "Score",
       y = "Log(Frequency)") +
  theme_minimal() +
  scale_y_continuous(labels = scales::label_comma())
```

```{r}
df_low_score <- df_hugo %>%
  filter(score %in% (-302:-10))

# Plotting the distribution of low scoring comments using ggplot2 with log transformation on both axes
ggplot(df_low_score, aes(x = score)) +
  geom_histogram(bins = 30, fill = viridis_5, color = "black", alpha = 0.7) +
  scale_y_log10() +
  scale_x_reverse() +
  labs(title = "Log-Distribution of Low Scoring (-10 and below) Comments",
       x = "Score",
       y = "Log(Frequency)") +
  theme_minimal() +
  scale_y_continuous(labels = scales::label_comma())

```

#### Score Categories
```{r}
df <- df %>%
  mutate(score_category = case_when(
    score >= 100 ~ "extremely_positive",
    score >= 20 & score < 100 ~ "very_positive",
    score >= 10 & score < 20 ~ "positive",
    score >= 2 & score < 10 ~ "somewhat_positive",
    score == 1 ~ "untouched",
    score > -10 & score <= 0 ~ "somewhat_negative",
    score >= -20 & score <= -10 ~ "negative",
    score < -20 ~ "very_negative"
  ))
```



```{r}
# Counting unique subreddits in the dataset
unique_subreddits <- n_distinct(df_high_score$subreddit)
print(paste0("Total number of unique subreddits in the High-score dataset: ", unique_subreddits))
```

Only keep the subreddits with 5 or more comments

```{r}
# Analyzing subreddit-wise tendency of high scores
df_subreddit_high_score <- df_high_score %>%
  group_by(subreddit) %>%
  summarise(avg_score = mean(score), median_score = median(score), comment_count = n()) %>%
  filter(comment_count >= 5) %>%
  arrange(desc(avg_score))

# Printing subreddit analysis
df_subreddit_high_score
```

```{r}
# Plotting histogram of the number of comments per subreddit for high scoring comments without subreddit names
count_distribution <- df_high_score %>%
  group_by(subreddit) %>%
  summarise(comment_count = n()) %>%
  filter(comment_count >= 5) %>%
  arrange(desc(comment_count))

# Plotting the distribution of number of comments per subreddit
ggplot(count_distribution, aes(x = comment_count)) +
  geom_histogram(bins = 30, fill = viridis_3, alpha = 0.8) +
  labs(title = "Distribution of Number of Comments per Subreddit for High Scoring Comments",
       x = "Number of Comments",
       y = "Frequency") +
  theme_minimal()
```

```{r}
# Plotting average score by subreddit for high scoring comments with 5 or more comments
ggplot(df_subreddit_high_score, aes(x = reorder(subreddit, avg_score), y = avg_score)) +
  geom_bar(stat = "identity", fill = viridis_4, alpha = 0.8) +
  coord_flip() +
  labs(title = "Average Score by Subreddit for High Scoring Comments (5 or more comments)",
       x = "Subreddit",
       y = "Average Score") +
  theme_minimal()
```

```{r}
# Analyzing subreddit-wise tendency of low scores
df_subreddit_low_score <- df_low_score %>%
  group_by(subreddit) %>%
  summarise(avg_score = mean(score), median_score = median(score), comment_count = n()) %>%
  filter(comment_count >= 2) %>%
  arrange(avg_score)

# Printing subreddit analysis for low scores
df_subreddit_low_score
```

```{r}
ggplot(df_subreddit_low_score, aes(x = avg_score, y = reorder(subreddit, avg_score))) +
  geom_bar(stat = "identity", fill = viridis_5, alpha = 0.8) +
  scale_x_reverse(limits = c(0, min(df_subreddit_low_score$avg_score))) +
  labs(title = "Average Score by Subreddit for Low Scoring Comments (5 or more comments)",
       x = "Subreddit",
       y = "Average Score") +
  theme_minimal()
```

```{r}
# Plotting histogram of the number of comments per subreddit for low scoring comments without subreddit names
count_distribution_low <- df_low_score %>%
  group_by(subreddit) %>%
  summarise(comment_count = n()) %>%
  filter(comment_count >= 2) %>%
  arrange(desc(comment_count))

# Plotting the distribution of number of comments per subreddit for low scores
ggplot(count_distribution_low, aes(x = comment_count)) +
  geom_histogram(bins = 20, fill = viridis_2, alpha = 0.8) +
  labs(title = "Distribution of Number of Comments per Subreddit for Low Scoring Comments",
       x = "Number of Comments",
       y = "Frequency") +
  theme_minimal()

```

```{r}
# Clustering analysis based on number of comments and average score for high scoring subreddits
df_subreddit_cluster <- df_high_score %>%
  group_by(subreddit) %>%
  summarise(avg_score = mean(score), comment_count = n())

# Scaling the data for clustering
cluster_data <- df_subreddit_cluster %>%
  select(comment_count, avg_score) %>%
  scale()

# Determining the optimal number of clusters using the elbow method
set.seed(123)
fviz_nbclust(cluster_data, kmeans, method = "wss") +
  labs(title = "Elbow Method for Determining Optimal Number of Clusters",
       x = "Number of Clusters",
       y = "Within-Cluster Sum of Squares") +
  theme_minimal()
```

We'll settle with 5 clusters

```{r}
# Running k-means clustering with 5 clusters
set.seed(123) # For reproducibility
kmeans_result <- kmeans(cluster_data, centers = 5)
df_subreddit_cluster$cluster <- as.factor(kmeans_result$cluster)

# Plotting the clusters
ggplot(df_subreddit_cluster, aes(x = comment_count, y = avg_score, color = cluster)) +
  geom_point(size = 3, alpha = 0.7) +
  scale_color_viridis_d() +
  labs(title = "Clustering of Subreddits Based on Number of Comments and Average Score",
       x = "Number of Comments",
       y = "Average Score") +
  theme_minimal() 
```

## Additional Variable

We now refine our dataset to explore some more relationships, as well as plotting a first knowledge graph.

### scrap related subreddits

```{r, eval=FALSE}
# Function to scrape related subreddits
get_related_subreddits <- function(subreddit) {
  # Construct the URL
  url <- paste0("https://www.reddit.com/r/", subreddit, "/")
  
  tryCatch({
    # Make the GET request with a User-Agent
    response <- GET(url, add_headers("User-Agent" = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"))
    
    # Check HTTP status code
    if (status_code(response) != 200) {
      cat(paste("Failed to fetch:", subreddit, "Status Code:", status_code(response), "\n"))
      return(data.frame(
        subreddit = subreddit,
        related_subreddits = NA,
        stringsAsFactors = FALSE
      ))
    }
     # Parse the HTML content
    page <- read_html(content(response, "text"))
    
    # Extract related subreddit elements
    related_subreddits <- page %>%
      html_nodes("li .overflow-ellipsis") %>%  # Target divs with the specific class inside list items
      html_text(trim = TRUE)                  # Extract the visible text
    
    # Check if any related subreddits were found
    if (length(related_subreddits) == 0) {
      cat(paste("No related subreddits found for:", subreddit, "\n"))
      related_subreddits <- NA
    }
    
    # Return the results
    return(data.frame(
      subreddit = subreddit,
      related_subreddits = paste(related_subreddits, collapse = ", "), # Join as a single string
      stringsAsFactors = FALSE
    ))
  }, error = function(e) {
    cat(paste("Error for subreddit:", subreddit, "with error:", e$message, "\n"))
    return(data.frame(
      subreddit = subreddit,
      related_subreddits = NA,
      stringsAsFactors = FALSE
    ))
  })
}

# Example usage with a few subreddit names
subreddit_names <- combined_subreddit_theme$subreddit[3501:nrow(combined_subreddit_theme)]
# Loop through subreddits and fetch related subreddits
results <- do.call(rbind, lapply(subreddit_names, function(subreddit) {
  cat(paste("Processing:", subreddit, "\n"))
  Sys.sleep(runif(1, min = 2, max = 3.5))  # Randomized delay
  get_related_subreddits(subreddit)
}))

# View the results
print(results)

```   
```{r}
#write_csv(results, "subreddit_relatedsub_part3.csv")

```
    
```{r, warning=FALSE, message=FALSE}
subreddit_related_part1 <- read_csv("../../data/subreddit_relatedsub_part1.csv")
#remove r/ in column related subreddit
subreddit_related_part1$related_subreddits <- gsub("r/", "", subreddit_related_part1$related_subreddits)

subreddit_related_part2 <- read_csv("../../data/subreddit_relatedsub_part2.csv")
#remove r/ in column related subreddit
subreddit_related_part2$related_subreddits <- gsub("r/", "", subreddit_related_part2$related_subreddits)

subreddit_related_part3 <- read_csv("../../data/subreddit_relatedsub_part3.csv")
#remove r/ in column related subreddit
subreddit_related_part3$related_subreddits <- gsub("r/", "", subreddit_related_part3$related_subreddits)

combined_subreddit_related <- rbind(subreddit_related_part1, subreddit_related_part2,subreddit_related_part3)

#remove Megathread and Announcement from related subreddits
combined_subreddit_related$related_subreddits <- gsub("Megathread", "", combined_subreddit_related$related_subreddits)
combined_subreddit_related$related_subreddits <- gsub("Announcement", "", combined_subreddit_related$related_subreddits)

#remove ", ", ", , ", ", , , " in the beginning of the string
combined_subreddit_related$related_subreddits <- gsub("^,\\s*", "", combined_subreddit_related$related_subreddits)
combined_subreddit_related$related_subreddits <- gsub("^,\\s*", "", combined_subreddit_related$related_subreddits)
combined_subreddit_related$related_subreddits <- gsub("^,\\s*", "", combined_subreddit_related$related_subreddits)
combined_subreddit_related$related_subreddits <- gsub("^,\\s*", "", combined_subreddit_related$related_subreddits)
combined_subreddit_related$related_subreddits <- gsub("^,\\s*", "", combined_subreddit_related$related_subreddits)

#yes the code could be more efficient than copy paste 5 times the line above 
```

add column to match related subreddit with the subreddit in our database

```{r}
# Split related subreddits into individual components
combined_subreddit_related$matching_related_subreddits <- sapply(
  strsplit(combined_subreddit_related$related_subreddits, ",\\s*"), 
  function(related) {
    # Return only related subreddits that are in the 'subreddit' column
    related[related %in% combined_subreddit_related$subreddit]
  }
)

# Convert the list back into a string for easy viewing
combined_subreddit_related$matching_related_subreddits <- sapply(
  combined_subreddit_related$matching_related_subreddits, 
  paste, collapse = ", "
)

#we remove the NA values
combined_subreddit_related <- na.omit(combined_subreddit_related)

# remove empty related_subreddit in combined_subreddit_related
combined_subreddit_related <- combined_subreddit_related %>%
  filter(related_subreddits != "")
```

merge df with combined_subreddit_theme and combined_subreddit_related

```{r, warning=FALSE}

# Perform a left join to add the columns to 'df'
df <- df %>%
  left_join(combined_subreddit_related, by = "subreddit")

head(df)
```

plot graph of matching related subreddits

```{r}
# Filter rows where `matching_related_subreddits` is not empty
filtered_data <- combined_subreddit_related[
  combined_subreddit_related$matching_related_subreddits != "", 
]

# Take only the first 100 rows
filtered_data <- head(filtered_data, 300)

# Prepare the edges for the graph
edges <- do.call(rbind, lapply(1:nrow(filtered_data), function(i) {
  source <- filtered_data$subreddit[i]
  targets <- unlist(strsplit(filtered_data$matching_related_subreddits[i], ",\\s*"))
  if (length(targets) > 0) {
    data.frame(from = source, to = targets, stringsAsFactors = FALSE)
  }
}))

# Ensure no duplicates in edges
edges <- unique(edges)

# Create nodes
nodes <- data.frame(
  id = unique(c(edges$from, edges$to)), # Unique nodes from edges
  label = unique(c(edges$from, edges$to)), # Labels are the same as IDs
  stringsAsFactors = FALSE
)

# Create the interactive graph
visNetwork(nodes, edges) %>%
  visNodes(size = 10) %>%
  visEdges(arrows = "to") %>%
  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %>%
  visInteraction(navigationButtons = TRUE) %>%
  visLayout(randomSeed = 42) # Consistent layout

```

other graph with everything

```{r}

# Filter rows where `matching_related_subreddits` is not empty
filtered_data <- combined_subreddit_related[
  combined_subreddit_related$matching_related_subreddits != "", 
]

# Take the first 1000 rows
filtered_data <- head(filtered_data, 1000)

# Prepare the edges for the graph
edges <- do.call(rbind, lapply(1:nrow(filtered_data), function(i) {
  source <- filtered_data$subreddit[i]
  targets <- unlist(strsplit(filtered_data$matching_related_subreddits[i], ",\\s*"))
  if (length(targets) > 0) {
    data.frame(from = source, to = targets, stringsAsFactors = FALSE)
  }
}))

# Ensure no duplicates in edges
edges <- unique(edges)

# Create the igraph object
graph <- graph_from_data_frame(
  d = edges,  # Edge list
  directed = TRUE # Directional relationships
)

# Use Kamada-Kawai layout for better node spacing
set.seed(42) # For consistent layout
layout <- layout_with_kk(graph)

# Scale layout to further spread nodes
layout <- layout * 2
# Plot the graph with spacing
plot(
  graph,
  vertex.size = 0.2,                      # Node size
  vertex.label = NA,                    # Disable labels
  # vertex.label.cex = 0.1,               # Reduced label text size
  # vertex.label.color = "black",         # Label color
  # vertex.label.dist = 1,                # Distance of label from node
  edge.arrow.size = 0.05,                # Smaller arrow size
  edge.color = "gray50",                # Edge color
  layout = layout,                      # Use the scaled layout
  main = "Subreddit Relationship Graph" # Add a title
)


```

top 20 most related

```{r}
# Filter rows where `matching_related_subreddits` is not empty
filtered_data <- combined_subreddit_related[
  combined_subreddit_related$matching_related_subreddits != "", 
]

# Take only the first 1000 rows (for performance reasons)
filtered_data <- head(filtered_data, 1000)

# Prepare the edges for the graph
edges <- do.call(rbind, lapply(1:nrow(filtered_data), function(i) {
  source <- filtered_data$subreddit[i]
  targets <- unlist(strsplit(filtered_data$matching_related_subreddits[i], ",\\s*"))
  if (length(targets) > 0) {
    data.frame(from = source, to = targets, stringsAsFactors = FALSE)
  }
}))

# Ensure no duplicates in edges
edges <- unique(edges)

# Count the number of links to each subreddit
link_counts <- table(edges$to)
link_counts <- sort(link_counts, decreasing = TRUE) # Sort by most links

# Extract the top 20 subreddits with the most links
top_20_subreddits <- head(link_counts, 20)

# Convert to a data frame for better readability
top_20_df <- as.data.frame(top_20_subreddits)
colnames(top_20_df) <- c("Subreddit", "Number_of_Links")

# Display the top 20 subreddits
print(top_20_df)

# Optionally, create a barplot
barplot(
  top_20_df$Number_of_Links, 
  names.arg = top_20_df$Subreddit, 
  las = 2, # Rotate labels for better readability
  col = viridis_4, 
  main = "Top 20 Subreddits with Most Links to Them",
  ylab = "Number of Links",
  cex.names = 0.8 # Adjust label size
)

```

### Adding Length of comment as a feature

```{r, eval=TRUE, echo=TRUE}
#show length of one random comment
df$body[1] %>% nchar() #char shows the number of characters in a string
#to show the number of words in a comment we can use str_count
df$body[1] %>% str_count("\\w+")
df$body[1]
#add length of comment as a feature
df$length <- str_count(df$body, "\\w+")
head(df)

#get summary statistics on column length
summary(df$length)
```

## Final Dataset

```{r}
head(df)
#rename column 'theme (cluster number)' to 'theme'
df <- df %>%
  select(id, author, subreddit, display_name, body, score, sentiment_category, length, description, theme, score_category)

# Adding score categories
df <- df %>%
  mutate(score_category = case_when(
    score >= 100 ~ "extremely_positive",
    score >= 20 & score < 100 ~ "very_positive",
    score >= 10 & score < 20 ~ "positive",
    score >= 2 & score < 10 ~ "somewhat_positive",
    score == 1 ~ "untouched",
    score > -10 & score <= 0 ~ "somewhat_negative",
    score >= -20 & score <= -10 ~ "negative",
    score < -20 ~ "very_negative"
  ))
nrow(df_final)

#count NA in related subreddits
sum(is.na(df_final$matching_related_subreddits))

#write.csv(df_final, "../../data/reddit_comments_15k_v2.csv", row.names = TRUE)
#load the data
df_final <- read_csv("../../data/reddit_comments_15k_v2.csv")
head(df_final)
```
