# EDA

## Text Mining
### A subreddit = A document

Here i take the idea that a 'document' is a subreddit. It is like all comments that are part of one subreddit will be a document. It is a bit an analysis per subreddit

```{r fig.width=10, fig.height=10}
#take the df data and group all comments by subreddit
df.subreddit <- df %>%
  group_by(subreddit) %>%
  summarise(body = paste(body, collapse = " "))

# show number of subreddits
cat("Number of subreddits:", nrow(df.subreddit), "\n")

# show top 10 subreddits by length in a bar plot
df.subreddit %>%
  mutate(subreddit = reorder(subreddit, nchar(body))) %>%
  top_n(10, nchar(body)) %>%
  ggplot(aes(x = subreddit, y = nchar(body))) +
  geom_col() +
  coord_flip() +
  labs(title = "Top 10 Subreddits by Length", x = "Subreddit", y = "Length")

# preprocess the text data
df.subreddit.cp <- corpus(df.subreddit$body)
docnames(df.subreddit.cp) <- df.subreddit$subreddit  # Assign subreddit names as document identifiers

df.subreddit.tk <- tokens(
  df.subreddit.cp,
  remove_numbers = TRUE,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_url = TRUE,
  remove_hyphens = TRUE,
  remove_separators = TRUE
)
df.subreddit.tk <- df.subreddit.tk %>%
  tokens_tolower() %>%
  tokens_remove(stop_words$word) %>%
  tokens_remove(c("reddit", "subreddit", "amp", "gt", "deleted", "x+"))

# tf and tf-idf analysis
#tf
df.subreddit.dfm <- dfm(df.subreddit.tk)
tf <- rowSums(t(df.subreddit.dfm))
tf <- data.frame(term = names(tf), count = tf) %>%
  tibble() %>%
  arrange(desc(count))

#freq per documents
tidy(df.subreddit.dfm) %>%
  arrange(desc(count))

#freq per term
df.subreddit.freq <- textstat_frequency(df.subreddit.dfm)
head(df.subreddit.freq,20)

#plot 20 most frequent words
df.subreddit.freq %>%
  top_n(20, frequency) %>%
  ggplot(aes(
    x = reorder(feature, frequency),
    y = frequency)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  xlab("Frequency") +
  ylab("term")

textplot_wordcloud(df.subreddit.dfm)

df.subreddit.freq %>%
  top_n(20, frequency) %>%
  ggplot(aes(label = feature, size = frequency)) +
  geom_text_wordcloud() +
  scale_size_area(max_size = 20) +
  theme_minimal()

#tf-idf
df.subreddit.tfidf <- dfm_tfidf(df.subreddit.dfm)
df.subreddit.tfidf.tidy <- tidy(df.subreddit.tfidf) %>%
  bind_tf_idf(term = term, document = document, n = count) %>%
  arrange(desc(tf_idf))
head(df.subreddit.tfidf.tidy, n = 20) %>% flextable() %>% autofit()

#plot per document
df.subreddit.dfm %>%
  tidy() %>%
  top_n(30, count) %>%
  ggplot(aes(x = term, y = count)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  theme(axis.text.y = element_text(size = 6),
        axis.ticks.y = element_blank()) +
  facet_wrap(~document, ncol = 5)
```

-   We observe that AskReddit has lots of words frequency which makes sense as it is the biggest subreddit by lenght.
-   We also observe that the term 'game' is associated in term of frequency with sports subreddits like CFB (College Football) and Hockey which makes sense.
-   Interestingly the subreddits LoL (League of Legends) is associated with the term 'happy', which show us that the community may be the happiest, careful bias here, it is just a word frequency analysis.
- In the subreddits 'pics' we observe a lot of 'gem' term which indicates treasure or something valuable. This is interesting as it is a subreddit for sharing pictures.
- The subreddits 'news' is associated in terms of frequency with the term 'people' which is logical as news is about people.

#### Similarity between Subreddits/Documents

we analyze similarities and dissimilarities between the documents (through words) in the data. We use quanteda extensively. Weâ€™ll use the objects created previously in the exercises.

Then use the functions textstat_simil() and textstat_dist() to compute the Jaccard index matrix, the cosine matrix, and the Euclidean distances matrix.

```{r}
#We restrict ourselves to a small subset of the highest tf-idf of the data to avoid memory issues and readability issues in the plots.
df.subreddit.tfidf.small <- df.subreddit.tfidf[1:50,]
df.subreddit.tfidf.small

df.subreddit.jac <- textstat_simil(
  df.subreddit.tfidf.small, method = "jaccard", margin = "documents")

df.subreddit.cos <- textstat_simil(
  df.subreddit.tfidf.small, method = "cosine", margin = "documents")

df.subreddit.eucl <- textstat_dist(
  df.subreddit.tfidf.small, method = "euclidean", margin = "documents")

#heatmap representation of similariteis between subreddits
## jaccard
df.subreddit.jac.mat <- melt(as.matrix(df.subreddit.jac))
ggplot( data = df.subreddit.jac.mat, 
        mapping = aes(x = Var1, y = Var2, fill = value)) +
  scale_fill_gradient2(
    low = "blue", 
    high = "red", 
    mid = 'white', 
    midpoint = 0.5, 
    limit = c(0,1), 
    name = "Jaccard") +
  geom_tile() + xlab("") + ylab("") +
  #incline x-axis labels
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

## cosine
df.subreddit.cos.mat <- melt(as.matrix(df.subreddit.cos))
ggplot( data = df.subreddit.cos.mat, 
        mapping = aes(x = Var1, y = Var2, fill = value)) +
  scale_fill_gradient2(
    low = "blue", 
    high = "red", 
    mid = 'white', 
    midpoint = 0.5, 
    limit = c(0,1), 
    name = "Cosine") +
  geom_tile() + xlab("") + ylab("") +
  #incline x-axis labels
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

## euclidean
df.subreddit.eucl.mat <- melt(as.matrix(df.subreddit.eucl))
ggplot( data = df.subreddit.eucl.mat, 
        mapping = aes(x = Var1, y = Var2, fill = value)) +
  scale_fill_gradient2(
    low = "blue", 
    high = "red", 
    mid = 'white', 
    midpoint = 0.5, 
    limit = c(0,1), 
    name = "Euclidean") +
  geom_tile() + xlab("") + ylab("") +
  #incline x-axis labels
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

#### Clustering of Documents

Hierarchical clustering and K-means. The first one is applied on the dissimilarities (Euclidean, inverted Jaccard, and inverted cosine). The second one is applied on the features, here, TF-IDF. To illustrate the methods, we decide to create five clusters.

```{r}
#hierarchical
df.subreddit.hc <- hclust(as.dist(df.subreddit.eucl))
## df.subreddit.hc <- hclust(as.dist(1 - df.subreddit.jac)) # use this line for Jaccard
## df.subreddit.hc <- hclust(as.dist(1 - df.subreddit.cos)) # use this line for Cosine
plot(df.subreddit.hc)
df.subreddit.clust <- cutree(df.subreddit.hc, k = 3)
df.subreddit.clust
#k-means

df.subreddit.km <- kmeans(df.subreddit.tfidf.small, centers = 3)
df.subreddit.km$cluster
```

#### Similarities between words

In this part we analyze similarities between words (through documents). We restrict ourselves to a subset corresponding to words with frequency rank less than 40 (it should correspond to the 40 most frequent words but several words have the same frequency rank). We use the cosine similarity and plot the heatmap.

```{r}
df.subreddit.freq <- textstat_frequency(df.subreddit.dfm) %>%
  filter(rank <= 40)
df.subreddit.freq$feature

df.subreddit.cos <- textstat_simil(
  df.subreddit.dfm[, df.subreddit.freq$feature],
  method = "cosine",
  margin = "feature")
df.subreddit.cos.mat <- melt(as.matrix(df.subreddit.cos)) # Convert the object to matrix then to data frame

ggplot(data = df.subreddit.cos.mat, aes(x=Var1, y=Var2, fill=value)) +
  scale_fill_gradient2(
    low = "blue",
    high = "red",
    mid = "white",
    midpoint = 0.5,
    limit = c(0, 1),
    name = "Cosine") +
  geom_tile() + 
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 5),
    axis.text.y = element_text(size = 5)) +
  xlab("") + 
  ylab("")
```

Lots of words are similar in terms of cosine similarity. this is peculiar which means that they are used in a similar proportion through subreddits.

#### Clustering Words

```{r}
#cluster and rotate labels
df.subreddit.hc <- hclust(as.dist(1 - df.subreddit.cos))
plot(df.subreddit.hc)
```


### Attempt 4 - Topic Modelling per subrreddit
#### LSA

```{r}
#select subset of data of the top 10 subreddits for readability purposes
df.top10_subreddit <- df.subreddit %>%
  arrange(desc(nchar(body))) %>%
  filter(row_number() <= 10)

#corpus
df.top10_subreddit.cp <- corpus(df.top10_subreddit$body)
# Assign subreddit names as document identifiers
docnames(df.top10_subreddit.cp) <- df.top10_subreddit$subreddit

#token
df.top10_subreddit.tk <- tokens(
  corpus(df.top10_subreddit$body),
  remove_numbers = TRUE,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_url = TRUE,
  remove_hyphens = TRUE,
  remove_separators = TRUE
)
df.top10_subreddit.tk <- df.top10_subreddit.tk %>%
  tokens_tolower() %>%
  tokens_remove(stop_words$word) %>%
  tokens_remove(c("reddit", "subreddit", "amp", "gt", "deleted", "x+")) %>%
  tokens_replace(
    pattern = hash_lemmas$token,
    replacement = hash_lemmas$lemma)

# Create Document-Feature Matrix (DFM)
df.top10_subreddit.dfm <- dfm(df.top10_subreddit.tk)

# Explicitly set document names in the DFM to the subreddit names
docnames(df.top10_subreddit.dfm) <- df.top10_subreddit$subreddit
df.top10_subreddit.dfm

#use textmodel_lsa() on dfm using n dimensions
df.subreddit.lsa <- textmodel_lsa(
  x = df.top10_subreddit.dfm,
  nd = 5)

head(df.subreddit.lsa$docs)
#interpretation
## we look at the five terms with the largest values and the five ones with the lowest value (i.e., largest negative value)

n.terms <- 5
##for dim 2
w.order <- sort(df.subreddit.lsa$features[, 2], decreasing = TRUE)
w.top2 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))
##for dim 3
w.order <- sort(df.subreddit.lsa$features[, 3], decreasing = TRUE)
w.top3 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))

w.top2
w.top3
```

Topic 2 is associated positively with topic like  (game, team, play, games, fuck, ) and negatively with topics like (questions, question, comments, post, people)

```{r}
#PCA bibplots for links between topics and documents and topcis with terms
w.subset <-
  ##restric chart to terms that are mostly related to dim2 and dim3 so w.top2 and w.top3
  df.subreddit.lsa$features[
    c(unique(c(names(w.top2), names(w.top3)))), 2:3]

biplot(
  y = df.subreddit.lsa$docs[, 2:3],
  x = df.subreddit.lsa$features[,2:3],
  col = c("black","red"),
  cex = c(0.5, 0.5),
  xlab = "Dim 2",
  ylab = "Dim 3")
```
We observe that the word `fuck` is associated with dim 3 and is interestingly negatively associated to `happy` which makes sense.

#### LDA

```{r}
docnames(df.top10_subreddit.dfm) <- df.top10_subreddit$subreddit
set.seed(1234) #To create reproducible results
df.top10_subbredit.lda <- textmodel_lda(
  x = df.top10_subreddit.dfm,
  k = 10)

#extract top 5 terms per topic
seededlda::terms(df.top10_subbredit.lda, 5)

# Extract subreddits per topic and rename output to match subreddit names
top_subreddits <- seededlda::topics(df.top10_subbredit.lda)
names(top_subreddits) <- df.top10_subreddit$subreddit
top_subreddits

#count the number of documents per topic
seededlda::topics(df.top10_subbredit.lda) %>% table()
```

Topic 8 with 3 subreddits ( AskReddit, funny and news) is the most popular topic with words like people, time, fuck, day, feel which may represent human experience and emotions. They touch on aspects of daily life, feelings, and the passage of time, with a bit of raw expression thrown in. It's a mix of the mundane and the profound, capturing the essence of what it means to be human. Indeed, what reddit is about haha

#### Topic-term Analysis

```{r}
#transform into a long df
phi.long <- melt(
  df.top10_subbredit.lda$phi,
  varnames = c("Topic", "Term"),
  value.name = "Phi") 

#plot 10 largest prob terms within each subject
phi.long %>% 
  group_by(Topic) %>% 
  top_n(10, Phi) %>% 
  ggplot(aes(reorder_within(Term, Phi, Topic), Phi)) + 
  geom_col(show.legend = FALSE) +
  coord_flip() +
  facet_wrap(~ Topic, scales = "free_y") +
  scale_x_reordered() + 
  xlab("Term") + 
  theme(
    axis.text.y = element_text(size = 5),
    strip.text = element_text(size = 5))
```
Similar analysis as the to the previous but now we see the probs so more precise

But we observe that lots of topics have the same terms. Therefore, adjusting k topics may be necessary.

We try with 7 topics

```{r}
docnames(df.top10_subreddit.dfm) <- df.top10_subreddit$subreddit
set.seed(1234) #To create reproducible results
df.top10_subbredit.lda <- textmodel_lda(
  x = df.top10_subreddit.dfm,
  k = 7)

#extract top 5 terms per topic
seededlda::terms(df.top10_subbredit.lda, 5)

# Extract subreddits per topic and rename output to match subreddit names
top_subreddits <- seededlda::topics(df.top10_subbredit.lda)
names(top_subreddits) <- df.top10_subreddit$subreddit
top_subreddits

#count the number of documents per topic
seededlda::topics(df.top10_subbredit.lda) %>% table()
#transform into a long df
phi.long <- melt(
  df.top10_subbredit.lda$phi,
  varnames = c("Topic", "Term"),
  value.name = "Phi") 

#plot 10 largest prob terms within each subject
phi.long %>% 
  group_by(Topic) %>% 
  top_n(10, Phi) %>% 
  ggplot(aes(reorder_within(Term, Phi, Topic), Phi)) + 
  geom_col(show.legend = FALSE) +
  coord_flip() +
  facet_wrap(~ Topic, scales = "free_y") +
  scale_x_reordered() + 
  xlab("Term") + 
  theme(
    axis.text.y = element_text(size = 5),
    strip.text = element_text(size = 5))
```

the distinction between topics is more clear now. We observe that topic1 may be related to 'hockey' and topic 2 to 'CFB' both sports so they have similar words expect specific ones. Topic 3 might more be related to 'news' as it has words like link (to refer to a source), interview, story, murder. Topic 4 is more related to feelings with words like time, day, feel, shit, pretty,... but harder to distinguish. Topic 5 might be related to government, nations or world maybe ? with words like police, american, russia, war, country. Topic 6 might well be a question topic with words like comment, contact,.... And topic 7 might more be a PC or PC gaming type of topic with words like game, steam, ram, build. we see that they reflect our subreddit which makes sense, now it would be interesting to take more that 10 subreddit as we might have a interesting regrouping of subreddits.

#### Topic document Analysis

```{r}
set.seed(1234)
theta.long <- melt(
  df.top10_subbredit.lda$theta,
  varnames = c("Doc", "Topic"),
  value.name = "Theta")

# Ensure `theta.long` has the same document order as `df.top10_subreddit`
theta.long$Doc <- rep(df.top10_subreddit$subreddit, each = ncol(df.top10_subreddit.dfm$theta))

theta.long %>% 
  group_by(Topic) %>% 
  top_n(10, Theta) %>% 
  ggplot(aes(reorder_within(Doc, Theta, Topic), Theta)) + 
  geom_col(show.legend = FALSE) +
  coord_flip()+
  facet_wrap(~ Topic, scales = "free_y") +
  scale_x_reordered() + 
  xlab("Document") + 
  theme(
    axis.text.y = element_text(size = 5),
    strip.text = element_text(size = 5))
```
This confirms my previous analysis. we see topic one related to hockey, topic 2 related to CFB but interestingly lol is also here. Topic 3 was in fact reports, topic 4 was indeed feelings as it encompass a large range of subreddits. Topic 5 was indeed related to world news, news in general, topic 6 was indeed questions and topic 7 was indeed PC.

#### LDA diagnostics

```{r}
#topic prevalence
rev(sort(colSums(df.top10_subbredit.lda$theta)/sum(df.top10_subbredit.lda$theta)))
```
Topic 4 most prevalent.

```{r}
#using topicmodels
df.top10_subbredit.LDA <- LDA(
  convert(df.top10_subreddit.dfm, to = "topicmodels"),
  k = 7
)

topicmodels::terms(df.top10_subbredit.LDA, 5)
topicmodels::topics(df.top10_subbredit.LDA)
topicmodels::topics(df.top10_subbredit.LDA) %>% table()

topic_diagnostics(
  topic_model = df.top10_subbredit.LDA, 
  dtm_data = convert(df.top10_subreddit.dfm, to = "topicmodels"))
```

```{r}
#reproduce  term-topic analysis with this package
beta.long <- tidy(
  df.top10_subbredit.LDA,
  matrix = "beta") # equivalent to melt (with this package)

beta.long %>% 
  group_by(topic) %>% 
  top_n(10, beta) %>% 
  ggplot(aes(reorder_within(term, beta, topic), beta)) + 
  geom_col(show.legend = FALSE) +
  coord_flip()+
  facet_wrap(~ topic, scales = "free_y") +
  scale_x_reordered() + 
  xlab("Term") +
  theme(
    axis.text.y = element_text(size = 5),
    axis.text.x = element_text(size = 5),
    strip.text = element_text(size = 5))
```


### Attempt 6 - Sentiment Analysis

#### AFINN

assgin sentiment score by word through the use of the inner joins

```{r}
#get top 50 subreddits
df.top50_subreddit <- df.subreddit %>%
  arrange(desc(nchar(body))) %>%
  filter(row_number() <= 50)

df.top50_subreddit.tokens <- df.top50_subreddit %>% unnest_tokens(word, body)
df.top50_subreddit.tokens

df.top50_subreddit.affin <- inner_join(df.top50_subreddit.tokens, get_sentiments("afinn"),
 by = c("word" = "word")) %>% group_by(subreddit) %>%
 summarize(Sentiment = mean(value)) %>% ungroup()
 ggplot(df.top50_subreddit.affin, aes(x = Sentiment, y = reorder(subreddit, Sentiment))) + geom_col() + ylab("")
 
```

there seems to be more positive subreddits than negative ones.

We interestingly observe that based on the affin dictionnary the `news` and `politics` subreddit are the most negative, which could reflect a state of the world.
And the `pcmasterrace` is the most positive, by far. 

#### NRC

```{r}
df.top50_subreddit.nrc <- inner_join(df.top50_subreddit.tokens, get_sentiments("nrc"),
 by = c("word" = "word")) %>% group_by(subreddit, sentiment) %>%
 summarize(n = n()) %>% spread(sentiment, n, fill = 0) %>%
 mutate(Sentiment = positive - negative) %>% select(subreddit, Sentiment)
 ggplot(df.top50_subreddit.nrc, aes(x = Sentiment, y = reorder(subreddit, Sentiment))) + geom_col() + ylab("")
```

Through the NRC dictionnary we observe far mor positive subreddits than negative ones. This difference could be explain by the fact that the NRC dictionnary is more balanced than the AFINN dictionnary ?

We still observe quite the same patterns though.

#### BING

```{r}
df.top50_subreddit.bing <- inner_join(df.top50_subreddit.tokens, get_sentiments("bing"),
 by = c("word" = "word")) %>% group_by(subreddit, sentiment) %>%
 summarize(n = n()) %>% spread(sentiment, n, fill = 0) %>%
 mutate(Sentiment = positive - negative) %>% select(subreddit, Sentiment)
 ggplot(df.top50_subreddit.bing, aes(x = Sentiment, y = reorder(subreddit, Sentiment))) + geom_col() + ylab("")

```

BING seems to have a bit the same proporiton of positive and negative as the AFINN dictionnary. As the BING dictionnary combines both positive and negative words, it may be more balanced than the AFINN dictionnary,which could indicate that the NRC is more biased towards positive words.

#### Sentiment by words in comment

```{r}
#count number of rows in df
nrow(df)

#use bing to analyse sentiment all the rows based on column body
df.sentiment <- inner_join(df %>% unnest_tokens(word, body),
 get_sentiments("bing"), by = c("word" = "word")) %>% group_by(id, sentiment = sentiment) %>%
 summarize(n = n()) %>% spread(sentiment, n, fill = 0) %>% mutate(Sentiment = positive - negative) %>%
 select(id, Sentiment) %>% inner_join(df, by = "id")

#save the data without body
# write.csv(df.sentiment %>% select(-body), here("data/reddit_comments_15k_cleaned_NOBODY.csv"), row.names = TRUE)

ggplot(df.sentiment, aes(x = Sentiment, y = score)) + geom_point() + geom_smooth(method = "lm") + xlab("Sentiment") + ylab("Score") 
 
```
There seem to be two outliers in this plot, let's examine them

```{r}
df.sentiment %>% filter(Sentiment > 100)
```

They seem to be due to the fact that one comment just said 'happy new year' over and over. And the other one just put 'GEM' over and over. This is why the sentiment is so high, as both represent highl positive words and they are repeated a lot.

#### Correlation sentiment - score

```{r}
# assess correlation between sentiment and score
cor.test(df.sentiment$Sentiment, df.sentiment$score)
```
There seems to be no relation between sentiment and score

```{r}
#numbers of rows in df.sentiment
nrow(df.sentiment)
```
#### create a new column sentiment based on the sentiment score of a comment

```{r}
## create a small dataset for testing purpose
df.small <- df %>% sample_n(2)

#step 1
## create a for loop
for (i in 1:nrow(df.small)) {
  
  # that tokenize the comment
  tokens <- df.small$body[i] %>% 
    tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_url = TRUE, remove_hyphens = TRUE, remove_separators = TRUE) %>%
    tokens_tolower() %>%
    tokens_remove(stop_words$word) %>%
    tokens_remove(c("reddit", "subreddit", "amp", "gt", "deleted", "xx+")) %>%
    tokens_replace(
      pattern = hash_lemmas$token,
      replacement = hash_lemmas$lemma)
  print(tokens)
  
  # show error if character or not
  if (!is.character(tokens)) {
    tokens <- as.character(tokens)
  }
  # Assign a sentiment score to the comment
  df.small$sentiment[i] <- sum(get_sentiments("bing")$value[get_sentiments("bing")$word %in% tokens])
  
  #show the sentiment score
  print(df.small$sentiment[i])
}

df.small
```


### Adding Length of comment as a feature

```{r}
#add length of comment as a feature
