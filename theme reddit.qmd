---
title: "test theme reddit"
format: html
---

## Quarto

```{r}
d_reddit <- read.csv("data/reddit_comments_15k_cleaned_NOBODY.csv")
```

You can add options to executable code like this

```{r}
# Get unique values in the 'subreddit' column
unique_subreddits <- unique(d_reddit$subreddit)

# Print the unique values
options(max.print = 5000)
print(unique_subreddits)


```

cluster

```{r, warning=FALSE, message=FALSE}
#install.packages(c("tm", "stringdist", "cluster", "factoextra"))
library(tm)         # For text processing
library(stringdist) # For computing text-based distances
library(cluster)    # For clustering methods
library(factoextra) # For visualizing clusters

```

```{r}
# Convert subreddit names to lowercase and remove punctuation
subreddit_names <- tolower(unique_subreddits)     # Lowercase
subreddit_names <- gsub("[[:punct:]]", "", subreddit_names)  # Remove punctuation
# Calculate the distance matrix using Levenshtein distance
distance_matrix <- stringdistmatrix(subreddit_names, subreddit_names, method = "lv")

```

```{r}
# Perform hierarchical clustering
hc <- hclust(as.dist(distance_matrix), method = "ward.D2")

# Plot the dendrogram to decide the number of clusters
plot(hc, labels = FALSE, main = "Hierarchical Clustering of Subreddits")

```

```{r}
num_clusters <- 30
clusters <- cutree(hc, k = num_clusters)
# Visualize hierarchical clustering
#fviz_cluster(list(data = as.dist(distance_matrix), cluster = clusters),
#             geom = "point", ellipse.type = "convex", main = "Subreddit Clusters")
# List subreddits by cluster
split(subreddit_names, clusters)

```

other method

```{r, warning=FALSE, message=FALSE}
#install.packages(c("text2vec", "topicmodels", "tm"))
library(text2vec)
library(topicmodels)
library(tm)

```

```{r}
# Create an iterator over the subreddit names
#subreddit_names <- unique(d_reddit$subreddit)
it <- itoken(subreddit_names, progressbar = FALSE)

# Create vocabulary and vectorize
vocab <- create_vocabulary(it)
vectorizer <- vocab_vectorizer(vocab)
dtm <- create_dtm(it, vectorizer)

```

```{r}
k <- 20  # Number of topics to find
lda_model <- LDA(dtm, k = k, control = list(seed = 123))


```

```{r}
# Extract the terms for each topic
terms(lda_model, 20)  # Show top 10 terms for each topic


```

```{r}
topic_probabilities <- posterior(lda_model)$topics
subreddit_themes <- apply(topic_probabilities, 1, which.max)  # Assigns each subreddit to its dominant topic

```

```{r}
# Set the threshold probability
threshold <- 0.05

# Get the topic probabilities for each subreddit from the LDA model
topic_probabilities <- posterior(lda_model)$topics

# Assign each subreddit to all topics where the probability exceeds the threshold
subreddit_themes <- apply(topic_probabilities, 1, function(probabilities) {
  # Identify topics where probability is greater than threshold
  topics_above_threshold <- which(probabilities > threshold)
  
  # Return these topics as a list for each subreddit
  return(topics_above_threshold)
})

# Convert the list to a data frame for easier viewing (optional)
subreddit_themes_df <- data.frame(
  Subreddit = rownames(topic_probabilities),
  Themes = sapply(subreddit_themes, paste, collapse = ", ")
)

# Display the first few rows
head(subreddit_themes_df)

```

The idea is each unique subreddit can be categorize by different theme, sometimes it's only 1 theme, sometimes it's more

# Theme with text of commentary

```{r}
d_reddit_comment <- read.csv("data/reddit_comments_15k_cleaned_v1.csv")
```
